#!/usr/bin/env python3
"""
Ø£Ø¯Ø§Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ø³ÙŠØ·Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
Simple and Reliable Website Extractor
"""

import os
import json
import csv
import time
import requests
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import traceback

# ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ­Ø¯
OUTPUT_DIR = Path("11")
OUTPUT_DIR.mkdir(exist_ok=True)

class SimpleWebsiteExtractor:
    """Ø£Ø¯Ø§Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ø³ÙŠØ·Ø© ÙˆÙØ¹Ø§Ù„Ø©"""
    
    def __init__(self, output_dir="11"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.session = self._create_session()
        
    def _create_session(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù„Ø³Ø© HTTP Ù…Ø­Ø³Ù†Ø©"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        return session
    
    def extract_website(self, url, extraction_type="basic"):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙ‚Ø¹ ÙƒØ§Ù…Ù„"""
        print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {url}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        domain = urlparse(url).netloc.replace('.', '_')
        extraction_folder = self.output_dir / f"{domain}_{timestamp}"
        extraction_folder.mkdir(exist_ok=True)
        
        results = {
            'url': url,
            'extraction_type': extraction_type,
            'timestamp': timestamp,
            'extraction_folder': str(extraction_folder),
            'success': False,
            'error': None,
            'data': {}
        }
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            print(f"ğŸ“¡ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            basic_info = self._extract_basic_info(soup, url, response)
            results['data']['basic_info'] = basic_info
            
            # Ø­ÙØ¸ HTML Ø§Ù„Ø£ØµÙ„ÙŠ
            html_file = extraction_folder / 'index.html'
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ HTML: {html_file}")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            links = self._extract_links(soup, url)
            results['data']['links'] = links
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
            if extraction_type in ['standard', 'advanced', 'complete']:
                images = self._extract_images(soup, url)
                results['data']['images'] = images
                
                # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
                images_folder = extraction_folder / 'images'
                images_folder.mkdir(exist_ok=True)
                downloaded_images = self._download_images(images, images_folder)
                results['data']['downloaded_images'] = downloaded_images
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSS Ùˆ JS Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
            if extraction_type in ['advanced', 'complete']:
                css_links = self._extract_css_links(soup, url)
                js_links = self._extract_js_links(soup, url)
                
                results['data']['css_links'] = css_links
                results['data']['js_links'] = js_links
                
                # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
                assets_folder = extraction_folder / 'assets'
                assets_folder.mkdir(exist_ok=True)
                
                downloaded_css = self._download_assets(css_links, assets_folder / 'css')
                downloaded_js = self._download_assets(js_links, assets_folder / 'js')
                
                results['data']['downloaded_css'] = downloaded_css
                results['data']['downloaded_js'] = downloaded_js
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØµÙŠØºØ© JSON
            json_file = extraction_folder / 'extraction_results.json'
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(results['data'], f, ensure_ascii=False, indent=2)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± HTML
            report_file = extraction_folder / 'report.html'
            self._create_html_report(results['data'], report_file)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù CSV Ù„Ù„Ø±ÙˆØ§Ø¨Ø·
            csv_file = extraction_folder / 'links.csv'
            self._create_csv_report(links, csv_file)
            
            results['success'] = True
            print(f"âœ… ØªÙ… Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ: {extraction_folder}")
            
        except Exception as e:
            results['error'] = str(e)
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")
            
        return results
    
    def _extract_basic_info(self, soup, url, response):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
        
        description_tag = soup.find('meta', attrs={'name': 'description'})
        description = description_tag.get('content', '') if description_tag else ''
        
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        keywords = keywords_tag.get('content', '') if keywords_tag else ''
        
        return {
            'title': title_text,
            'description': description,
            'keywords': keywords,
            'url': url,
            'status_code': response.status_code,
            'content_length': len(response.text),
            'headers': dict(response.headers),
            'extraction_time': datetime.now().isoformat()
        }
    
    def _extract_links(self, soup, base_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            absolute_url = urljoin(base_url, href)
            text = link.get_text().strip()
            
            links.append({
                'href': href,
                'absolute_url': absolute_url,
                'text': text,
                'is_internal': urlparse(absolute_url).netloc == urlparse(base_url).netloc
            })
        
        return links
    
    def _extract_images(self, soup, base_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ±"""
        images = []
        
        for img in soup.find_all('img', src=True):
            src = img['src']
            absolute_url = urljoin(base_url, src)
            alt = img.get('alt', '')
            
            images.append({
                'src': src,
                'absolute_url': absolute_url,
                'alt': alt
            })
        
        return images
    
    def _extract_css_links(self, soup, base_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· CSS"""
        css_links = []
        
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                absolute_url = urljoin(base_url, href)
                css_links.append(absolute_url)
        
        return css_links
    
    def _extract_js_links(self, soup, base_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· JavaScript"""
        js_links = []
        
        for script in soup.find_all('script', src=True):
            src = script['src']
            absolute_url = urljoin(base_url, src)
            js_links.append(absolute_url)
        
        return js_links
    
    def _download_images(self, images, images_folder):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±"""
        images_folder.mkdir(exist_ok=True)
        downloaded = []
        
        for img in images[:10]:  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø¥ÙØ±Ø§Ø·
            try:
                response = self.session.get(img['absolute_url'], timeout=10)
                if response.status_code == 200:
                    filename = Path(img['absolute_url']).name or 'image.jpg'
                    file_path = images_folder / filename
                    
                    with open(file_path, 'wb') as f:
                        f.write(response.content)
                    
                    downloaded.append({
                        'url': img['absolute_url'],
                        'file_path': str(file_path),
                        'size': len(response.content)
                    })
                    
            except Exception as e:
                print(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© {img['absolute_url']}: {e}")
        
        return downloaded
    
    def _download_assets(self, urls, assets_folder):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª CSS/JS"""
        assets_folder.mkdir(exist_ok=True, parents=True)
        downloaded = []
        
        for url in urls[:5]:  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    filename = Path(url).name or 'asset.txt'
                    file_path = assets_folder / filename
                    
                    with open(file_path, 'wb') as f:
                        f.write(response.content)
                    
                    downloaded.append({
                        'url': url,
                        'file_path': str(file_path),
                        'size': len(response.content)
                    })
                    
            except Exception as e:
                print(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù {url}: {e}")
        
        return downloaded
    
    def _create_html_report(self, data, report_file):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± HTML"""
        html_content = f"""
<!DOCTYPE html>
<html dir="rtl" lang="ar">
<head>
    <meta charset="UTF-8">
    <title>ØªÙ‚Ø±ÙŠØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; direction: rtl; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ccc; border-radius: 5px; }}
        .info {{ background: #f8f9fa; }}
        .links {{ background: #e9ecef; }}
        .images {{ background: #fff3cd; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th, td {{ padding: 8px; border: 1px solid #ddd; text-align: right; }}
        th {{ background: #f8f9fa; }}
    </style>
</head>
<body>
    <h1>ØªÙ‚Ø±ÙŠØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹</h1>
    
    <div class="section info">
        <h2>Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©</h2>
        <p><strong>Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:</strong> {data['basic_info'].get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
        <p><strong>Ø§Ù„Ø±Ø§Ø¨Ø·:</strong> {data['basic_info'].get('url', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
        <p><strong>Ø§Ù„ÙˆØµÙ:</strong> {data['basic_info'].get('description', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
        <p><strong>Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:</strong> {data['basic_info'].get('status_code', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
        <p><strong>Ø­Ø¬Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰:</strong> {data['basic_info'].get('content_length', 0)} Ø­Ø±Ù</p>
    </div>
    
    <div class="section links">
        <h2>Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© ({len(data.get('links', []))} Ø±Ø§Ø¨Ø·)</h2>
        <table>
            <tr><th>Ø§Ù„Ù†Øµ</th><th>Ø§Ù„Ø±Ø§Ø¨Ø·</th><th>Ù†ÙˆØ¹ Ø§Ù„Ø±Ø§Ø¨Ø·</th></tr>
            {"".join(f"<tr><td>{link.get('text', '')[:50]}</td><td>{link.get('absolute_url', '')}</td><td>{'Ø¯Ø§Ø®Ù„ÙŠ' if link.get('is_internal') else 'Ø®Ø§Ø±Ø¬ÙŠ'}</td></tr>" for link in data.get('links', [])[:20])}
        </table>
    </div>
    
    {"<div class='section images'><h2>Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© (" + str(len(data.get('images', []))) + " ØµÙˆØ±Ø©)</h2></div>" if 'images' in data else ""}
    
    <div class="section">
        <p><strong>ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬:</strong> {data['basic_info'].get('extraction_time', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
    </div>
</body>
</html>
        """
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _create_csv_report(self, links, csv_file):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± CSV Ù„Ù„Ø±ÙˆØ§Ø¨Ø·"""
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['text', 'href', 'absolute_url', 'is_internal'])
            writer.writeheader()
            writer.writerows(links)

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    extractor = SimpleWebsiteExtractor()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø®ØªÙ„ÙØ©
    test_urls = [
        "https://example.com",
        "https://httpbin.org/html"
    ]
    
    for url in test_urls:
        print(f"\n{'='*50}")
        print(f"Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙˆÙ‚Ø¹: {url}")
        print('='*50)
        
        try:
            result = extractor.extract_website(url, "standard")
            
            if result['success']:
                print(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬")
                print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {result['extraction_folder']}")
                print(f"ğŸ“„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {result['data']['basic_info']['title']}")
                print(f"ğŸ”— Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {len(result['data']['links'])}")
                if 'images' in result['data']:
                    print(f"ğŸ–¼ï¸ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±: {len(result['data']['images'])}")
            else:
                print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {result['error']}")
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
            traceback.print_exc()

if __name__ == "__main__":
    main()