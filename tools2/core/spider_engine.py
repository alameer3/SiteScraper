"""
Ù…Ø­Ø±Ùƒ Ø§Ù„Ø²Ø­Ù Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
Advanced Spider Engine for Multi-Page Crawling
"""

import re
import time
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse, urlunparse
from urllib.robotparser import RobotFileParser
from dataclasses import dataclass
from collections import deque
import xml.etree.ElementTree as ET


@dataclass
class SpiderConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø±Ùƒ Ø§Ù„Ø²Ø­Ù"""
    max_pages: int = 50
    max_depth: int = 3
    delay_between_requests: float = 1.0
    respect_robots_txt: bool = True
    follow_external_links: bool = False
    allowed_file_types: List[str] = None
    excluded_patterns: List[str] = None
    concurrent_requests: int = 5
    
    def __post_init__(self):
        if self.allowed_file_types is None:
            self.allowed_file_types = ['.html', '.htm', '.php', '.asp', '.aspx', '.jsp']
        if self.excluded_patterns is None:
            self.excluded_patterns = ['/admin/', '/login/', '/logout/', '/wp-admin/', '/cgi-bin/']


class AdvancedSpiderEngine:
    """Ù…Ø­Ø±Ùƒ Ø²Ø­Ù Ù…ØªØ·ÙˆØ± Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹"""
    
    def __init__(self, config: SpiderConfig):
        self.config = config
        self.visited_urls = set()
        self.discovered_urls = set()
        self.crawl_queue = deque()
        self.robots_parser = None
        self.sitemap_urls = set()
        self.crawl_errors = []
        
    def crawl_website(self, start_url: str, extraction_config) -> Dict[str, Any]:
        """Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø²Ø­Ù Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        
        crawl_result = {
            'start_url': start_url,
            'pages_crawled': [],
            'total_pages_discovered': 0,
            'total_pages_crawled': 0,
            'crawl_depth_reached': 0,
            'sitemap_analysis': {},
            'robots_analysis': {},
            'link_structure': {},
            'content_summary': {},
            'errors': [],
            'crawl_duration': 0
        }
        
        start_time = time.time()
        
        try:
            print(f"ğŸ•·ï¸ Ø¨Ø¯Ø¡ Ø§Ù„Ø²Ø­Ù Ù…Ù†: {start_url}")
            
            # 1. ØªØ­Ù„ÙŠÙ„ robots.txt
            robots_analysis = self._analyze_robots_txt(start_url)
            crawl_result['robots_analysis'] = robots_analysis
            
            # 2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ§Ø³ØªÙƒØ´Ø§Ù sitemap
            sitemap_analysis = self._discover_and_analyze_sitemap(start_url)
            crawl_result['sitemap_analysis'] = sitemap_analysis
            
            # 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø²Ø­Ù
            self._initialize_crawl(start_url)
            
            # 4. ØªÙ†ÙÙŠØ° Ø§Ù„Ø²Ø­Ù
            pages_crawled = self._perform_crawl(start_url, extraction_config)
            crawl_result['pages_crawled'] = pages_crawled
            
            # 5. ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            link_structure = self._analyze_link_structure()
            crawl_result['link_structure'] = link_structure
            
            # 6. Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content_summary = self._generate_content_summary(pages_crawled)
            crawl_result['content_summary'] = content_summary
            
            # 7. Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø²Ø­Ù
            crawl_result['total_pages_discovered'] = len(self.discovered_urls)
            crawl_result['total_pages_crawled'] = len(pages_crawled)
            crawl_result['crawl_depth_reached'] = max([page.get('depth', 0) for page in pages_crawled] + [0])
            crawl_result['errors'] = self.crawl_errors
            
        except Exception as e:
            crawl_result['errors'].append(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø²Ø­Ù: {str(e)}")
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø²Ø­Ù: {e}")
        
        crawl_result['crawl_duration'] = round(time.time() - start_time, 2)
        print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø²Ø­Ù ÙÙŠ {crawl_result['crawl_duration']} Ø«Ø§Ù†ÙŠØ©")
        
        return crawl_result
    
    def _analyze_robots_txt(self, base_url: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù robots.txt"""
        
        robots_analysis = {
            'exists': False,
            'user_agents': [],
            'disallowed_paths': [],
            'allowed_paths': [],
            'crawl_delay': None,
            'sitemap_urls': [],
            'compliance_score': 0
        }
        
        try:
            robots_url = urljoin(base_url, '/robots.txt')
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ robots.txt
            self.robots_parser = RobotFileParser()
            self.robots_parser.set_url(robots_url)
            self.robots_parser.read()
            
            robots_analysis['exists'] = True
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (ØªØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ± Ø¥Ø¶Ø§ÙÙŠ)
            # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ØªØ­Ù„ÙŠÙ„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹
            
            print(f"ğŸ“‹ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ robots.txt: {robots_url}")
            
        except Exception as e:
            robots_analysis['error'] = str(e)
            print(f"âš ï¸ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù€ robots.txt: {e}")
        
        return robots_analysis
    
    def _discover_and_analyze_sitemap(self, base_url: str) -> Dict[str, Any]:
        """Ø§ÙƒØªØ´Ø§Ù ÙˆØªØ­Ù„ÙŠÙ„ sitemap"""
        
        sitemap_analysis = {
            'sitemaps_found': [],
            'total_urls': 0,
            'url_types': {},
            'last_modified_dates': [],
            'priorities': [],
            'change_frequencies': []
        }
        
        # Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù€ sitemap
        potential_sitemaps = [
            '/sitemap.xml',
            '/sitemap_index.xml',
            '/sitemap.txt',
            '/sitemaps.xml',
            '/wp-sitemap.xml'
        ]
        
        for sitemap_path in potential_sitemaps:
            try:
                sitemap_url = urljoin(base_url, sitemap_path)
                # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ÙƒÙˆØ¯ Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ù„ÙŠÙ„ sitemap
                # Ù†Ø­ØªØ§Ø¬ session manager Ù„Ù„Ø·Ù„Ø¨Ø§Øª
                print(f"ğŸ—ºï¸ ÙØ­Øµ sitemap: {sitemap_url}")
                
            except Exception as e:
                continue
        
        return sitemap_analysis
    
    def _initialize_crawl(self, start_url: str):
        """ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø²Ø­Ù"""
        
        # Ø¥Ø¶Ø§ÙØ© URL Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
        parsed_start = urlparse(start_url)
        self.base_domain = parsed_start.netloc
        
        self.crawl_queue.append({
            'url': start_url,
            'depth': 0,
            'parent_url': None,
            'link_text': 'START'
        })
        
        self.discovered_urls.add(start_url)
    
    def _perform_crawl(self, start_url: str, extraction_config) -> List[Dict[str, Any]]:
        """ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø²Ø­Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        
        pages_crawled = []
        
        while self.crawl_queue and len(pages_crawled) < self.config.max_pages:
            
            current_item = self.crawl_queue.popleft()
            current_url = current_item['url']
            current_depth = current_item['depth']
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù†Ù†Ø§ Ù„Ù… Ù†Ø²Ø­Ù Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ù‚Ø¨Ù„
            if current_url in self.visited_urls:
                continue
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ù…Ù‚ Ø§Ù„Ø²Ø­Ù
            if current_depth > self.config.max_depth:
                continue
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† robots.txt
            if self.config.respect_robots_txt and self.robots_parser:
                if not self.robots_parser.can_fetch('*', current_url):
                    print(f"ğŸš« Ù…Ù†Ø¹ Ø¨ÙˆØ§Ø³Ø·Ø© robots.txt: {current_url}")
                    continue
            
            try:
                print(f"ğŸ” Ø²Ø­Ù: {current_url} (Ø¹Ù…Ù‚: {current_depth})")
                
                # Ù‡Ù†Ø§ Ù†Ø­ØªØ§Ø¬ session manager Ù„Ù„Ø·Ù„Ø¨
                page_data = self._crawl_single_page(current_url, current_depth, current_item)
                
                if page_data:
                    pages_crawled.append(page_data)
                    self.visited_urls.add(current_url)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                    if current_depth < self.config.max_depth:
                        new_links = self._extract_links_from_page(page_data, current_url, current_depth)
                        self._add_links_to_queue(new_links, current_depth + 1, current_url)
                
                # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
                time.sleep(self.config.delay_between_requests)
                
            except Exception as e:
                error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ø²Ø­Ù {current_url}: {str(e)}"
                self.crawl_errors.append(error_msg)
                print(f"âŒ {error_msg}")
                continue
        
        return pages_crawled
    
    def _crawl_single_page(self, url: str, depth: int, item_info: Dict) -> Optional[Dict[str, Any]]:
        """Ø²Ø­Ù ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø© (ÙŠØ­ØªØ§Ø¬ session manager)"""
        
        # Ù‡Ù†Ø§ Ù†Ø­ØªØ§Ø¬ session manager Ù„Ù„Ø·Ù„Ø¨ Ø§Ù„ÙØ¹Ù„ÙŠ
        # Ø­Ø§Ù„ÙŠØ§Ù‹ Ø³Ù†Ø±Ø¬Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        
        page_data = {
            'url': url,
            'depth': depth,
            'parent_url': item_info.get('parent_url'),
            'link_text': item_info.get('link_text', ''),
            'title': '',
            'description': '',
            'content_length': 0,
            'links_count': 0,
            'images_count': 0,
            'response_code': 200,
            'content_type': 'text/html',
            'discovered_links': [],
            'timestamp': time.time()
        }
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø©
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ù‡Ù†Ø§ Ø³Ù†Ø³ØªØ®Ø¯Ù… session manager Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        # Ø«Ù… Ù†Ø­Ù„Ù„ HTML Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup
        
        return page_data
    
    def _extract_links_from_page(self, page_data: Dict, base_url: str, current_depth: int) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        
        links = []
        
        # Ù‡Ù†Ø§ Ù†Ø­ØªØ§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù„ØµÙØ­Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        # Ø­Ø§Ù„ÙŠØ§Ù‹ Ø³Ù†Ø±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ©
        
        return links
    
    def _add_links_to_queue(self, links: List[Dict], depth: int, parent_url: str):
        """Ø¥Ø¶Ø§ÙØ© Ø±ÙˆØ§Ø¨Ø· Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø²Ø­Ù"""
        
        for link in links:
            url = link.get('url', '')
            
            if not url or url in self.visited_urls:
                continue
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©
            if not self._is_allowed_url(url):
                continue
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
            if not self.config.follow_external_links:
                parsed = urlparse(url)
                if parsed.netloc and parsed.netloc != self.base_domain:
                    continue
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
            if self._is_excluded_url(url):
                continue
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
            if url not in self.discovered_urls:
                self.crawl_queue.append({
                    'url': url,
                    'depth': depth,
                    'parent_url': parent_url,
                    'link_text': link.get('text', '')
                })
                self.discovered_urls.add(url)
    
    def _is_allowed_url(self, url: str) -> bool:
        """ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø³Ù…ÙˆØ­"""
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù
        parsed = urlparse(url)
        path = parsed.path.lower()
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø§Ù…ØªØ¯Ø§Ø¯ØŒ Ø§Ø¹ØªØ¨Ø±Ù‡ Ù…Ø³Ù…ÙˆØ­
        if '.' not in path.split('/')[-1]:
            return True
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©
        for ext in self.config.allowed_file_types:
            if path.endswith(ext.lower()):
                return True
        
        return False
    
    def _is_excluded_url(self, url: str) -> bool:
        """ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ØºÙŠØ± Ù…Ø³ØªØ¨Ø¹Ø¯"""
        
        url_lower = url.lower()
        
        for pattern in self.config.excluded_patterns:
            if pattern.lower() in url_lower:
                return True
        
        return False
    
    def _analyze_link_structure(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        
        structure = {
            'total_internal_links': 0,
            'total_external_links': 0,
            'depth_distribution': {},
            'most_linked_pages': [],
            'orphan_pages': [],
            'link_density': 0
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©
        # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ØªØ­Ù„ÙŠÙ„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹
        
        return structure
    
    def _generate_content_summary(self, pages_crawled: List[Dict]) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        
        summary = {
            'total_pages': len(pages_crawled),
            'average_content_length': 0,
            'content_types_distribution': {},
            'title_analysis': {
                'average_length': 0,
                'most_common_words': []
            },
            'page_types_detected': [],
            'crawl_efficiency': 0
        }
        
        if not pages_crawled:
            return summary
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        total_length = sum(page.get('content_length', 0) for page in pages_crawled)
        summary['average_content_length'] = total_length // len(pages_crawled)
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_types = {}
        for page in pages_crawled:
            content_type = page.get('content_type', 'unknown')
            content_types[content_type] = content_types.get(content_type, 0) + 1
        
        summary['content_types_distribution'] = content_types
        
        # Ø­Ø³Ø§Ø¨ ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø²Ø­Ù
        if self.discovered_urls:
            efficiency = (len(pages_crawled) / len(self.discovered_urls)) * 100
            summary['crawl_efficiency'] = round(efficiency, 2)
        
        return summary
    
    def get_crawl_statistics(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø²Ø­Ù"""
        
        return {
            'urls_discovered': len(self.discovered_urls),
            'urls_visited': len(self.visited_urls),
            'urls_in_queue': len(self.crawl_queue),
            'crawl_completion_rate': (len(self.visited_urls) / max(len(self.discovered_urls), 1)) * 100,
            'errors_count': len(self.crawl_errors),
            'base_domain': getattr(self, 'base_domain', ''),
            'config': {
                'max_pages': self.config.max_pages,
                'max_depth': self.config.max_depth,
                'delay_between_requests': self.config.delay_between_requests,
                'respect_robots_txt': self.config.respect_robots_txt
            }
        }
    
    def export_discovered_urls(self, output_file: Path) -> bool:
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©"""
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("# URLs Ù…ÙƒØªØ´ÙØ© Ù…Ù† Ø§Ù„Ø²Ø­Ù\n")
                f.write(f"# Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(self.discovered_urls)}\n\n")
                
                for url in sorted(self.discovered_urls):
                    status = "âœ… ØªÙ… Ø§Ù„Ø²Ø­Ù" if url in self.visited_urls else "â³ ÙÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±"
                    f.write(f"{url} - {status}\n")
            
            return True
        
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
            return False
    
    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        self.visited_urls.clear()
        self.discovered_urls.clear()
        self.crawl_queue.clear()
        self.crawl_errors.clear()
        self.sitemap_urls.clear()
        self.robots_parser = None