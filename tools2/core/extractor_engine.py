"""
Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØ·ÙˆØ± - Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
Advanced Extraction Engine - Main Class
"""

import time
from datetime import datetime
from urllib.parse import urlparse
from typing import Dict, List, Any, Optional
from pathlib import Path
from bs4 import BeautifulSoup

from .config import ExtractionConfig, get_preset_config
from .session_manager import SessionManager
from .file_manager import FileManager
from .content_extractor import ContentExtractor
from .security_analyzer import SecurityAnalyzer
from .asset_downloader import AssetDownloader


class AdvancedExtractorEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ·ÙˆØ± ÙˆØ´Ø§Ù…Ù„"""
    
    def __init__(self, config: Optional[ExtractionConfig] = None):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
        self.config = config or ExtractionConfig()
        self.extraction_id = 0
        self.results_storage = {}
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.session_manager = SessionManager(self.config)
        self.file_manager = FileManager(self.config.output_directory)
        self.content_extractor = ContentExtractor(self.config, self.session_manager)
        self.security_analyzer = SecurityAnalyzer(self.config, self.session_manager)
        self.asset_downloader = AssetDownloader(self.config, self.session_manager, self.file_manager)
        
    def extract_website(self, url: str, extraction_type: str = None) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        if extraction_type:
            self.config.extraction_type = extraction_type
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…ÙØ¹Ø±ÙÙ‘ÙØ© Ù…Ø³Ø¨Ù‚Ø§Ù‹
        if self.config.extraction_type in ['basic', 'standard', 'advanced', 'complete']:
            preset_config = get_preset_config(self.config.extraction_type)
            # Ø¯Ù…Ø¬ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ URL
            preset_config.target_url = url
            self.config = preset_config
        
        self.config.target_url = url
        self.extraction_id += 1
        
        start_time = time.time()
        
        try:
            print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {url}")
            print(f"ğŸ“Š Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {self.config.extraction_type}")
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø·
            if not self._validate_url(url):
                raise ValueError(f"Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ­ÙŠØ­: {url}")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
            extraction_folder = self.file_manager.create_extraction_folder(
                str(self.extraction_id), 
                url
            )
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            print("ğŸ“¥ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ...")
            response = self.session_manager.make_request(url)
            
            if not response:
                raise Exception("ÙØ´Ù„ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹")
            
            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù…
            self.file_manager.save_html_content(response.text, extraction_folder)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            basic_info = self.content_extractor.extract_basic_info(soup, url, response)
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            result = {
                'extraction_id': self.extraction_id,
                'url': url,
                'extraction_type': self.config.extraction_type,
                'timestamp': datetime.now().isoformat(),
                'extraction_folder': str(extraction_folder),
                'success': True,
                **basic_info
            }
            
            # ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
            if self.config.extraction_type in ['standard', 'advanced', 'complete']:
                result.update(self._perform_advanced_analysis(soup, url, response, extraction_folder))
            
            # ØªØ­Ù„ÙŠÙ„Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„
            if self.config.extraction_type in ['advanced', 'complete']:
                result.update(self._perform_comprehensive_analysis(soup, url, response, extraction_folder))
            
            # ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„
            if self.config.extract_assets:
                print("ğŸ’¾ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„...")
                assets_result = self.asset_downloader.download_all_assets(soup, url, extraction_folder)
                result['assets'] = assets_result
            
            # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°
            result['duration'] = round(time.time() - start_time, 2)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self.file_manager.save_json_data(result, extraction_folder, 'extraction_results.json')
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
            if self.config.export_html:
                print("ğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±...")
                self.file_manager.generate_html_report(result, extraction_folder)
            
            if self.config.export_csv and 'links_analysis' in result:
                self._export_csv_data(result, extraction_folder)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø±Ø´ÙŠÙ Ù…Ø¶ØºÙˆØ· Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„
            if self.config.extraction_type == 'complete':
                print("ğŸ—œï¸ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø±Ø´ÙŠÙ...")
                archive_path = self.file_manager.create_archive(extraction_folder)
                if archive_path:
                    result['archive_path'] = str(archive_path)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.results_storage[self.extraction_id] = result
            
            print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙŠ {result['duration']} Ø«Ø§Ù†ÙŠØ©")
            print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {extraction_folder}")
            
            return result
            
        except Exception as e:
            error_result = {
                'extraction_id': self.extraction_id,
                'url': url,
                'extraction_type': self.config.extraction_type,
                'success': False,
                'error': str(e),
                'duration': round(time.time() - start_time, 2),
                'timestamp': datetime.now().isoformat()
            }
            
            self.results_storage[self.extraction_id] = error_result
            print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")
            return error_result
    
    def _validate_url(self, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø·"""
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
                self.config.target_url = url
            
            parsed = urlparse(url)
            return bool(parsed.netloc and parsed.scheme)
        except:
            return False
    
    def _perform_advanced_analysis(self, soup: BeautifulSoup, url: str, response, extraction_folder: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        advanced_data = {}
        
        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„ØµÙˆØ±...")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        if self.config.extract_links:
            links_analysis = self.content_extractor.extract_links_analysis(soup, url)
            advanced_data['links_analysis'] = links_analysis
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±
        if self.config.extract_images:
            images_analysis = self.content_extractor.extract_images_analysis(soup, url)
            advanced_data['images_analysis'] = images_analysis
        
        # ØªØ­Ù„ÙŠÙ„ SEO
        if self.config.analyze_seo:
            print("ğŸ” ØªØ­Ù„ÙŠÙ„ SEO...")
            seo_analysis = self.content_extractor.extract_seo_analysis(soup)
            advanced_data['seo_analysis'] = seo_analysis
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if self.config.analyze_performance:
            print("âš¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡...")
            performance_analysis = self._analyze_performance(response, len(response.text))
            advanced_data['performance_analysis'] = performance_analysis
        
        return advanced_data
    
    def _perform_comprehensive_analysis(self, soup: BeautifulSoup, url: str, response, extraction_folder: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„"""
        comprehensive_data = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ø§Ù†
        if self.config.analyze_security:
            print("ğŸ”’ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ø§Ù†...")
            security_analysis = self.security_analyzer.analyze_security(soup, url, response)
            comprehensive_data['security_analysis'] = security_analysis
        
        # ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹
        print("ğŸ—ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡ÙŠÙƒÙ„...")
        structure_analysis = self._analyze_website_structure(soup)
        comprehensive_data['structure_analysis'] = structure_analysis
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† API endpoints
        print("ğŸ”— Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† API endpoints...")
        api_endpoints = self._find_api_endpoints(soup, response.text)
        comprehensive_data['api_endpoints'] = api_endpoints
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªÙØ§Ø¹Ù„
        print("âš¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©...")
        interactive_analysis = self._analyze_interactive_elements(soup)
        comprehensive_data['interactive_analysis'] = interactive_analysis
        
        return comprehensive_data
    
    def _analyze_performance(self, response, content_size: int) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        headers = response.headers
        
        return {
            'response_time_seconds': response.elapsed.total_seconds(),
            'content_size_bytes': content_size,
            'content_size_mb': round(content_size / (1024 * 1024), 2),
            'compression_enabled': 'gzip' in headers.get('Content-Encoding', ''),
            'cache_control': headers.get('Cache-Control', ''),
            'expires': headers.get('Expires', ''),
            'etag': headers.get('ETag', ''),
            'last_modified': headers.get('Last-Modified', ''),
            'server': headers.get('Server', ''),
            'content_type': headers.get('Content-Type', ''),
            'performance_score': self._calculate_performance_score(response, content_size)
        }
    
    def _calculate_performance_score(self, response, content_size: int) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        score = 0
        issues = []
        
        # ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        response_time = response.elapsed.total_seconds()
        if response_time < 1:
            score += 25
        elif response_time < 3:
            score += 20
        elif response_time < 5:
            score += 15
        else:
            score += 5
            issues.append(f"ÙˆÙ‚Øª Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø·ÙŠØ¡: {response_time:.1f} Ø«Ø§Ù†ÙŠØ©")
        
        # Ø­Ø¬Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_mb = content_size / (1024 * 1024)
        if content_mb < 1:
            score += 25
        elif content_mb < 2:
            score += 20
        elif content_mb < 5:
            score += 15
        else:
            score += 5
            issues.append(f"Ø­Ø¬Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙƒØ¨ÙŠØ±: {content_mb:.1f} MB")
        
        # Ø¶ØºØ· Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if 'gzip' in response.headers.get('Content-Encoding', ''):
            score += 15
        else:
            issues.append("Ø¶ØºØ· Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ù…ÙØ¹Ù„")
        
        # Cache headers
        if response.headers.get('Cache-Control') or response.headers.get('Expires'):
            score += 15
        else:
            issues.append("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù…ÙÙ‚ÙˆØ¯Ø©")
        
        # CDN detection
        server = response.headers.get('Server', '').lower()
        cdn_indicators = ['cloudflare', 'amazon', 'fastly', 'akamai']
        if any(indicator in server for indicator in cdn_indicators):
            score += 10
        
        return {
            'score': min(score, 100),
            'percentage': min(score, 100),
            'level': self._get_performance_level(score),
            'issues': issues
        }
    
    def _get_performance_level(self, score: int) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if score >= 80:
            return 'Ù…Ù…ØªØ§Ø²'
        elif score >= 60:
            return 'Ø¬ÙŠØ¯'
        elif score >= 40:
            return 'Ù…ØªÙˆØ³Ø·'
        else:
            return 'Ø¶Ø¹ÙŠÙ'
    
    def _analyze_website_structure(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        
        # ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ HTML
        html_structure = {
            'has_doctype': bool(soup.find(string=lambda text: isinstance(text, str) and 'DOCTYPE' in text)),
            'html_lang': soup.find('html').get('lang', '') if soup.find('html') else '',
            'head_elements': len(soup.find('head').find_all()) if soup.find('head') else 0,
            'body_elements': len(soup.find('body').find_all()) if soup.find('body') else 0
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        semantic_elements = {
            'header': len(soup.find_all('header')),
            'nav': len(soup.find_all('nav')),
            'main': len(soup.find_all('main')),
            'article': len(soup.find_all('article')),
            'section': len(soup.find_all('section')),
            'aside': len(soup.find_all('aside')),
            'footer': len(soup.find_all('footer'))
        }
        
        # ØªØ­Ù„ÙŠÙ„ accessibility
        accessibility = {
            'images_with_alt': len(soup.find_all('img', alt=True)),
            'total_images': len(soup.find_all('img')),
            'form_labels': len(soup.find_all('label')),
            'heading_structure': {f'h{i}': len(soup.find_all(f'h{i}')) for i in range(1, 7)},
            'skip_links': len(soup.find_all('a', href=lambda href: href and href.startswith('#')))
        }
        
        return {
            'html_structure': html_structure,
            'semantic_elements': semantic_elements,
            'accessibility': accessibility,
            'structure_score': self._calculate_structure_score(html_structure, semantic_elements, accessibility)
        }
    
    def _calculate_structure_score(self, html_structure: Dict, semantic_elements: Dict, accessibility: Dict) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‡ÙŠÙƒÙ„"""
        score = 0
        issues = []
        
        # HTML structure
        if html_structure['has_doctype']:
            score += 10
        else:
            issues.append("DOCTYPE Ù…ÙÙ‚ÙˆØ¯")
        
        if html_structure['html_lang']:
            score += 10
        else:
            issues.append("Ø®Ø§ØµÙŠØ© lang Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ HTML")
        
        # Semantic elements
        semantic_count = sum(semantic_elements.values())
        if semantic_count >= 5:
            score += 20
        elif semantic_count >= 3:
            score += 15
        elif semantic_count >= 1:
            score += 10
        else:
            issues.append("Ø¹Ù†Ø§ØµØ± Ø¯Ù„Ø§Ù„ÙŠØ© Ù‚Ù„ÙŠÙ„Ø© Ø£Ùˆ Ù…ÙÙ‚ÙˆØ¯Ø©")
        
        # Accessibility
        total_images = accessibility['total_images']
        images_with_alt = accessibility['images_with_alt']
        
        if total_images > 0:
            alt_ratio = images_with_alt / total_images
            if alt_ratio >= 0.9:
                score += 15
            elif alt_ratio >= 0.7:
                score += 10
            elif alt_ratio >= 0.5:
                score += 5
            else:
                issues.append("Ù†ØµÙˆØµ Alt Ù…ÙÙ‚ÙˆØ¯Ø© Ù„Ù„ØµÙˆØ±")
        
        # Heading structure
        headings = accessibility['heading_structure']
        if headings['h1'] == 1:
            score += 10
        elif headings['h1'] > 1:
            issues.append("Ø£ÙƒØ«Ø± Ù…Ù† H1 ÙˆØ§Ø­Ø¯")
        elif headings['h1'] == 0:
            issues.append("H1 Ù…ÙÙ‚ÙˆØ¯")
        
        return {
            'score': min(score, 100),
            'percentage': min(score, 100),
            'issues': issues
        }
    
    def _find_api_endpoints(self, soup: BeautifulSoup, content: str) -> Dict[str, Any]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† API endpoints"""
        import re
        
        endpoints = {
            'ajax_calls': [],
            'api_urls': [],
            'rest_patterns': [],
            'graphql_endpoints': []
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† AJAX calls ÙÙŠ JavaScript
        ajax_patterns = [
            r'\.ajax\s*\(\s*["\']([^"\']+)["\']',
            r'fetch\s*\(\s*["\']([^"\']+)["\']',
            r'axios\.[get|post|put|delete]+\s*\(\s*["\']([^"\']+)["\']'
        ]
        
        for pattern in ajax_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            endpoints['ajax_calls'].extend(matches[:5])  # Ø£ÙˆÙ„ 5 Ù†ØªØ§Ø¦Ø¬
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† API URLs
        api_url_patterns = [
            r'["\']([^"\']*api[^"\']*)["\']',
            r'["\']([^"\']*\/v\d+\/[^"\']*)["\']'
        ]
        
        for pattern in api_url_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            endpoints['api_urls'].extend(matches[:5])
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† GraphQL
        if 'graphql' in content.lower():
            graphql_matches = re.findall(r'["\']([^"\']*graphql[^"\']*)["\']', content, re.IGNORECASE)
            endpoints['graphql_endpoints'].extend(graphql_matches[:3])
        
        return {
            'endpoints': endpoints,
            'total_found': sum(len(v) for v in endpoints.values()),
            'has_api_integration': any(len(v) > 0 for v in endpoints.values())
        }
    
    def _analyze_interactive_elements(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©"""
        
        interactive_elements = {
            'forms': len(soup.find_all('form')),
            'buttons': len(soup.find_all('button')) + len(soup.find_all('input', type='button')),
            'input_fields': len(soup.find_all('input')),
            'select_dropdowns': len(soup.find_all('select')),
            'textareas': len(soup.find_all('textarea')),
            'clickable_elements': len(soup.find_all(['a', 'button'])),
            'modal_triggers': len(soup.find_all(attrs={'data-toggle': 'modal'})),
            'tabs': len(soup.find_all(attrs={'role': 'tab'})),
            'accordions': len(soup.find_all(class_=lambda x: x and 'accordion' in str(x).lower()))
        }
        
        # ØªØ­Ù„ÙŠÙ„ JavaScript events
        script_tags = soup.find_all('script')
        js_events = {
            'onclick_handlers': 0,
            'event_listeners': 0,
            'jquery_events': 0
        }
        
        for script in script_tags:
            if script.string:
                content = script.string.lower()
                js_events['onclick_handlers'] += content.count('onclick')
                js_events['event_listeners'] += content.count('addeventlistener')
                js_events['jquery_events'] += content.count('.on(') + content.count('.click(')
        
        return {
            'interactive_elements': interactive_elements,
            'javascript_events': js_events,
            'interactivity_score': self._calculate_interactivity_score(interactive_elements, js_events),
            'total_interactive_elements': sum(interactive_elements.values())
        }
    
    def _calculate_interactivity_score(self, interactive_elements: Dict, js_events: Dict) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙØ§Ø¹Ù„"""
        score = 0
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
        total_elements = sum(interactive_elements.values())
        if total_elements >= 20:
            score += 30
        elif total_elements >= 10:
            score += 25
        elif total_elements >= 5:
            score += 20
        elif total_elements >= 1:
            score += 15
        
        # Ù†Ù‚Ø§Ø· JavaScript events
        total_events = sum(js_events.values())
        if total_events >= 10:
            score += 25
        elif total_events >= 5:
            score += 20
        elif total_events >= 1:
            score += 15
        
        # Ù…ÙƒØ§ÙØ¢Øª Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        advanced_elements = ['modal_triggers', 'tabs', 'accordions']
        advanced_count = sum(interactive_elements.get(elem, 0) for elem in advanced_elements)
        if advanced_count >= 3:
            score += 15
        elif advanced_count >= 1:
            score += 10
        
        level = 'Ø¹Ø§Ù„ÙŠ' if score >= 60 else 'Ù…ØªÙˆØ³Ø·' if score >= 30 else 'Ù…Ù†Ø®ÙØ¶'
        
        return {
            'score': min(score, 100),
            'percentage': min(score, 100),
            'level': level
        }
    
    def _export_csv_data(self, result: Dict[str, Any], extraction_folder: Path):
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØºØ© CSV"""
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        if 'links_analysis' in result:
            links_data = []
            links_analysis = result['links_analysis']
            
            for link in links_analysis.get('internal_links', []):
                links_data.append({
                    'type': 'Internal',
                    'url': link.get('href', ''),
                    'text': link.get('text', ''),
                    'title': link.get('title', '')
                })
            
            for link in links_analysis.get('external_links', []):
                links_data.append({
                    'type': 'External',
                    'url': link.get('href', ''),
                    'text': link.get('text', ''),
                    'title': link.get('title', '')
                })
            
            if links_data:
                self.file_manager.save_csv_data(links_data, extraction_folder, 'links.csv')
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„ØµÙˆØ±
        if 'images_analysis' in result:
            images_data = []
            for img in result['images_analysis'].get('images', []):
                images_data.append({
                    'src': img.get('src', ''),
                    'alt': img.get('alt', ''),
                    'width': img.get('width', ''),
                    'height': img.get('height', ''),
                    'format': img.get('format', '')
                })
            
            if images_data:
                self.file_manager.save_csv_data(images_data, extraction_folder, 'images.csv')
    
    def get_extraction_results(self, extraction_id: int = None) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
        if extraction_id:
            return self.results_storage.get(extraction_id, {})
        return self.results_storage
    
    def get_statistics(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ"""
        successful_extractions = sum(1 for result in self.results_storage.values() if result.get('success', False))
        failed_extractions = len(self.results_storage) - successful_extractions
        
        return {
            'total_extractions': len(self.results_storage),
            'successful_extractions': successful_extractions,
            'failed_extractions': failed_extractions,
            'success_rate': round((successful_extractions / max(len(self.results_storage), 1)) * 100, 1),
            'session_stats': self.session_manager.get_session_stats(),
            'storage_stats': self.file_manager.get_storage_stats()
        }
    
    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.session_manager.close()
        self.file_manager.cleanup_temp_files()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()