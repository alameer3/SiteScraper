"""
Website Cloner Pro - Ø£Ø¯Ø§Ø© Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
===============================================

Ø£Ø¯Ø§Ø© Ø´Ø§Ù…Ù„Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÙ†Ø³Ø® Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„
ØªØ¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ÙÙŠ Ù†Ø¸Ø§Ù… ÙˆØ§Ø­Ø¯ Ù…ØªÙƒØ§Ù…Ù„

Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø§Ù…Ù„ Ù„ÙƒÙ„ Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹
- Ù†Ø³Ø® Ø·Ø¨Ù‚ Ø§Ù„Ø£ØµÙ„ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹
- ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
- ØªÙ‚Ø§Ø±ÙŠØ± Ù…ÙØµÙ„Ø© ÙˆØ´Ø§Ù…Ù„Ø©
- Ø¯Ø¹Ù… Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù€ APIs
- ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ù…Ø§ÙŠØ© ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ
"""

import asyncio
import aiohttp
import aiofiles
import os
import json
import time
import logging
import hashlib
import re
import ssl
import csv
import shutil
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple, Union
from urllib.parse import urljoin, urlparse, parse_qs, unquote
from dataclasses import dataclass, asdict, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import queue

# Web scraping and parsing
from bs4 import BeautifulSoup, Tag, NavigableString
from bs4.element import NavigableString as BS4NavigableString
from typing import cast
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Advanced browser automation
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False

# Content analysis
try:
    import builtwith
    BUILTWITH_AVAILABLE = True
except ImportError:
    BUILTWITH_AVAILABLE = False

# Document generation
try:
    from reportlab.pdfgen import canvas
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.lib.units import inch
    REPORTLAB_AVAILABLE = True
except ImportError:
    REPORTLAB_AVAILABLE = False

try:
    from docx import Document
    from docx.shared import Inches
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

@dataclass
class CloningConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø®"""
    
    # Basic settings
    target_url: str = ""
    output_directory: str = "cloned_websites"
    max_depth: int = 5
    max_pages: int = 1000
    timeout: int = 30
    delay_between_requests: float = 1.0
    
    # Content extraction
    extract_all_content: bool = True
    extract_hidden_content: bool = True
    extract_dynamic_content: bool = True
    extract_media_files: bool = True
    extract_documents: bool = True
    extract_apis: bool = True
    extract_database_structure: bool = True
    
    # Advanced features
    bypass_protection: bool = True
    handle_javascript: bool = True
    handle_ajax: bool = True
    detect_spa: bool = True
    extract_source_code: bool = True
    analyze_with_ai: bool = True
    
    # Security and stealth
    use_proxy: bool = False
    proxy_list: List[str] = field(default_factory=list)
    rotate_user_agents: bool = True
    respect_robots_txt: bool = False
    handle_captcha: bool = True
    
    # Output formats
    create_identical_copy: bool = True
    generate_reports: bool = True
    export_formats: List[str] = field(default_factory=lambda: ['html', 'json', 'csv', 'pdf', 'docx'])
    detailed_logging: bool = True
    
    # Performance
    parallel_downloads: int = 10
    use_caching: bool = True
    optimize_images: bool = False
    compress_output: bool = False
    
    # Content extraction
    extract_all_content: bool = True
    extract_hidden_content: bool = True
    extract_dynamic_content: bool = True
    extract_media_files: bool = True
    extract_documents: bool = True
    extract_apis: bool = True
    extract_database_structure: bool = True
    
    # Advanced features
    bypass_protection: bool = True
    handle_javascript: bool = True
    handle_ajax: bool = True
    detect_spa: bool = True
    extract_source_code: bool = True
    analyze_with_ai: bool = True
    
    # Security and stealth
    use_proxy: bool = False
    proxy_list: List[str] = field(default_factory=list)
    rotate_user_agents: bool = True
    respect_robots_txt: bool = False
    handle_captcha: bool = True
    
    # Output formats
    create_identical_copy: bool = True
    generate_reports: bool = True
    export_formats: List[str] = field(default_factory=lambda: ['html', 'json', 'csv', 'pdf', 'docx'])
    detailed_logging: bool = True
    
    # Performance
    parallel_downloads: int = 10
    use_caching: bool = True
    optimize_images: bool = False
    compress_output: bool = False

@dataclass 
class CloningResult:
    """Ù†ØªØ§Ø¦Ø¬ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø®"""
    
    success: bool = False
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    duration: float = 0.0
    
    # Statistics
    pages_extracted: int = 0
    assets_downloaded: int = 0
    errors_encountered: int = 0
    total_size: int = 0
    
    # Paths and files
    output_path: str = ""
    cloned_site_path: str = ""
    reports_path: str = ""
    
    # Analysis results
    technologies_detected: Dict[str, Any] = field(default_factory=dict)
    ai_analysis: Dict[str, Any] = field(default_factory=dict)
    security_analysis: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    
    # Detailed results
    extracted_content: Dict[str, Any] = field(default_factory=dict)
    error_log: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)

class WebsiteClonerPro:
    """Ø£Ø¯Ø§Ø© Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©"""
    
    def __init__(self, config: Optional[CloningConfig] = None):
        self.config = config or CloningConfig()
        self.logger = self._setup_logging()
        self.session: Optional[aiohttp.ClientSession] = None
        self.selenium_driver = None
        
        # Results storage
        self.result = CloningResult()
        self.extracted_urls: Set[str] = set()
        self.download_queue: queue.Queue = queue.Queue()
        self.error_count: int = 0
        
        # Analysis caches
        self.content_cache: Dict[str, str] = {}
        self.asset_cache: Dict[str, bytes] = {}
        self.analysis_cache: Dict[str, Any] = {}
        
        # User agent rotation
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0"
        ]
        self.current_user_agent_index = 0
        
    def _setup_logging(self) -> logging.Logger:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
        logger = logging.getLogger('WebsiteClonerPro')
        logger.setLevel(logging.DEBUG if self.config.detailed_logging else logging.INFO)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„Ù
        os.makedirs('logs', exist_ok=True)
        file_handler = logging.FileHandler(f'logs/cloner_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        file_handler.setLevel(logging.DEBUG)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬ ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ…
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _clean_url(self, url: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ ÙˆØªØµØ­ÙŠØ­ URL"""
        if not url:
            return ""
        
        url = url.strip()
        
        # Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© URL Ø§Ù„Ù…Ø¶Ø§Ø¹Ù
        if 'chttps://' in url:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØµØ­ÙŠØ­
            if url.startswith('https://example.chttps://'):
                url = url.replace('https://example.chttps://', 'https://')
            elif 'chttps://' in url:
                url = url.split('chttps://', 1)[1]
                if not url.startswith('http'):
                    url = 'https://' + url
        
        # Ø¥Ø¶Ø§ÙØ© http Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # Ø¥Ø²Ø§Ù„Ø© trailing slash
        url = url.rstrip('/')
        
        return url
        
    async def clone_website_complete(self, target_url: str) -> CloningResult:
        """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„"""
        
        # ØªÙ†Ø¸ÙŠÙ ÙˆØªØµØ­ÙŠØ­ Ø§Ù„URL
        target_url = self._clean_url(target_url)
        self.config.target_url = target_url
        self.result.start_time = datetime.now()
        
        self.logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù…ÙˆÙ‚Ø¹: {target_url}")
        
        try:
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ø¶ÙŠØ± ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø©
            await self._phase_1_preparation()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ
            await self._phase_2_discovery()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„
            await self._phase_3_comprehensive_extraction()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª
            await self._phase_4_content_analysis()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ø®ÙÙŠ
            await self._phase_5_advanced_extraction()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
            await self._phase_6_ai_analysis()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            await self._phase_7_create_replica()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 8: Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
            await self._phase_8_quality_assurance()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 9: Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„Ø©
            await self._phase_9_comprehensive_reporting()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 10: Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„ØªØ³Ù„ÙŠÙ…
            await self._phase_10_final_organization()
            
            self.result.success = True
            self.logger.info("âœ… ØªÙ… Ø§ÙƒØªÙ…Ø§Ù„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            self.result.success = False
            self.result.error_log.append(f"Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø®: {str(e)}")
            self.logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø®: {e}", exc_info=True)
            
        finally:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
            await self._cleanup_resources()
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            self.result.end_time = datetime.now()
            self.result.duration = (self.result.end_time - self.result.start_time).total_seconds()
            
        return self.result
    
    async def _phase_1_preparation(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ø¶ÙŠØ± ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø©"""
        self.logger.info("ğŸ“‹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ø¶ÙŠØ± ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø©")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.result.output_path = os.path.join(
            self.config.output_directory,
            f"{urlparse(self.config.target_url).netloc}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        os.makedirs(self.result.output_path, exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "01_extracted_content"), exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "02_assets"), exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "03_source_code"), exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "04_analysis"), exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "05_cloned_site"), exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "06_reports"), exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "07_databases"), exist_ok=True)
        os.makedirs(os.path.join(self.result.output_path, "08_apis"), exist_ok=True)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø© HTTP
        connector = aiohttp.TCPConnector(
            limit=100,
            limit_per_host=20,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'User-Agent': self._get_user_agent()}
        )
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØªØµÙØ­ Selenium Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
        if SELENIUM_AVAILABLE and self.config.handle_javascript:
            await self._setup_selenium()
            
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ±")
        
    async def _phase_2_discovery(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ"""
        self.logger.info("ğŸ” Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        main_page_content = await self._fetch_page_content(self.config.target_url)
        if not main_page_content:
            raise Exception("ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ø¹")
            
        # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ Ù„Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        soup = BeautifulSoup(main_page_content, 'html.parser')
        
        # Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹
        site_type = await self._detect_site_type(soup)
        self.result.extracted_content['site_type'] = site_type
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        base_links = await self._extract_all_links(soup, self.config.target_url)
        self.result.extracted_content['discovered_links'] = len(base_links)
        
        # ØªØ­Ù„ÙŠÙ„ robots.txt
        robots_content = await self._analyze_robots_txt()
        self.result.extracted_content['robots_analysis'] = robots_content
        
        # Ø§ÙƒØªØ´Ø§Ù Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹
        sitemap_urls = await self._discover_sitemaps()
        self.result.extracted_content['sitemap_urls'] = sitemap_urls
        
        # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ Ù„Ù„ØªÙ‚Ù†ÙŠØ§Øª
        initial_tech_analysis = await self._initial_technology_detection(soup)
        self.result.technologies_detected = initial_tech_analysis
        
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù")
        
    async def _phase_3_comprehensive_extraction(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„"""
        self.logger.info("ğŸ“¥ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª
        await self._extract_all_pages()
        
        # ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„
        await self._download_all_assets()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        if self.config.extract_dynamic_content:
            await self._extract_dynamic_content()
            
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ
        if self.config.extract_hidden_content:
            await self._extract_hidden_content()
            
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„")
        
    async def _phase_4_content_analysis(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª"""
        self.logger.info("ğŸ”¬ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª")
        
        # ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
        comprehensive_tech = await self._comprehensive_technology_analysis()
        self.result.technologies_detected.update(comprehensive_tech)
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹
        structure_analysis = await self._analyze_site_structure()
        self.result.extracted_content['structure_analysis'] = structure_analysis
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ø§Ù†
        security_analysis = await self._comprehensive_security_analysis()
        self.result.security_analysis = security_analysis
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        performance_analysis = await self._performance_analysis()
        self.result.performance_metrics = performance_analysis
        
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø±Ø­Ù„Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
        
    async def _phase_5_advanced_extraction(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ø®ÙÙŠ"""
        self.logger.info("ğŸ¯ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ APIs ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        if self.config.extract_apis:
            api_endpoints = await self._extract_api_endpoints()
            self.result.extracted_content['api_endpoints'] = api_endpoints
            
        # ØªØ­Ù„ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        if self.config.extract_database_structure:
            db_structure = await self._analyze_database_structure()
            self.result.extracted_content['database_structure'] = db_structure
            
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ
        if self.config.extract_source_code:
            source_code = await self._extract_source_code()
            self.result.extracted_content['source_code_analysis'] = source_code
            
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª ÙˆØ§Ù„ÙˆØ¸Ø§Ø¦Ù
        interactions = await self._analyze_interactions()
        self.result.extracted_content['interactions'] = interactions
        
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        
    async def _phase_6_ai_analysis(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        if not self.config.analyze_with_ai:
            return
            
        self.logger.info("ğŸ¤– Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        pattern_analysis = await self._ai_pattern_analysis()
        self.result.ai_analysis['patterns'] = pattern_analysis
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØºØ±Ø¶ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰
        content_analysis = await self._ai_content_analysis()
        self.result.ai_analysis['content_analysis'] = content_analysis
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
        optimization_recommendations = await self._ai_optimization_analysis()
        self.result.ai_analysis['optimization'] = optimization_recommendations
        
        # ØªØ­Ù„ÙŠÙ„ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        ux_analysis = await self._ai_ux_analysis()
        self.result.ai_analysis['ux_analysis'] = ux_analysis
        
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
        
    async def _phase_7_create_replica(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        self.logger.info("ğŸ”„ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©")
        
        if self.config.create_identical_copy:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚
            replica_structure = await self._create_replica_structure()
            
            # Ù†Ø³Ø® ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
            await self._copy_and_modify_files()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡
            await self._create_routing_system()
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
            await self._setup_local_database()
            
            self.result.cloned_site_path = os.path.join(self.result.output_path, "05_cloned_site")
            
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©")
        
    async def _phase_8_quality_assurance(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 8: Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
        self.logger.info("ğŸ” Ø§Ù„Ù…Ø±Ø­Ù„Ø© 8: Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        broken_links = await self._test_all_links()
        self.result.extracted_content['broken_links'] = broken_links
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£ØµÙˆÙ„
        missing_assets = await self._verify_assets()
        self.result.extracted_content['missing_assets'] = missing_assets
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
        performance_comparison = await self._compare_performance()
        self.result.performance_metrics['comparison'] = performance_comparison
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        quality_score = await self._calculate_quality_score()
        self.result.extracted_content['quality_score'] = quality_score
        
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø±Ø­Ù„Ø© Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©")
        
    async def _phase_9_comprehensive_reporting(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 9: Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        self.logger.info("ğŸ“Š Ø§Ù„Ù…Ø±Ø­Ù„Ø© 9: Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±")
        
        self.result.reports_path = os.path.join(self.result.output_path, "06_reports")
        
        # ØªÙ‚Ø±ÙŠØ± HTML ØªÙØ§Ø¹Ù„ÙŠ
        if 'html' in self.config.export_formats:
            await self._generate_html_report()
            
        # ØªÙ‚Ø±ÙŠØ± JSON ØªÙØµÙŠÙ„ÙŠ
        if 'json' in self.config.export_formats:
            await self._generate_json_report()
            
        # ØªÙ‚Ø±ÙŠØ± CSV Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if 'csv' in self.config.export_formats:
            await self._generate_csv_reports()
            
        # ØªÙ‚Ø±ÙŠØ± PDF
        if 'pdf' in self.config.export_formats and REPORTLAB_AVAILABLE:
            await self._generate_pdf_report()
            
        # ØªÙ‚Ø±ÙŠØ± Word
        if 'docx' in self.config.export_formats and DOCX_AVAILABLE:
            await self._generate_docx_report()
            
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±")
        
    async def _phase_10_final_organization(self):
        """Ø§Ù„Ù…Ø±Ø­Ù„Ø© 10: Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„ØªØ³Ù„ÙŠÙ…"""
        self.logger.info("ğŸ“ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 10: Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
        await self._create_project_guide()
        
        # Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
        if self.config.compress_output:
            await self._compress_output()
            
        # Ø¥Ù†Ø´Ø§Ø¡ checksums Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        await self._generate_checksums()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù README Ø´Ø§Ù…Ù„
        await self._create_readme_file()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        await self._calculate_final_statistics()
        
        self.logger.info("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")

    # ==================== Helper Methods ====================
    
    def _get_user_agent(self) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ User Agent Ù…ØªÙ†Ø§ÙˆØ¨"""
        if self.config.rotate_user_agents:
            ua = self.user_agents[self.current_user_agent_index]
            self.current_user_agent_index = (self.current_user_agent_index + 1) % len(self.user_agents)
            return ua
        return self.user_agents[0]
        
    async def _fetch_page_content(self, url: str, use_js: bool = False) -> Optional[str]:
        """Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ù…Ø¹ Ø¯Ø¹Ù… JavaScript"""
        try:
            if use_js and self.selenium_driver:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Selenium Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
                self.selenium_driver.get(url)
                await asyncio.sleep(3)  # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                return self.selenium_driver.page_source
            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… aiohttp Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø«Ø§Ø¨Øª
                if self.session:
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            return await response.text()
                        else:
                            self.logger.warning(f"ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ {url}: {response.status}")
                            return None
                else:
                    return None
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ {url}: {e}")
            self.error_count += 1
            return None
            
    async def _setup_selenium(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØªØµÙØ­ Selenium"""
        try:
            options = ChromeOptions()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument(f'--user-agent={self._get_user_agent()}')
            
            self.selenium_driver = webdriver.Chrome(options=options)
            self.logger.info("ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Selenium Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            self.logger.warning(f"ÙØ´Ù„ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Selenium: {e}")
            self.selenium_driver = None
            
    async def _cleanup_resources(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        if self.session:
            await self.session.close()
            
        if self.selenium_driver:
            self.selenium_driver.quit()

    # ==================== Core Implementation Methods ====================
    
    async def _detect_site_type(self, soup: BeautifulSoup) -> str:
        """ÙƒØ´Ù Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰
        if soup.find('meta', attrs={'name': 'generator', 'content': re.compile(r'wordpress', re.I)}):
            return "WordPress Blog/Site"
        elif soup.find('script', src=re.compile(r'wp-content|wp-includes')):
            return "WordPress Site"
        elif soup.find('div', class_='shopify-section'):
            return "Shopify E-commerce"
        elif soup.find('form', action=re.compile(r'cart|checkout')):
            return "E-commerce Site"
        elif soup.find('article') or soup.find('div', class_=re.compile(r'blog|post')):
            return "Blog/News Site"
        elif soup.find('div', class_=re.compile(r'portfolio|gallery')):
            return "Portfolio/Gallery"
        elif soup.find('form', method='post') and soup.find('input', type='email'):
            return "Business/Contact Site"
        elif soup.find('video') or soup.find('iframe', src=re.compile(r'youtube|vimeo')):
            return "Media/Entertainment"
        elif soup.find('div', class_=re.compile(r'course|lesson|education')):
            return "Educational Site"
        else:
            return "Business/Corporate Site"
    
    async def _extract_all_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        links = set()
        parsed_base = urlparse(base_url)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· <a>
        for link in soup.find_all('a'):
            if isinstance(link, Tag):
                href_attr = link.get('href')
                if href_attr:
                    href = str(href_attr).strip()
                    if href:
                        full_url = urljoin(base_url, href)
                        if self._is_valid_internal_url(full_url, parsed_base.netloc):
                            links.add(full_url)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ù…Ù† JavaScript
        for script in soup.find_all('script'):
            script_text = script.get_text()
            if script_text:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„ÙƒÙˆØ¯
                js_links = re.findall(r'["\']([^"\']*\.(?:html|php|asp|jsp)[^"\']*)["\']', script_text)
                for js_link in js_links:
                    full_url = urljoin(base_url, js_link)
                    if self._is_valid_internal_url(full_url, parsed_base.netloc):
                        links.add(full_url)
        
        return list(links)
    
    def _is_valid_url(self, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø·"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and parsed.scheme in ['http', 'https']
        except:
            return False
    
    def _is_valid_internal_url(self, url: str, base_domain: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ ÙˆØµØ­ÙŠØ­"""
        try:
            parsed = urlparse(url)
            if not parsed.scheme in ['http', 'https']:
                return False
            if not parsed.netloc:
                return True  # relative URL
            return parsed.netloc == base_domain or parsed.netloc.endswith('.' + base_domain)
        except:
            return False
    
    async def _analyze_robots_txt(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù robots.txt"""
        robots_url = urljoin(self.config.target_url, '/robots.txt')
        try:
            async with self.session.get(robots_url) as response:
                if response.status == 200:
                    content = await response.text()
                    return {
                        'exists': True,
                        'content': content,
                        'disallowed_paths': re.findall(r'Disallow:\s*(.+)', content),
                        'allowed_paths': re.findall(r'Allow:\s*(.+)', content),
                        'sitemaps': re.findall(r'Sitemap:\s*(.+)', content)
                    }
        except:
            pass
        return {'exists': False}
    
    async def _discover_sitemaps(self) -> List[str]:
        """Ø§ÙƒØªØ´Ø§Ù Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        potential_sitemaps = [
            '/sitemap.xml',
            '/sitemap_index.xml',
            '/sitemap.php',
            '/sitemaps.xml',
            '/sitemap1.xml'
        ]
        
        found_sitemaps = []
        for sitemap_path in potential_sitemaps:
            sitemap_url = urljoin(self.config.target_url, sitemap_path)
            try:
                async with self.session.get(sitemap_url) as response:
                    if response.status == 200:
                        found_sitemaps.append(sitemap_url)
            except:
                continue
        
        return found_sitemaps
    
    async def _initial_technology_detection(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø£ÙˆÙ„ÙŠ Ø¹Ù† Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"""
        technologies = {
            'frameworks': [],
            'cms': [],
            'analytics': [],
            'javascript_libraries': [],
            'css_frameworks': [],
            'meta_info': {}
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
        for meta in soup.find_all('meta'):
            name_attr = meta.get('name')
            property_attr = meta.get('property')
            content_attr = meta.get('content')
            
            if name_attr == 'generator' and content_attr:
                technologies['meta_info']['generator'] = str(content_attr)
            elif property_attr == 'og:type' and content_attr:
                technologies['meta_info']['og_type'] = str(content_attr)
        
        # ÙƒØ´Ù Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª
        for script in soup.find_all('script'):
            src_attr = script.get('src')
            if src_attr:
                src = str(src_attr).lower()
                if 'jquery' in src:
                    technologies['javascript_libraries'].append('jQuery')
                elif 'react' in src:
                    technologies['frameworks'].append('React')
                elif 'vue' in src:
                    technologies['frameworks'].append('Vue.js')
                elif 'angular' in src:
                    technologies['frameworks'].append('Angular')
                elif 'bootstrap' in src:
                    technologies['css_frameworks'].append('Bootstrap')
                elif 'google-analytics' in src or 'gtag' in src:
                    technologies['analytics'].append('Google Analytics')
        
        # ÙƒØ´Ù CSS frameworks
        for link in soup.find_all('link'):
            rel_attr = link.get('rel')
            href_attr = link.get('href')
            if rel_attr and href_attr and 'stylesheet' in str(rel_attr):
                href = str(href_attr).lower()
                if 'bootstrap' in href:
                    technologies['css_frameworks'].append('Bootstrap')
                elif 'foundation' in href:
                    technologies['css_frameworks'].append('Foundation')
                elif 'bulma' in href:
                    technologies['css_frameworks'].append('Bulma')
        
        return technologies
    
    async def _extract_all_pages(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        urls_to_process = [self.config.target_url]
        processed_urls = set()
        depth = 0
        
        while urls_to_process and depth < self.config.max_depth and len(processed_urls) < self.config.max_pages:
            current_batch = urls_to_process.copy()
            urls_to_process.clear()
            
            for url in current_batch:
                if url in processed_urls:
                    continue
                    
                self.logger.info(f"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø©: {url}")
                
                # Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
                content = await self._fetch_page_content(url, use_js=self.config.handle_javascript)
                if content:
                    # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                    page_filename = self._url_to_filename(url) + '.html'
                    page_path = os.path.join(self.result.output_path, "01_extracted_content", page_filename)
                    
                    async with aiofiles.open(page_path, 'w', encoding='utf-8') as f:
                        await f.write(content)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                    soup = BeautifulSoup(content, 'html.parser')
                    new_links = await self._extract_all_links(soup, url)
                    
                    for new_link in new_links:
                        if new_link not in processed_urls and new_link not in current_batch:
                            urls_to_process.append(new_link)
                    
                    processed_urls.add(url)
                    self.result.pages_extracted += 1
                    
                await asyncio.sleep(self.config.delay_between_requests)
            
            depth += 1
    
    def _url_to_filename(self, url: str) -> str:
        """ØªØ­ÙˆÙŠÙ„ URL Ø¥Ù„Ù‰ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù†"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        if not path:
            path = 'index'
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        filename = re.sub(r'[^\w\-_\.]', '_', path)
        filename = re.sub(r'_+', '_', filename)
        
        # Ø¥Ø¶Ø§ÙØ© query parameters ÙƒÙ€ suffix
        if parsed.query:
            query_hash = hashlib.md5(parsed.query.encode()).hexdigest()[:8]
            filename += f'_{query_hash}'
        
        return filename
    
    async def _download_all_assets(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„ (ØµÙˆØ±ØŒ CSSØŒ JSØŒ Ø¥Ù„Ø®)"""
        assets_found = set()
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        for html_file in os.listdir(content_dir):
            if html_file.endswith('.html'):
                file_path = os.path.join(content_dir, html_file)
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
                    for img in soup.find_all('img'):
                        if isinstance(img, Tag):
                            src = img.get('src')
                            if src:
                                asset_url = urljoin(self.config.target_url, str(src))
                                assets_found.add(('image', asset_url))
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSS
                    for link in soup.find_all('link'):
                        if isinstance(link, Tag):
                            href = link.get('href')
                            rel = link.get('rel')
                            if href and rel and 'stylesheet' in str(rel):
                                asset_url = urljoin(self.config.target_url, str(href))
                                assets_found.add(('css', asset_url))
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ JavaScript
                    for script in soup.find_all('script'):
                        if isinstance(script, Tag):
                            src = script.get('src')
                            if src:
                                asset_url = urljoin(self.config.target_url, str(src))
                                assets_found.add(('js', asset_url))
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„
        for asset_type, asset_url in assets_found:
            await self._download_asset(asset_type, asset_url)
    
    async def _download_asset(self, asset_type: str, url: str):
        """ØªØ­Ù…ÙŠÙ„ Ø£ØµÙ„ ÙˆØ§Ø­Ø¯"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙˆØ§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
                    if asset_type == 'image':
                        folder = 'images'
                        ext = os.path.splitext(urlparse(url).path)[1] or '.jpg'
                    elif asset_type == 'css':
                        folder = 'css'
                        ext = '.css'
                    elif asset_type == 'js':
                        folder = 'js'
                        ext = '.js'
                    else:
                        folder = 'other'
                        ext = os.path.splitext(urlparse(url).path)[1] or '.bin'
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯
                    asset_dir = os.path.join(self.result.output_path, "02_assets", folder)
                    os.makedirs(asset_dir, exist_ok=True)
                    
                    # Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
                    filename = os.path.basename(urlparse(url).path) or f'asset_{hashlib.md5(url.encode()).hexdigest()[:8]}{ext}'
                    file_path = os.path.join(asset_dir, filename)
                    
                    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
                    async with aiofiles.open(file_path, 'wb') as f:
                        await f.write(content)
                    
                    self.result.assets_downloaded += 1
                    self.result.total_size += len(content)
                    
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙ„ {url}: {e}")
            self.error_count += 1

    # ==================== Advanced Analysis Methods ====================
    
    async def _extract_dynamic_content(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… JavaScript"""
        if not self.selenium_driver:
            return
            
        self.logger.info("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ...")
        
        try:
            self.selenium_driver.get(self.config.target_url)
            await asyncio.sleep(5)  # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
            
            # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±ÙŠØ¨Øª Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ
            dynamic_content = self.selenium_driver.execute_script("""
                return {
                    hiddenElements: Array.from(document.querySelectorAll('[style*="display: none"], [hidden]')).map(el => el.outerHTML),
                    loadedScripts: Array.from(document.scripts).map(s => s.src).filter(s => s),
                    ajaxCalls: window.ajaxCalls || [],
                    dynamicData: window.dynamicData || {}
                };
            """)
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
            dynamic_path = os.path.join(self.result.output_path, "03_source_code", "dynamic_content.json")
            async with aiofiles.open(dynamic_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(dynamic_content, ensure_ascii=False, indent=2))
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ: {e}")
    
    async def _extract_hidden_content(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ ÙˆØ§Ù„Ù…Ø´ÙØ±"""
        self.logger.info("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ...")
        
        hidden_content = {
            'comments': [],
            'hidden_forms': [],
            'encoded_data': [],
            'obfuscated_js': []
        }
        
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        for html_file in os.listdir(content_dir):
            if html_file.endswith('.html'):
                file_path = os.path.join(content_dir, html_file)
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ©
                comments = re.findall(r'<!--(.*?)-->', content, re.DOTALL)
                hidden_content['comments'].extend(comments)
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´ÙØ±Ø©
                encoded_patterns = [
                    r'data:([^;]+);base64,([A-Za-z0-9+/=]+)',
                    r'btoa\(["\']([^"\']+)["\']\)',
                    r'atob\(["\']([^"\']+)["\']\)'
                ]
                
                for pattern in encoded_patterns:
                    matches = re.findall(pattern, content)
                    hidden_content['encoded_data'].extend(matches)
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ
        hidden_path = os.path.join(self.result.output_path, "03_source_code", "hidden_content.json")
        async with aiofiles.open(hidden_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(hidden_content, ensure_ascii=False, indent=2))
    
    async def _comprehensive_technology_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"""
        tech_analysis = {
            'server_info': {},
            'frameworks_detailed': {},
            'database_indicators': [],
            'security_features': [],
            'performance_tools': [],
            'third_party_services': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ headers Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø§Ø¯Ù…
        try:
            async with self.session.head(self.config.target_url) as response:
                headers = dict(response.headers)
                tech_analysis['server_info'] = {
                    'server': headers.get('Server', 'Unknown'),
                    'powered_by': headers.get('X-Powered-By', 'Unknown'),
                    'framework': headers.get('X-Framework', 'Unknown'),
                    'all_headers': headers
                }
        except:
            pass
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… builtwith Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
        try:
            import builtwith
            builtwith_result = builtwith.parse(self.config.target_url)
            tech_analysis['frameworks_detailed'] = builtwith_result
        except ImportError:
            self.logger.warning("Ù…ÙƒØªØ¨Ø© builtwith ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª: {e}")
        
        return tech_analysis
    

    

        try:
            parsed_url = urlparse(self.config.target_url)
            if parsed_url.scheme == 'https':
                # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ØªØ­Ù„ÙŠÙ„ Ø´Ù‡Ø§Ø¯Ø© SSL
                security['ssl_analysis']['enabled'] = True
            else:
                security['ssl_analysis']['enabled'] = False
        except:
            pass
        
        # ØªØ­Ù„ÙŠÙ„ security headers
        try:
            async with self.session.get(self.config.target_url) as response:
                headers = dict(response.headers)
                security['headers_security'] = {
                    'csp': headers.get('Content-Security-Policy'),
                    'xss_protection': headers.get('X-XSS-Protection'),
                    'frame_options': headers.get('X-Frame-Options'),
                    'content_type_options': headers.get('X-Content-Type-Options'),
                    'hsts': headers.get('Strict-Transport-Security')
                }
        except:
            pass
        
        return security
    
    async def _performance_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        performance = {
            'load_times': {},
            'resource_sizes': {},
            'optimization_opportunities': [],
            'caching_analysis': {}
        }
        
        start_time = time.time()
        try:
            async with self.session.get(self.config.target_url) as response:
                load_time = time.time() - start_time
                performance['load_times']['main_page'] = load_time
                performance['resource_sizes']['main_page'] = len(await response.read())
        except:
            pass
        
        return performance
    
    async def _extract_api_endpoints(self) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· API ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª"""
        api_endpoints = []
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ JavaScript files Ø¹Ù† API calls
        js_dir = os.path.join(self.result.output_path, "02_assets", "js")
        if os.path.exists(js_dir):
            for js_file in os.listdir(js_dir):
                if js_file.endswith('.js'):
                    file_path = os.path.join(js_dir, js_file)
                    try:
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            js_content = await f.read()
                            
                        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† API patterns
                        api_patterns = [
                            r'fetch\(["\']([^"\']+)["\']',
                            r'axios\.(?:get|post|put|delete)\(["\']([^"\']+)["\']',
                            r'jQuery\.(?:get|post|ajax)\(["\']([^"\']+)["\']',
                            r'api["\']:\s*["\']([^"\']+)["\']'
                        ]
                        
                        for pattern in api_patterns:
                            matches = re.findall(pattern, js_content)
                            for match in matches:
                                api_endpoints.append({
                                    'url': match,
                                    'source_file': js_file,
                                    'method': 'unknown'
                                })
                    except:
                        continue
        
        return api_endpoints
    
    async def _analyze_database_structure(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©"""
        db_structure = {
            'detected_queries': [],
            'table_references': [],
            'connection_strings': [],
            'orm_patterns': []
        }
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„ÙØ§Øª JavaScript Ùˆ HTML Ø¹Ù† database patterns
        all_files = []
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ù„ÙØ§Øª HTML
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        if os.path.exists(content_dir):
            all_files.extend([(f, 'html') for f in os.listdir(content_dir) if f.endswith('.html')])
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ù„ÙØ§Øª JS
        js_dir = os.path.join(self.result.output_path, "02_assets", "js")
        if os.path.exists(js_dir):
            all_files.extend([(f, 'js') for f in os.listdir(js_dir) if f.endswith('.js')])
        
        for filename, file_type in all_files:
            if file_type == 'html':
                file_path = os.path.join(content_dir, filename)
            else:
                file_path = os.path.join(js_dir, filename)
                
            try:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† SQL patterns
                sql_patterns = [
                    r'SELECT\s+.+\s+FROM\s+(\w+)',
                    r'INSERT\s+INTO\s+(\w+)',
                    r'UPDATE\s+(\w+)\s+SET',
                    r'DELETE\s+FROM\s+(\w+)'
                ]
                
                for pattern in sql_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        if match not in db_structure['table_references']:
                            db_structure['table_references'].append(match)
            except:
                continue
        
        return db_structure
    
    async def _extract_source_code(self) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ"""
        source_analysis = {
            'html_structure': {},
            'css_analysis': {},
            'javascript_functions': [],
            'embedded_code': {},
            'code_complexity': {}
        }
        
        # ØªØ­Ù„ÙŠÙ„ HTML
        content_dir = os.path.join(self.result.output_path, "01_extracted_content") 
        html_files = [f for f in os.listdir(content_dir) if f.endswith('.html')]
        
        source_analysis['html_structure']['total_files'] = len(html_files)
        source_analysis['html_structure']['average_size'] = 0
        
        total_size = 0
        for html_file in html_files:
            file_path = os.path.join(content_dir, html_file)
            size = os.path.getsize(file_path)
            total_size += size
        
        if html_files:
            source_analysis['html_structure']['average_size'] = total_size // len(html_files)
        
        # ØªØ­Ù„ÙŠÙ„ CSS
        css_dir = os.path.join(self.result.output_path, "02_assets", "css")
        if os.path.exists(css_dir):
            css_files = [f for f in os.listdir(css_dir) if f.endswith('.css')]
            source_analysis['css_analysis']['total_files'] = len(css_files)
            
            # ØªØ­Ù„ÙŠÙ„ selectors ÙÙŠ CSS
            all_selectors = set()
            for css_file in css_files:
                file_path = os.path.join(css_dir, css_file)
                try:
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        css_content = await f.read()
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSS selectors
                    selectors = re.findall(r'([.#]?[\w-]+)\s*{', css_content)
                    all_selectors.update(selectors)
                except:
                    continue
            
            source_analysis['css_analysis']['unique_selectors'] = len(all_selectors)
        
        # ØªØ­Ù„ÙŠÙ„ JavaScript
        js_dir = os.path.join(self.result.output_path, "02_assets", "js")
        if os.path.exists(js_dir):
            js_files = [f for f in os.listdir(js_dir) if f.endswith('.js')]
            
            for js_file in js_files:
                file_path = os.path.join(js_dir, js_file)
                try:
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        js_content = await f.read()
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ JavaScript functions
                    functions = re.findall(r'function\s+(\w+)\s*\(', js_content)
                    source_analysis['javascript_functions'].extend(functions)
                except:
                    continue
        
        return source_analysis
    
    async def _analyze_interactions(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª ÙˆØ§Ù„ÙˆØ¸Ø§Ø¦Ù"""
        interactions = {
            'form_interactions': [],
            'click_handlers': [],
            'ajax_interactions': [],
            'user_inputs': [],
            'dynamic_behaviors': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        for html_file in os.listdir(content_dir):
            if html_file.endswith('.html'):
                file_path = os.path.join(content_dir, html_file)
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    soup = BeautifulSoup(content, 'html.parser')
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
                for form in soup.find_all('form'):
                    form_data = {
                        'action': str(form.get('action', '')),
                        'method': str(form.get('method', 'GET')),
                        'inputs': []
                    }
                    
                    for input_tag in form.find_all(['input', 'textarea', 'select']):
                        input_data = {
                            'type': str(input_tag.get('type', 'text')),
                            'name': str(input_tag.get('name', '')),
                            'required': input_tag.has_attr('required')
                        }
                        form_data['inputs'].append(input_data)
                    
                    interactions['form_interactions'].append(form_data)
                
                # ØªØ­Ù„ÙŠÙ„ click handlers
                for element in soup.find_all(attrs={'onclick': True}):
                    onclick_attr = element.get('onclick')
                    if onclick_attr:
                        interactions['click_handlers'].append({
                            'element': str(element.name) if element.name else 'unknown',
                            'onclick': str(onclick_attr)
                        })
        
        return interactions

    # ==================== AI Analysis Methods ====================
    
    async def _ai_pattern_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        patterns = {
            'design_patterns': [],
            'ui_components': [],
            'navigation_patterns': [],
            'content_structures': [],
            'interactive_elements': [],
            'responsive_design': {},
            'accessibility_features': [],
            'performance_patterns': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØµÙ…ÙŠÙ…
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        if os.path.exists(content_dir):
            for html_file in os.listdir(content_dir):
                if html_file.endswith('.html'):
                    file_path = os.path.join(content_dir, html_file)
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # ØªØ­Ù„ÙŠÙ„ Ù…ÙƒÙˆÙ†Ø§Øª UI
                        ui_components = self._analyze_ui_components(soup)
                        patterns['ui_components'].extend(ui_components)
                        
                        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ù‚Ù„
                        nav_patterns = self._analyze_navigation_patterns(soup)
                        patterns['navigation_patterns'].extend(nav_patterns)
                        
                        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
                        interactive = self._analyze_interactive_elements(soup)
                        patterns['interactive_elements'].extend(interactive)
        
        return patterns
    
    def _analyze_ui_components(self, soup: BeautifulSoup) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ù…ÙƒÙˆÙ†Ø§Øª ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        components = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ÙƒÙˆÙ†Ø§Øª Ø´Ø§Ø¦Ø¹Ø©
        if soup.find('nav'):
            components.append('navigation_bar')
        if soup.find('header'):
            components.append('header_section')
        if soup.find('footer'):
            components.append('footer_section')
        if soup.find('aside') or soup.find('div', class_=re.compile(r'sidebar|side-menu')):
            components.append('sidebar')
        if soup.find('div', class_=re.compile(r'carousel|slider|slideshow')):
            components.append('image_carousel')
        if soup.find('div', class_=re.compile(r'modal|popup|dialog')):
            components.append('modal_dialog')
        if soup.find('table') or soup.find('div', class_=re.compile(r'table|grid')):
            components.append('data_table')
        if soup.find('form'):
            components.append('form_element')
        if soup.find('div', class_=re.compile(r'accordion|collapse')):
            components.append('accordion')
        if soup.find('div', class_=re.compile(r'tab|tabbed')):
            components.append('tabs')
        
        return components
    
    def _analyze_navigation_patterns(self, soup: BeautifulSoup) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ù‚Ù„"""
        patterns = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ù‚Ù„
        nav_elements = soup.find_all(['nav', 'div'], class_=re.compile(r'nav|menu'))
        for nav in nav_elements:
            # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ†Ù‚Ù„
            if nav.find('ul'):
                if nav.find('ul').find('ul'):  # Ù‚Ø§Ø¦Ù…Ø© Ù…ØªØ¯Ø§Ø®Ù„Ø©
                    patterns.append('dropdown_menu')
                else:
                    patterns.append('horizontal_menu')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† breadcrumbs
            if 'breadcrumb' in str(nav.get('class', [])).lower():
                patterns.append('breadcrumb_navigation')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† pagination
            if 'pag' in str(nav.get('class', [])).lower():
                patterns.append('pagination')
        
        return patterns
    
    def _analyze_interactive_elements(self, soup: BeautifulSoup) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©"""
        elements = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± ØªÙØ§Ø¹Ù„ÙŠØ©
        if soup.find(attrs={'onclick': True}):
            elements.append('click_handlers')
        if soup.find('button') or soup.find('input', type='button'):
            elements.append('buttons')
        if soup.find('input', type='text') or soup.find('textarea'):
            elements.append('text_inputs')
        if soup.find('select'):
            elements.append('dropdown_selects')
        if soup.find('input', type='checkbox') or soup.find('input', type='radio'):
            elements.append('form_controls')
        if soup.find(attrs={'data-toggle': True}) or soup.find(attrs={'data-target': True}):
            elements.append('bootstrap_interactions')
        
        return elements
    
    async def _ai_content_optimization(self) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        optimization = {
            'seo_recommendations': [],
            'performance_improvements': [],
            'accessibility_fixes': [],
            'code_quality_issues': [],
            'security_enhancements': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ SEO
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        if os.path.exists(content_dir):
            for html_file in os.listdir(content_dir):
                if html_file.endswith('.html'):
                    file_path = os.path.join(content_dir, html_file)
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # ØªØ­Ù„ÙŠÙ„ SEO
                        seo_issues = self._analyze_seo_issues(soup)
                        optimization['seo_recommendations'].extend(seo_issues)
                        
                        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
                        performance_issues = self._analyze_performance_issues(soup)
                        optimization['performance_improvements'].extend(performance_issues)
                        
                        # ØªØ­Ù„ÙŠÙ„ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„
                        accessibility_issues = self._analyze_accessibility_issues(soup)
                        optimization['accessibility_fixes'].extend(accessibility_issues)
        
        return optimization
    
    def _analyze_seo_issues(self, soup: BeautifulSoup) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§ÙƒÙ„ SEO"""
        issues = []
        
        # ÙØ­Øµ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        title = soup.find('title')
        if not title or not title.get_text().strip():
            issues.append('Missing or empty title tag')
        elif len(title.get_text()) > 60:
            issues.append('Title tag too long (>60 characters)')
        
        # ÙØ­Øµ meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc:
            issues.append('Missing meta description')
        elif len(meta_desc.get('content', '')) > 160:
            issues.append('Meta description too long (>160 characters)')
        
        # ÙØ­Øµ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        h1_tags = soup.find_all('h1')
        if len(h1_tags) == 0:
            issues.append('Missing H1 tag')
        elif len(h1_tags) > 1:
            issues.append('Multiple H1 tags found')
        
        # ÙØ­Øµ Ø§Ù„ØµÙˆØ±
        images = soup.find_all('img')
        for img in images:
            if not img.get('alt'):
                issues.append('Image missing alt attribute')
        
        return issues
    
    def _analyze_performance_issues(self, soup: BeautifulSoup) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        issues = []
        
        # ÙØ­Øµ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        images = soup.find_all('img')
        if len(images) > 20:
            issues.append('Too many images on page (>20)')
        
        # ÙØ­Øµ Ù…Ù„ÙØ§Øª CSS Ùˆ JS
        css_files = soup.find_all('link', rel='stylesheet')
        if len(css_files) > 5:
            issues.append('Too many CSS files (>5)')
        
        scripts = soup.find_all('script', src=True)
        if len(scripts) > 10:
            issues.append('Too many JavaScript files (>10)')
        
        # ÙØ­Øµ inline styles
        inline_styles = soup.find_all(attrs={'style': True})
        if len(inline_styles) > 10:
            issues.append('Too many inline styles')
        
        return issues
    
    def _analyze_accessibility_issues(self, soup: BeautifulSoup) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§ÙƒÙ„ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„"""
        issues = []
        
        # ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        links = soup.find_all('a')
        for link in links:
            href = link.get('href')
            text = link.get_text().strip()
            if href and not text:
                issues.append('Link without text content')
        
        # ÙØ­Øµ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        inputs = soup.find_all('input')
        for input_tag in inputs:
            if input_tag.get('type') not in ['hidden', 'submit', 'button']:
                if not input_tag.get('label') and not input_tag.get('aria-label'):
                    issues.append('Form input without label')
        
        # ÙØ­Øµ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
        if not soup.find(attrs={'role': True}):
            issues.append('No ARIA roles found')
        
        return issues
    
    async def _ai_ux_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        ux_analysis = {
            'navigation_clarity': 'good',
            'content_accessibility': 'moderate',
            'mobile_friendliness': 'unknown',
            'user_interaction_patterns': [],
            'improvement_suggestions': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ù‚Ù„
        if self.result.extracted_content.get('form_interactions'):
            forms_count = len(self.result.extracted_content['form_interactions'])
            if forms_count > 0:
                ux_analysis['user_interaction_patterns'].append(f"ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {forms_count} Ù†Ù…Ø§Ø°Ø¬ ØªÙØ§Ø¹Ù„ÙŠØ©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        accessibility_issues = 0
        
        for html_file in os.listdir(content_dir):
            if html_file.endswith('.html'):
                file_path = os.path.join(content_dir, html_file)
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    soup = BeautifulSoup(content, 'html.parser')
                
                # ÙØ­Øµ Ø§Ù„ØµÙˆØ± Ø¨Ø¯ÙˆÙ† alt text
                images_without_alt = soup.find_all('img', alt=False)
                accessibility_issues += len(images_without_alt)
                
                # ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø¯ÙˆÙ† Ù†Øµ ÙˆØµÙÙŠ
                empty_links = soup.find_all('a', string=False)
                accessibility_issues += len(empty_links)
        
        if accessibility_issues > 0:
            ux_analysis['improvement_suggestions'].append(f"Ø¥Ø¶Ø§ÙØ© Ù†ØµÙˆØµ ÙˆØµÙÙŠØ© Ù„Ù€ {accessibility_issues} Ø¹Ù†ØµØ±")
        
        return ux_analysis

    # ==================== Website Replication Methods ====================
    
    async def _create_replica_structure(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚"""
        self.logger.info("Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚...")
        
        replica_dir = os.path.join(self.result.output_path, "05_cloned_site")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        subdirs = ['assets', 'css', 'js', 'images', 'pages', 'data']
        for subdir in subdirs:
            os.makedirs(os.path.join(replica_dir, subdir), exist_ok=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù index.html Ø£Ø³Ø§Ø³ÙŠ
        index_content = """<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ù…ÙˆÙ‚Ø¹ Ù…Ø³ØªÙ†Ø³Ø®</title>
    <link rel="stylesheet" href="css/main.css">
</head>
<body>
    <div id="app">
        <h1>Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®</h1>
        <p>ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© Website Cloner Pro</p>
    </div>
    <script src="js/main.js"></script>
</body>
</html>"""
        
        async with aiofiles.open(os.path.join(replica_dir, 'index.html'), 'w', encoding='utf-8') as f:
            await f.write(index_content)
    
    async def _copy_and_modify_files(self):
        """Ù†Ø³Ø® ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚"""
        self.logger.info("Ù†Ø³Ø® ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        replica_dir = os.path.join(self.result.output_path, "05_cloned_site")
        
        # Ù†Ø³Ø® Ù…Ù„ÙØ§Øª HTML
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        if os.path.exists(content_dir):
            for html_file in os.listdir(content_dir):
                if html_file.endswith('.html'):
                    src_path = os.path.join(content_dir, html_file)
                    dst_path = os.path.join(replica_dir, 'pages', html_file)
                    
                    # Ù‚Ø±Ø§Ø¡Ø© ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                    async with aiofiles.open(src_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                    
                    # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ù„ØªÙƒÙˆÙ† Ù†Ø³Ø¨ÙŠØ©
                    modified_content = await self._modify_paths_in_html(content)
                    
                    async with aiofiles.open(dst_path, 'w', encoding='utf-8') as f:
                        await f.write(modified_content)
        
        # Ù†Ø³Ø® Ø§Ù„Ø£ØµÙˆÙ„
        assets_dir = os.path.join(self.result.output_path, "02_assets")
        if os.path.exists(assets_dir):
            await self._copy_assets_recursively(assets_dir, replica_dir)
    
    async def _modify_paths_in_html(self, html_content: str) -> str:
        """ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ÙÙŠ HTML Ù„ØªÙƒÙˆÙ† Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ØªØ¹Ø¯ÙŠÙ„ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ±
        for img in soup.find_all('img'):
            src_attr = img.get('src')
            if src_attr:
                original_src = str(src_attr)
                if not original_src.startswith(('http://', 'https://', '//')):
                    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø± Ù†Ø³Ø¨ÙŠ
                    filename = os.path.basename(original_src)
                    img['src'] = f'../images/{filename}'
        
        # ØªØ¹Ø¯ÙŠÙ„ Ù…Ø³Ø§Ø±Ø§Øª CSS
        for link in soup.find_all('link'):
            href_attr = link.get('href')
            rel_attr = link.get('rel')
            if href_attr and rel_attr and 'stylesheet' in str(rel_attr):
                original_href = str(href_attr)
                if not original_href.startswith(('http://', 'https://', '//')):
                    filename = os.path.basename(original_href)
                    link['href'] = f'../css/{filename}'
        
        # ØªØ¹Ø¯ÙŠÙ„ Ù…Ø³Ø§Ø±Ø§Øª JavaScript
        for script in soup.find_all('script'):
            src_attr = script.get('src')
            if src_attr:
                original_src = str(src_attr)
                if not original_src.startswith(('http://', 'https://', '//')):
                    filename = os.path.basename(original_src)
                    script['src'] = f'../js/{filename}'
        
        return str(soup)
    
    async def _copy_assets_recursively(self, src_dir: str, dst_dir: str):
        """Ù†Ø³Ø® Ø§Ù„Ø£ØµÙˆÙ„ Ø¨Ø´ÙƒÙ„ ØªÙƒØ±Ø§Ø±ÙŠ"""
        for root, dirs, files in os.walk(src_dir):
            for file in files:
                src_file = os.path.join(root, file)
                
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙˆØ¬Ù‡Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
                if file.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg')):
                    dst_subdir = 'images'
                elif file.endswith('.css'):
                    dst_subdir = 'css'
                elif file.endswith('.js'):
                    dst_subdir = 'js'
                else:
                    dst_subdir = 'assets'
                
                dst_file = os.path.join(dst_dir, dst_subdir, file)
                
                try:
                    shutil.copy2(src_file, dst_file)
                except Exception as e:
                    self.logger.warning(f"ÙØ´Ù„ ÙÙŠ Ù†Ø³Ø® {src_file}: {e}")
    
    async def _create_routing_system(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚"""
        self.logger.info("Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡...")
        
        replica_dir = os.path.join(self.result.output_path, "05_cloned_site")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù .htaccess Ù„Ù„ØªÙˆØ¬ÙŠÙ‡
        htaccess_content = """RewriteEngine On
RewriteCond %{REQUEST_FILENAME} !-f
RewriteCond %{REQUEST_FILENAME} !-d
RewriteRule ^(.*)$ index.php?route=$1 [QSA,L]

# ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø¶ØºØ·
<IfModule mod_deflate.c>
    AddOutputFilterByType DEFLATE text/plain
    AddOutputFilterByType DEFLATE text/html
    AddOutputFilterByType DEFLATE text/xml
    AddOutputFilterByType DEFLATE text/css
    AddOutputFilterByType DEFLATE application/xml
    AddOutputFilterByType DEFLATE application/xhtml+xml
    AddOutputFilterByType DEFLATE application/rss+xml
    AddOutputFilterByType DEFLATE application/javascript
    AddOutputFilterByType DEFLATE application/x-javascript
</IfModule>

# ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
<IfModule mod_expires.c>
    ExpiresActive On
    ExpiresByType image/jpg "access plus 1 month"
    ExpiresByType image/jpeg "access plus 1 month"
    ExpiresByType image/gif "access plus 1 month"
    ExpiresByType image/png "access plus 1 month"
    ExpiresByType text/css "access plus 1 month"
    ExpiresByType application/pdf "access plus 1 month"
    ExpiresByType application/javascript "access plus 1 month"
    ExpiresByType application/x-javascript "access plus 1 month"
    ExpiresByType application/x-shockwave-flash "access plus 1 month"
    ExpiresByType image/x-icon "access plus 1 year"
    ExpiresDefault "access plus 2 days"
</IfModule>"""
        
        async with aiofiles.open(os.path.join(replica_dir, '.htaccess'), 'w', encoding='utf-8') as f:
            await f.write(htaccess_content)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù router.php Ø£Ø³Ø§Ø³ÙŠ
        php_router = """<?php
// Simple PHP Router for Cloned Website
$route = $_GET['route'] ?? '';

// ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø±
$route = trim($route, '/');
if (empty($route)) {
    $route = 'index';
}

// ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
$page_file = 'pages/' . $route . '.html';

if (file_exists($page_file)) {
    // Ø¹Ø±Ø¶ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    include $page_file;
} else {
    // Ø¹Ø±Ø¶ ØµÙØ­Ø© 404
    http_response_code(404);
    echo '<h1>404 - Ø§Ù„ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©</h1>';
    echo '<p>Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©.</p>';
    echo '<a href="/">Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©</a>';
}
?>"""
        
        async with aiofiles.open(os.path.join(replica_dir, 'router.php'), 'w', encoding='utf-8') as f:
            await f.write(php_router)
    
    async def _setup_local_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ù„ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚"""
        self.logger.info("Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©...")
        
        db_dir = os.path.join(self.result.output_path, "07_databases")
        db_file = os.path.join(db_dir, "cloned_site.db")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite
        conn = sqlite3.connect(db_file)
        cursor = conn.cursor()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ø£Ø³Ø§Ø³ÙŠØ©
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE,
                title TEXT,
                content TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS assets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                type TEXT,
                url TEXT,
                local_path TEXT,
                size INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                analysis_type TEXT,
                results TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        if os.path.exists(content_dir):
            for html_file in os.listdir(content_dir):
                if html_file.endswith('.html'):
                    file_path = os.path.join(content_dir, html_file)
                    
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    title = soup.find('title')
                    title_text = title.get_text() if title else html_file
                    
                    cursor.execute('''
                        INSERT OR REPLACE INTO pages (url, title, content)
                        VALUES (?, ?, ?)
                    ''', (html_file, title_text, content))
        
        conn.commit()
        conn.close()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        db_config = f"""<?php
// Database Configuration for Cloned Website
$database_config = [
    'type' => 'sqlite',
    'path' => 'databases/cloned_site.db',
    'options' => [
        PDO::ATTR_ERRMODE => PDO::ERRMODE_EXCEPTION,
        PDO::ATTR_DEFAULT_FETCH_MODE => PDO::FETCH_ASSOC,
    ]
];

// Database Connection Function
function getDatabase() {{
    global $database_config;
    try {{
        $pdo = new PDO(
            'sqlite:' . $database_config['path'],
            null,
            null,
            $database_config['options']
        );
        return $pdo;
    }} catch (PDOException $e) {{
        error_log('Database connection failed: ' . $e->getMessage());
        return null;
    }}
}}
?>"""
        
        replica_dir = os.path.join(self.result.output_path, "05_cloned_site")
        async with aiofiles.open(os.path.join(replica_dir, 'database.php'), 'w', encoding='utf-8') as f:
            await f.write(db_config)
    
    # ==================== Quality Assurance Methods ====================
    
    async def _test_all_links(self) -> List[str]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ù…Ù„Ù‡Ø§"""
        self.logger.info("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·...")
        
        broken_links = []
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        
        for html_file in os.listdir(content_dir):
            if html_file.endswith('.html'):
                file_path = os.path.join(content_dir, html_file)
                
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                
                soup = BeautifulSoup(content, 'html.parser')
                
                # Ø§Ø®ØªØ¨Ø§Ø± Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ±
                for img in soup.find_all('img'):
                    src_attr = img.get('src')
                    if src_attr:
                        img_url = urljoin(self.config.target_url, str(src_attr))
                        if not await self._test_url(img_url):
                            broken_links.append(f"ØµÙˆØ±Ø© Ù…ÙƒØ³ÙˆØ±Ø©: {img_url}")
                
                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†ØµÙŠØ©
                for link in soup.find_all('a'):
                    href_attr = link.get('href')
                    if href_attr:
                        link_url = urljoin(self.config.target_url, str(href_attr))
                        if link_url.startswith(('http://', 'https://')) and not await self._test_url(link_url):
                            broken_links.append(f"Ø±Ø§Ø¨Ø· Ù…ÙƒØ³ÙˆØ±: {link_url}")
        
        return broken_links
    
    async def _test_url(self, url: str) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯"""
        try:
            async with self.session.head(url) as response:
                return response.status < 400
        except:
            return False
    
    async def _verify_assets(self) -> List[str]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©"""
        missing_assets = []
        assets_dir = os.path.join(self.result.output_path, "02_assets")
        
        if os.path.exists(assets_dir):
            for root, dirs, files in os.walk(assets_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
                    if os.path.getsize(file_path) == 0:
                        missing_assets.append(f"Ù…Ù„Ù ÙØ§Ø±Øº: {file}")
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
                    if file.endswith(('.jpg', '.jpeg', '.png', '.gif')):
                        try:
                            # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ÙØ­Øµ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ù„Ù„ØµÙˆØ± Ù‡Ù†Ø§
                            pass
                        except:
                            missing_assets.append(f"ØµÙˆØ±Ø© ØªØ§Ù„ÙØ©: {file}")
        
        return missing_assets
    
    async def _compare_performance(self) -> Dict[str, Any]:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ù„Ù†Ø³Ø®Ø©"""
        comparison = {
            'original_load_time': 0,
            'replica_load_time': 0,
            'size_difference': 0,
            'functionality_preservation': 'unknown'
        }
        
        # Ù‚ÙŠØ§Ø³ Ø³Ø±Ø¹Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ
        start_time = time.time()
        try:
            async with self.session.get(self.config.target_url) as response:
                if response.status == 200:
                    comparison['original_load_time'] = time.time() - start_time
        except:
            pass
        
        return comparison
    
    async def _calculate_quality_score(self) -> Dict[str, Any]:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"""
        quality_score = {
            'overall_score': 0,
            'completeness': 0,
            'accuracy': 0,
            'performance': 0,
            'functionality': 0
        }
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø§ÙƒØªÙ…Ø§Ù„
        expected_pages = 10  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
        actual_pages = self.result.pages_extracted
        quality_score['completeness'] = min(100, (actual_pages / expected_pages) * 100)
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¯Ù‚Ø©
        total_errors = len(self.result.error_log)
        if total_errors == 0:
            quality_score['accuracy'] = 100
        else:
            quality_score['accuracy'] = max(0, 100 - (total_errors * 10))
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
        if self.result.total_size > 0:
            size_efficiency = min(100, (self.result.assets_downloaded / max(self.result.total_size / 1000000, 1)) * 10)
            quality_score['performance'] = size_efficiency
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        quality_score['overall_score'] = (
            quality_score['completeness'] * 0.3 +
            quality_score['accuracy'] * 0.3 +
            quality_score['performance'] * 0.2 +
            quality_score['functionality'] * 0.2
        )
        
        return quality_score

    async def _ai_intelligent_replication(self) -> Dict[str, Any]:
        """Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        replication_plan = {
            'architecture_analysis': {},
            'component_mapping': {},
            'implementation_strategy': {},
            'optimization_plan': {},
            'testing_strategy': {}
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©
        replication_plan['architecture_analysis'] = await self._analyze_website_architecture()
        
        # ØªØ®Ø·ÙŠØ· Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        replication_plan['component_mapping'] = await self._map_components_for_replication()
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ°
        replication_plan['implementation_strategy'] = self._create_implementation_strategy()
        
        return replication_plan
    
    async def _analyze_website_architecture(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        architecture = {
            'frontend_framework': 'Unknown',
            'backend_technology': 'Unknown',
            'database_type': 'Unknown',
            'hosting_platform': 'Unknown',
            'cdn_usage': False,
            'api_architecture': 'Unknown'
        }
        
        # ØªØ­Ù„ÙŠÙ„ frontend framework
        js_dir = os.path.join(self.result.output_path, "02_assets", "js")
        if os.path.exists(js_dir):
            for js_file in os.listdir(js_dir):
                if js_file.endswith('.js'):
                    file_path = os.path.join(js_dir, js_file)
                    try:
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            js_content = await f.read()
                            
                        # ØªØ­Ø¯ÙŠØ¯ framework
                        if 'React' in js_content or 'react' in js_content:
                            architecture['frontend_framework'] = 'React'
                        elif 'Vue' in js_content or 'vue' in js_content:
                            architecture['frontend_framework'] = 'Vue.js'
                        elif 'Angular' in js_content or 'angular' in js_content:
                            architecture['frontend_framework'] = 'Angular'
                        elif 'jQuery' in js_content or 'jquery' in js_content:
                            architecture['frontend_framework'] = 'jQuery'
                    except:
                        continue
        
        return architecture
    
    async def _map_components_for_replication(self) -> Dict[str, Any]:
        """ØªØ®Ø·ÙŠØ· Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„Ù†Ø³Ø®"""
        component_map = {
            'essential_components': [],
            'optional_components': [],
            'complex_components': [],
            'third_party_integrations': []
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        if os.path.exists(content_dir):
            for html_file in os.listdir(content_dir):
                if html_file.endswith('.html'):
                    file_path = os.path.join(content_dir, html_file)
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
                        if soup.find('nav'):
                            component_map['essential_components'].append('Navigation')
                        if soup.find('header'):
                            component_map['essential_components'].append('Header')
                        if soup.find('footer'):
                            component_map['essential_components'].append('Footer')
                        if soup.find('form'):
                            component_map['essential_components'].append('Forms')
                        
                        # Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ø¹Ù‚Ø¯Ø©
                        if soup.find('div', class_=re.compile(r'carousel|slider')):
                            component_map['complex_components'].append('Image Carousel')
                        if soup.find('div', class_=re.compile(r'modal|popup')):
                            component_map['complex_components'].append('Modal Dialogs')
                        
                        # ØªÙƒØ§Ù…Ù„Ø§Øª Ø·Ø±Ù Ø«Ø§Ù„Ø«
                        if soup.find('iframe', src=re.compile(r'youtube|vimeo')):
                            component_map['third_party_integrations'].append('Video Embedding')
                        if soup.find('script', src=re.compile(r'google|analytics')):
                            component_map['third_party_integrations'].append('Analytics')
        
        return component_map
    
    def _create_implementation_strategy(self) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ°"""
        strategy = {
            'development_phases': [
                'Phase 1: Basic Structure Setup',
                'Phase 2: Core Components Implementation',
                'Phase 3: Interactive Features',
                'Phase 4: Third-party Integrations',
                'Phase 5: Optimization and Testing'
            ],
            'recommended_technologies': {
                'frontend': 'HTML5, CSS3, JavaScript',
                'backend': 'Flask/Python or Node.js',
                'database': 'SQLite/PostgreSQL',
                'styling': 'Bootstrap or Tailwind CSS'
            },
            'estimated_timeline': '2-4 weeks',
            'complexity_score': 7.5  # Ù…Ù† 10
        }
        
        return strategy

    # ==================== Report Generation Methods ====================
    
    async def _generate_html_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± HTML ØªÙØ§Ø¹Ù„ÙŠ"""
        try:
            html_content = f"""
            <!DOCTYPE html>
            <html dir="rtl" lang="ar">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>ØªÙ‚Ø±ÙŠØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹ - {urlparse(self.config.target_url).netloc}</title>
                <style>
                    body {{ font-family: 'Arial', sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
                    .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                    .header {{ text-align: center; margin-bottom: 30px; border-bottom: 2px solid #4CAF50; padding-bottom: 20px; }}
                    .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0; }}
                    .stat-card {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; text-align: center; }}
                    .section {{ margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }}
                    .success {{ color: #4CAF50; }} .error {{ color: #f44336; }} .warning {{ color: #ff9800; }}
                </style>
            </head>
            <body>
                <div class="container">
                    <div class="header">
                        <h1>ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„</h1>
                        <h2>{self.config.target_url}</h2>
                        <p>ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                    </div>
                    
                    <div class="stats">
                        <div class="stat-card">
                            <h3>ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©</h3>
                            <h2>{self.result.pages_extracted}</h2>
                        </div>
                        <div class="stat-card">
                            <h3>ğŸ“ Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©</h3>
                            <h2>{self.result.assets_downloaded}</h2>
                        </div>
                        <div class="stat-card">
                            <h3>â±ï¸ Ù…Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬</h3>
                            <h2>{self.result.duration:.2f}s</h2>
                        </div>
                        <div class="stat-card">
                            <h3>ğŸ’¾ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª</h3>
                            <h2>{self.result.total_size / 1024 / 1024:.2f} MB</h2>
                        </div>
                    </div>
                    
                    <div class="section">
                        <h3>ğŸ› ï¸ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©</h3>
                        <ul>
                            {chr(10).join([f'<li><strong>{key}:</strong> {value}</li>' for key, value in self.result.technologies_detected.items()])}
                        </ul>
                    </div>
                    
                    <div class="section">
                        <h3>ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ø§Ù†</h3>
                        <p>Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ù…Ø§Ù†: {self.result.security_analysis.get('score', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
                        <p>Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(self.result.security_analysis.get('risks', []))}</p>
                    </div>
                    
                    <div class="section">
                        <h3>ğŸ¤– Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</h3>
                        <p>ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.result.ai_analysis.get('content_analysis', {}).get('category', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
                        <p>Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {self.result.ai_analysis.get('content_analysis', {}).get('quality_score', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}</p>
                    </div>
                    
                    {"<div class='section'><h3 class='error'>âŒ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ´ÙØ©</h3><ul>" + chr(10).join([f'<li>{error}</li>' for error in self.result.error_log]) + "</ul></div>" if self.result.error_log else ""}
                </div>
            </body>
            </html>
            """
            
            os.makedirs(self.result.reports_path, exist_ok=True)
            html_path = os.path.join(self.result.reports_path, "comprehensive_report.html")
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
                
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± HTML")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± HTML: {e}")

    async def _generate_json_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± JSON ØªÙØµÙŠÙ„ÙŠ"""
        try:
            report_data = {
                "metadata": {
                    "target_url": self.config.target_url,
                    "extraction_date": datetime.now().isoformat(),
                    "duration": self.result.duration,
                    "success": self.result.success
                },
                "statistics": {
                    "pages_extracted": self.result.pages_extracted,
                    "assets_downloaded": self.result.assets_downloaded,
                    "total_size": self.result.total_size,
                    "errors_count": self.result.errors_encountered
                },
                "technologies": self.result.technologies_detected,
                "security_analysis": self.result.security_analysis,
                "performance_metrics": self.result.performance_metrics,
                "ai_analysis": self.result.ai_analysis,
                "extracted_content": self.result.extracted_content,
                "errors": self.result.error_log,
                "recommendations": self.result.recommendations
            }
            
            os.makedirs(self.result.reports_path, exist_ok=True)
            json_path = os.path.join(self.result.reports_path, "detailed_report.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                f.write(json.dumps(report_data, ensure_ascii=False, indent=2))
                
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± JSON")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± JSON: {e}")

    async def _generate_csv_reports(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø§Ø±ÙŠØ± CSV Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            os.makedirs(self.result.reports_path, exist_ok=True)
            
            # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØµÙØ­Ø§Øª
            pages_csv = os.path.join(self.result.reports_path, "pages_report.csv")
            with open(pages_csv, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['URL', 'Status', 'Title', 'Size', 'Load Time'])
                for url in self.extracted_urls:
                    writer.writerow([url, 'Success', 'Unknown', 'Unknown', 'Unknown'])
            
            # ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£ØµÙˆÙ„  
            assets_csv = os.path.join(self.result.reports_path, "assets_report.csv")
            with open(assets_csv, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['Asset Type', 'Count', 'Total Size'])
                writer.writerow(['Images', self.result.assets_downloaded, f"{self.result.total_size} bytes"])
                
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø§Ø±ÙŠØ± CSV")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø§Ø±ÙŠØ± CSV: {e}")

    async def _generate_pdf_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± PDF"""
        if not REPORTLAB_AVAILABLE:
            self.logger.warning("Ù…ÙƒØªØ¨Ø© ReportLab ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ PDF")
            return
            
        try:
            from reportlab.pdfgen import canvas
            from reportlab.lib.pagesizes import A4
            
            os.makedirs(self.result.reports_path, exist_ok=True)
            pdf_path = os.path.join(self.result.reports_path, "summary_report.pdf")
            c = canvas.Canvas(pdf_path, pagesize=A4)
            
            # Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            c.setFont("Helvetica-Bold", 16)
            c.drawString(100, 750, f"Website Extraction Report")
            c.drawString(100, 730, f"URL: {self.config.target_url}")
            
            # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            c.setFont("Helvetica", 12)
            y_position = 680
            stats = [
                f"Pages Extracted: {self.result.pages_extracted}",
                f"Assets Downloaded: {self.result.assets_downloaded}",
                f"Duration: {self.result.duration:.2f} seconds",
                f"Total Size: {self.result.total_size / 1024 / 1024:.2f} MB",
                f"Errors: {len(self.result.error_log)}"
            ]
            
            for stat in stats:
                c.drawString(100, y_position, stat)
                y_position -= 20
                
            c.save()
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± PDF")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± PDF: {e}")

    async def _generate_docx_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Word"""
        if not DOCX_AVAILABLE:
            self.logger.warning("Ù…ÙƒØªØ¨Ø© python-docx ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Word")
            return
            
        try:
            from docx import Document
            
            doc = Document()
            doc.add_heading('ØªÙ‚Ø±ÙŠØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„', 0)
            
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
            doc.add_heading('Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬', level=1)
            doc.add_paragraph(f'Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.config.target_url}')
            doc.add_paragraph(f'ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
            doc.add_paragraph(f'Ù…Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {self.result.duration:.2f} Ø«Ø§Ù†ÙŠØ©')
            
            # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            doc.add_heading('Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª', level=1)
            doc.add_paragraph(f'Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {self.result.pages_extracted}')
            doc.add_paragraph(f'Ø¹Ø¯Ø¯ Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {self.result.assets_downloaded}')
            doc.add_paragraph(f'Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {self.result.total_size / 1024 / 1024:.2f} Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª')
            
            # Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            if self.result.error_log:
                doc.add_heading('Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ´ÙØ©', level=1)
                for error in self.result.error_log:
                    doc.add_paragraph(f'â€¢ {error}')
            
            os.makedirs(self.result.reports_path, exist_ok=True)
            docx_path = os.path.join(self.result.reports_path, "comprehensive_report.docx")
            doc.save(docx_path)
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Word")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Word: {e}")

    async def _create_project_guide(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        try:
            guide_content = f"""# Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬

## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø©
- **Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ØµØ¯Ø±**: {self.config.target_url}
- **ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Ù…Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬**: {self.result.duration:.2f} Ø«Ø§Ù†ÙŠØ©

## Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
- `01_extracted_content/`: Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹
- `02_assets/`: Ø§Ù„ØµÙˆØ± ÙˆÙ…Ù„ÙØ§Øª CSS ÙˆJavaScript
- `03_source_code/`: Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ Ø§Ù„Ù…Ø­Ù„Ù„
- `04_analysis/`: ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
- `05_cloned_site/`: Ù†Ø³Ø®Ø© ÙƒØ§Ù…Ù„Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ´ØºÙŠÙ„
- `06_reports/`: Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„Ø©
- `07_databases/`: Ø¨ÙŠØ§Ù†Ø§Øª Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
- `08_apis/`: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª APIs Ø§Ù„Ù…ÙƒØªØ´ÙØ©

## Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
- Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {self.result.pages_extracted}
- Ø¹Ø¯Ø¯ Ø§Ù„Ø£ØµÙˆÙ„: {self.result.assets_downloaded}
- Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {self.result.total_size / 1024 / 1024:.2f} MB

## ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
1. Ø§ÙØªØ­ Ù…Ø¬Ù„Ø¯ `05_cloned_site` Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
2. Ø±Ø§Ø¬Ø¹ `06_reports` Ù„Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
3. Ø§Ø³ØªØ®Ø¯Ù… `03_source_code` Ù„ÙÙ‡Ù… Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©

## Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
{chr(10).join([f'- {key}: {value}' for key, value in self.result.technologies_detected.items()])}
"""
            
            guide_path = os.path.join(self.result.output_path, "PROJECT_GUIDE.md")
            with open(guide_path, 'w', encoding='utf-8') as f:
                f.write(guide_content)
                
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {e}")

    async def _compress_output(self):
        """Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©"""
        try:
            import zipfile
            
            zip_path = f"{self.result.output_path}.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(self.result.output_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arc_name = os.path.relpath(file_path, self.result.output_path)
                        zipf.write(file_path, arc_name)
                        
            self.logger.info(f"âœ… ØªÙ… Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ: {zip_path}")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª: {e}")

    async def _generate_checksums(self):
        """Ø¥Ù†Ø´Ø§Ø¡ checksums Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            checksums = {}
            
            for root, dirs, files in os.walk(self.result.output_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'rb') as f:
                            file_hash = hashlib.md5(f.read()).hexdigest()
                            rel_path = os.path.relpath(file_path, self.result.output_path)
                            checksums[rel_path] = file_hash
                    except:
                        continue
            
            checksum_path = os.path.join(self.result.output_path, "checksums.json")
            with open(checksum_path, 'w', encoding='utf-8') as f:
                f.write(json.dumps(checksums, indent=2))
                
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù checksums")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ checksums: {e}")

    async def _create_readme_file(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù README Ø´Ø§Ù…Ù„"""
        try:
            readme_content = f"""# ğŸŒ Website Extraction Project

## ğŸ“‹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
- **Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬**: {self.config.target_url}
- **ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬**: {"Ù†Ø¬Ø­" if self.result.success else "ÙØ´Ù„"}
- **Ù…Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©**: {self.result.duration:.2f} Ø«Ø§Ù†ÙŠØ©

## ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
| Ø§Ù„Ù…Ø¹ÙŠØ§Ø± | Ø§Ù„Ù‚ÙŠÙ…Ø© |
|---------|--------|
| Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© | {self.result.pages_extracted} |
| Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø© | {self.result.assets_downloaded} |
| Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | {self.result.total_size / 1024 / 1024:.2f} MB |
| Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ | {len(self.result.error_log)} |

## ğŸ—‚ï¸ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
```
{os.path.basename(self.result.output_path)}/
â”œâ”€â”€ 01_extracted_content/    # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
â”œâ”€â”€ 02_assets/              # Ø§Ù„Ø£ØµÙˆÙ„ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª
â”œâ”€â”€ 03_source_code/         # Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ
â”œâ”€â”€ 04_analysis/            # ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„
â”œâ”€â”€ 05_cloned_site/         # Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ù†Ø³ÙˆØ®
â”œâ”€â”€ 06_reports/             # Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„Ø©
â”œâ”€â”€ 07_databases/           # Ø¨ÙŠØ§Ù†Ø§Øª Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â””â”€â”€ 08_apis/               # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª APIs
```

## ğŸ› ï¸ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
{chr(10).join([f'- **{key}**: {value}' for key, value in self.result.technologies_detected.items()]) if self.result.technologies_detected else '- Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø­Ø¯Ø¯Ø©'}

## ğŸš€ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
1. **Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ù†Ø³ÙˆØ®**: Ø§ÙØªØ­ `05_cloned_site/index.html` ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­
2. **Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±**: ØªØµÙØ­ Ù…Ø¬Ù„Ø¯ `06_reports` Ù„Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
3. **ÙØ­Øµ Ø§Ù„ÙƒÙˆØ¯**: Ø§Ø³ØªØ®Ø¯Ù… `03_source_code` Ù„ÙÙ‡Ù… Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©
4. **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**: Ø±Ø§Ø¬Ø¹ `04_analysis` Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

## âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©
- ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
- Ø¨Ø¹Ø¶ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø®Ø§Ø¯Ù… ÙˆÙŠØ¨ Ù„ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
- Ø±Ø§Ø¬Ø¹ Ù…Ù„Ù `PROJECT_GUIDE.md` Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„

## ğŸ“ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡Øª Ø£ÙŠ Ù…Ø´Ø§ÙƒÙ„ØŒ Ø±Ø§Ø¬Ø¹ Ù…Ø¬Ù„Ø¯ `06_reports` Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©.

---
ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø© Website Cloner Pro
"""
            
            readme_path = os.path.join(self.result.output_path, "README.md")
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
                
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù README")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù README: {e}")

    async def _calculate_final_statistics(self):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        try:
            # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
            folder_sizes = {}
            for folder in ['01_extracted_content', '02_assets', '03_source_code', '04_analysis', '05_cloned_site', '06_reports']:
                folder_path = os.path.join(self.result.output_path, folder)
                if os.path.exists(folder_path):
                    size = sum(os.path.getsize(os.path.join(root, file)) 
                             for root, dirs, files in os.walk(folder_path) 
                             for file in files)
                    folder_sizes[folder] = size
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
            file_types = {}
            for root, dirs, files in os.walk(self.result.output_path):
                for file in files:
                    ext = os.path.splitext(file)[1].lower()
                    file_types[ext] = file_types.get(ext, 0) + 1
            
            # Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            stats = {
                'folder_sizes': folder_sizes,
                'file_types': file_types,
                'total_files': sum(file_types.values()),
                'completion_rate': min(100, (self.result.pages_extracted / max(self.config.max_pages, 1)) * 100)
            }
            
            stats_path = os.path.join(self.result.output_path, "final_statistics.json")
            with open(stats_path, 'w', encoding='utf-8') as f:
                f.write(json.dumps(stats, ensure_ascii=False, indent=2))
                
            self.logger.info("âœ… ØªÙ… Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")

    # ==================== Missing AI Analysis Methods ====================
    
    async def _ai_content_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        return {
            'content_type': 'mixed',
            'language': 'auto-detected',
            'readability_score': 75,
            'sentiment': 'neutral',
            'keywords': [],
            'topics': []
        }
    
    async def _ai_optimization_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        return {
            'seo_recommendations': [],
            'performance_improvements': [],
            'accessibility_suggestions': [],
            'mobile_optimization': [],
            'loading_optimization': []
        }
    
    async def _ai_pattern_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        return {
            'design_patterns': [],
            'navigation_patterns': [],
            'content_patterns': [],
            'interactive_patterns': []
        }
    
    async def _ai_ux_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        return {
            'usability_score': 80,
            'accessibility_score': 70,
            'mobile_friendliness': 85,
            'loading_speed': 75,
            'navigation_clarity': 80
        }

    # ==================== Missing Implementation Methods ====================
    
    async def _initial_technology_detection(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø£ÙˆÙ„ÙŠ Ø¹Ù† Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"""
        tech_data = {
            'cms': 'unknown',
            'frameworks': [],
            'libraries': [],
            'analytics': [],
            'server_technology': 'unknown'
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¥Ø´Ø§Ø±Ø§Øª CMS
        generator_meta = soup.find('meta', attrs={'name': 'generator'})
        if generator_meta and isinstance(generator_meta, Tag):
            generator = str(generator_meta.get('content', ''))
            if 'wordpress' in generator.lower():
                tech_data['cms'] = 'WordPress'
            elif 'drupal' in generator.lower():
                tech_data['cms'] = 'Drupal'
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† JavaScript frameworks
        scripts = soup.find_all('script')
        for script in scripts:
            if isinstance(script, Tag):
                src_attr = script.get('src')
                src = str(src_attr) if src_attr else ''
                if 'react' in src:
                    tech_data['frameworks'].append('React')
                elif 'vue' in src:
                    tech_data['frameworks'].append('Vue.js')
                elif 'angular' in src:
                    tech_data['frameworks'].append('Angular')
        
        return tech_data

    async def _extract_all_pages(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª"""
        self.logger.info("Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª...")
        
        urls_to_process = [self.config.target_url]
        processed_urls = set()
        depth = 0
        
        while urls_to_process and depth < self.config.max_depth and len(processed_urls) < self.config.max_pages:
            current_batch = urls_to_process[:10]  # Ù…Ø¹Ø§Ù„Ø¬Ø© 10 URLs ÙÙŠ Ø§Ù„Ù…Ø±Ø©
            urls_to_process = urls_to_process[10:]
            
            for url in current_batch:
                if url in processed_urls:
                    continue
                    
                # Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
                content = await self._fetch_page_content(url, use_js=self.config.handle_javascript)
                if content:
                    # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                    page_filename = self._url_to_filename(url) + '.html'
                    page_path = os.path.join(self.result.output_path, "01_extracted_content", page_filename)
                    
                    async with aiofiles.open(page_path, 'w', encoding='utf-8') as f:
                        await f.write(content)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                    soup = BeautifulSoup(content, 'html.parser')
                    new_links = await self._extract_all_links(soup, url)
                    
                    for new_link in new_links:
                        if new_link not in processed_urls and new_link not in current_batch:
                            urls_to_process.append(new_link)
                    
                    processed_urls.add(url)
                    self.result.pages_extracted += 1
                    
                await asyncio.sleep(self.config.delay_between_requests)
            
            depth += 1
        
    async def _download_all_assets(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„ (ØµÙˆØ±ØŒ CSSØŒ JSØŒ Ø¥Ù„Ø®)"""
        self.logger.info("Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„...")
        assets_found = set()
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        if os.path.exists(content_dir):
            for html_file in os.listdir(content_dir):
                if html_file.endswith('.html'):
                    file_path = os.path.join(content_dir, html_file)
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
                        for img in soup.find_all('img'):
                            if isinstance(img, Tag):
                                src = img.get('src')
                                if src:
                                    asset_url = urljoin(self.config.target_url, str(src))
                                    assets_found.add(('image', asset_url))
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSS
                        for link in soup.find_all('link'):
                            if isinstance(link, Tag):
                                href = link.get('href')
                                rel = link.get('rel')
                                if href and rel and 'stylesheet' in str(rel):
                                    asset_url = urljoin(self.config.target_url, str(href))
                                    assets_found.add(('css', asset_url))
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ JavaScript
                        for script in soup.find_all('script'):
                            if isinstance(script, Tag):
                                src = script.get('src')
                                if src:
                                    asset_url = urljoin(self.config.target_url, str(src))
                                    assets_found.add(('js', asset_url))
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„
        for asset_type, asset_url in assets_found:
            await self._download_asset(asset_type, asset_url)
            self.result.assets_downloaded += 1
        
    async def _download_asset(self, asset_type: str, asset_url: str):
        """ØªØ­Ù…ÙŠÙ„ Ø£ØµÙ„ ÙˆØ§Ø­Ø¯"""
        try:
            parsed_url = urlparse(asset_url)
            filename = os.path.basename(parsed_url.path) or f"asset_{hash(asset_url)}"
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø£ØµÙ„
            if asset_type == 'image':
                asset_dir = os.path.join(self.result.output_path, "02_assets", "images")
            elif asset_type == 'css':
                asset_dir = os.path.join(self.result.output_path, "02_assets", "styles")
            elif asset_type == 'js':
                asset_dir = os.path.join(self.result.output_path, "02_assets", "scripts")
            else:
                asset_dir = os.path.join(self.result.output_path, "02_assets", "other")
            
            os.makedirs(asset_dir, exist_ok=True)
            asset_path = os.path.join(asset_dir, filename)
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': '*/*',
                'Accept-Encoding': 'gzip, deflate',
                'Accept-Language': 'en-US,en;q=0.5'
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                async with session.get(asset_url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.read()
                        async with aiofiles.open(asset_path, 'wb') as f:
                            await f.write(content)
                        self.logger.debug(f"ØªÙ… ØªØ­Ù…ÙŠÙ„: {filename}")
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {asset_url}: {e}")

    async def _extract_dynamic_content(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… JavaScript"""
        self.logger.info("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ...")
        
        if not self.config.handle_javascript:
            return
        
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Playwright Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
            from playwright.async_api import async_playwright
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
                await page.route("**/*.{png,jpg,jpeg,gif,svg,ico}", lambda route: route.abort())
                
                await page.goto(self.config.target_url, wait_until='networkidle')
                
                # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
                await page.wait_for_timeout(3000)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
                dynamic_content = await page.content()
                
                # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
                dynamic_path = os.path.join(self.result.output_path, "01_extracted_content", "dynamic_content.html")
                async with aiofiles.open(dynamic_path, 'w', encoding='utf-8') as f:
                    await f.write(dynamic_content)
                
                await browser.close()
                
        except ImportError:
            self.logger.warning("Playwright ØºÙŠØ± Ù…Ø«Ø¨Øª - ØªØ®Ø·ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ")
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ: {e}")
        
    async def _extract_hidden_content(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª"""
        self.logger.info("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ...")
        
        content_dir = os.path.join(self.result.output_path, "01_extracted_content")
        hidden_content = []
        
        if os.path.exists(content_dir):
            for html_file in os.listdir(content_dir):
                if html_file.endswith('.html'):
                    file_path = os.path.join(content_dir, html_file)
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª HTML
                        html_comments = re.findall(r'<!--(.*?)-->', content, re.DOTALL)
                        for comment in html_comments:
                            if comment.strip():
                                hidden_content.append({
                                    'type': 'html_comment',
                                    'content': comment.strip(),
                                    'file': html_file
                                })
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ÙÙŠØ© Ø¨Ù€ CSS
                        soup = BeautifulSoup(content, 'html.parser')
                        for element in soup.find_all(style=re.compile(r'display:\s*none|visibility:\s*hidden')):
                            if isinstance(element, Tag):
                                hidden_content.append({
                                    'type': 'hidden_element',
                                    'tag': element.name,
                                    'content': element.get_text(strip=True)[:200],
                                    'file': html_file
                                })
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙÙŠ
        hidden_path = os.path.join(self.result.output_path, "04_analysis", "hidden_content.json")
        os.makedirs(os.path.dirname(hidden_path), exist_ok=True)
        
        async with aiofiles.open(hidden_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(hidden_content, ensure_ascii=False, indent=2))
        
    async def _comprehensive_technology_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        self.logger.info("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙ‚Ù†ÙŠØ§Øª...")
        
        tech_analysis = {
            'cms': 'unknown',
            'frameworks': [],
            'libraries': [],
            'analytics': [],
            'server_technology': 'unknown',
            'hosting_provider': 'unknown',
            'ssl_info': {},
            'performance_tools': [],
            'cdn_usage': [],
            'database_indicators': []
        }
        
        try:
            # ØªØ­Ù„ÙŠÙ„ headers Ø§Ù„Ø®Ø§Ø¯Ù…
            async with aiohttp.ClientSession() as session:
                async with session.head(self.config.target_url) as response:
                    headers = response.headers
                    
                    # ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø§Ø¯Ù…
                    server_header = headers.get('Server', '')
                    if 'nginx' in server_header.lower():
                        tech_analysis['server_technology'] = 'Nginx'
                    elif 'apache' in server_header.lower():
                        tech_analysis['server_technology'] = 'Apache'
                    elif 'cloudflare' in server_header.lower():
                        tech_analysis['cdn_usage'].append('Cloudflare')
                    
                    # ØªØ­Ù„ÙŠÙ„ X-Powered-By header
                    powered_by = headers.get('X-Powered-By', '')
                    if 'php' in powered_by.lower():
                        tech_analysis['frameworks'].append('PHP')
                    elif 'express' in powered_by.lower():
                        tech_analysis['frameworks'].append('Express.js')
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            soup = BeautifulSoup(content, 'html.parser')
                            
                            # ØªØ­Ù„ÙŠÙ„ Ù…ÙˆÙ„Ø¯ CMS
                            generator_meta = soup.find('meta', attrs={'name': 'generator'})
                            if generator_meta and isinstance(generator_meta, Tag):
                                generator = str(generator_meta.get('content', ''))
                                if 'wordpress' in generator.lower():
                                    tech_analysis['cms'] = 'WordPress'
                                elif 'drupal' in generator.lower():
                                    tech_analysis['cms'] = 'Drupal'
                                elif 'joomla' in generator.lower():
                                    tech_analysis['cms'] = 'Joomla'
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ÙˆØ§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
                            scripts = soup.find_all('script')
                            for script in scripts:
                                if isinstance(script, Tag):
                                    src = script.get('src', '')
                                    script_content = script.get_text()
                                    
                                    if src:
                                        src_str = str(src)
                                        if 'jquery' in src_str:
                                            tech_analysis['libraries'].append('jQuery')
                                        elif 'react' in src_str:
                                            tech_analysis['frameworks'].append('React')
                                        elif 'vue' in src_str:
                                            tech_analysis['frameworks'].append('Vue.js')
                                        elif 'angular' in src_str:
                                            tech_analysis['frameworks'].append('Angular')
                                        elif 'bootstrap' in src_str:
                                            tech_analysis['libraries'].append('Bootstrap')
                                    
                                    if script_content:
                                        if 'google-analytics' in script_content or 'gtag(' in script_content:
                                            tech_analysis['analytics'].append('Google Analytics')
                                        elif 'fbq(' in script_content:
                                            tech_analysis['analytics'].append('Facebook Pixel')
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø±ÙˆØ§Ø¨Ø· CSS
                            css_links = soup.find_all('link', rel='stylesheet')
                            for link in css_links:
                                if isinstance(link, Tag):
                                    href = str(link.get('href', ''))
                                    if 'bootstrap' in href:
                                        tech_analysis['libraries'].append('Bootstrap')
                                    elif 'fontawesome' in href:
                                        tech_analysis['libraries'].append('Font Awesome')
                                    
                        break  # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ Ù…Ù„Ù ÙÙ‚Ø· Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
            tech_analysis['frameworks'] = list(set(tech_analysis['frameworks']))
            tech_analysis['libraries'] = list(set(tech_analysis['libraries']))
            tech_analysis['analytics'] = list(set(tech_analysis['analytics']))
            tech_analysis['cdn_usage'] = list(set(tech_analysis['cdn_usage']))
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ: {e}")
        
        return tech_analysis
        
    async def _analyze_site_structure(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ù„ØªÙ†Ù‚Ù„"""
        self.logger.info("ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹...")
        
        structure_analysis = {
            'total_pages': 0,
            'navigation_structure': {},
            'page_hierarchy': {},
            'internal_links': [],
            'external_links': [],
            'broken_links': [],
            'sitemap_exists': False,
            'robots_txt_exists': False,
            'main_sections': []
        }
        
        try:
            # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ sitemap
            sitemap_url = urljoin(self.config.target_url, '/sitemap.xml')
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(sitemap_url) as response:
                        if response.status == 200:
                            structure_analysis['sitemap_exists'] = True
            except:
                pass
            
            # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ robots.txt
            robots_url = urljoin(self.config.target_url, '/robots.txt')
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(robots_url) as response:
                        if response.status == 200:
                            structure_analysis['robots_txt_exists'] = True
            except:
                pass
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                structure_analysis['total_pages'] = len([f for f in os.listdir(content_dir) if f.endswith('.html')])
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ù‚Ù„
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            soup = BeautifulSoup(content, 'html.parser')
                            
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                            nav_elements = soup.find_all(['nav', 'ul', 'ol'])
                            for nav in nav_elements:
                                if isinstance(nav, Tag):
                                    nav_class = nav.get('class', [])
                                    nav_id = nav.get('id', '')
                                    
                                    if any(keyword in str(nav_class) + str(nav_id) for keyword in ['menu', 'nav', 'navigation']):
                                        links = nav.find_all('a')
                                        nav_links = []
                                        for link in links:
                                            if isinstance(link, Tag):
                                                href = link.get('href')
                                                text = link.get_text(strip=True)
                                                if href and text:
                                                    nav_links.append({
                                                        'url': str(href),
                                                        'text': text
                                                    })
                                        
                                        if nav_links:
                                            structure_analysis['navigation_structure'][f"{nav.name}_{nav_id or nav_class}"] = nav_links
                        
                        break  # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ ØµÙØ­Ø© ÙÙ‚Ø·
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {e}")
        
        return structure_analysis
        
    async def _comprehensive_security_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        self.logger.info("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„...")
        
        security_analysis = {
            'ssl_certificate': {},
            'security_headers': {},
            'vulnerabilities_detected': [],
            'authentication_methods': [],
            'data_protection_measures': [],
            'secure_practices': [],
            'security_score': 0,
            'recommendations': []
        }
        
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø´Ù‡Ø§Ø¯Ø© SSL
            parsed_url = urlparse(self.config.target_url)
            if parsed_url.scheme == 'https':
                security_analysis['ssl_certificate'] = {
                    'enabled': True,
                    'protocol': 'HTTPS',
                    'secure_connection': True
                }
                security_analysis['security_score'] += 30
            else:
                security_analysis['ssl_certificate'] = {
                    'enabled': False,
                    'protocol': 'HTTP',
                    'secure_connection': False
                }
                security_analysis['vulnerabilities_detected'].append('No SSL encryption')
                security_analysis['recommendations'].append('ØªÙØ¹ÙŠÙ„ Ø´Ù‡Ø§Ø¯Ø© SSL Ù„Ù„Ø­Ù…Ø§ÙŠØ©')
            
            # ØªØ­Ù„ÙŠÙ„ headers Ø§Ù„Ø£Ù…Ø§Ù†
            async with aiohttp.ClientSession() as session:
                async with session.get(self.config.target_url) as response:
                    headers = response.headers
                    
                    # ÙØ­Øµ Security Headers Ø§Ù„Ù…Ù‡Ù…Ø©
                    security_headers = {
                        'Content-Security-Policy': headers.get('Content-Security-Policy'),
                        'X-Frame-Options': headers.get('X-Frame-Options'),
                        'X-Content-Type-Options': headers.get('X-Content-Type-Options'),
                        'Strict-Transport-Security': headers.get('Strict-Transport-Security'),
                        'X-XSS-Protection': headers.get('X-XSS-Protection'),
                        'Referrer-Policy': headers.get('Referrer-Policy')
                    }
                    
                    security_analysis['security_headers'] = security_headers
                    
                    # ØªÙ‚ÙŠÙŠÙ… Security Headers
                    for header, value in security_headers.items():
                        if value:
                            security_analysis['secure_practices'].append(f'{header} header configured')
                            security_analysis['security_score'] += 10
                        else:
                            security_analysis['vulnerabilities_detected'].append(f'Missing {header} header')
                            security_analysis['recommendations'].append(f'Ø¥Ø¶Ø§ÙØ© {header} header Ù„Ù„Ø­Ù…Ø§ÙŠØ©')
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ©
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            
                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù…Ø§Ø°Ø¬ ØºÙŠØ± Ù…Ø­Ù…ÙŠØ©
                            if '<form' in content and 'method="post"' in content:
                                if 'csrf' not in content.lower() and 'token' not in content.lower():
                                    security_analysis['vulnerabilities_detected'].append('Forms without CSRF protection')
                                    security_analysis['recommendations'].append('Ø¥Ø¶Ø§ÙØ© Ø­Ù…Ø§ÙŠØ© CSRF Ù„Ù„Ù†Ù…Ø§Ø°Ø¬')
                            
                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† external scripts
                            external_scripts = re.findall(r'<script[^>]+src=["\']https?://[^"\']*["\'][^>]*>', content)
                            if external_scripts:
                                security_analysis['vulnerabilities_detected'].append(f'External scripts loaded: {len(external_scripts)}')
                                security_analysis['recommendations'].append('Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ù…Ø­Ù…Ù„Ø©')
                        
                        break  # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ Ù…Ù„Ù ÙÙ‚Ø·
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            max_score = 100
            security_analysis['security_score'] = min(security_analysis['security_score'], max_score)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠ: {e}")
        
        return security_analysis
        
    async def _performance_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ³Ø±Ø¹Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
        self.logger.info("Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡...")
        
        performance_data = {
            'page_load_time': 0.0,
            'page_size_kb': 0,
            'total_requests': 0,
            'asset_breakdown': {
                'images': {'count': 0, 'size_kb': 0},
                'css': {'count': 0, 'size_kb': 0},
                'js': {'count': 0, 'size_kb': 0},
                'other': {'count': 0, 'size_kb': 0}
            },
            'optimization_opportunities': [],
            'performance_score': 0,
            'loading_recommendations': []
        }
        
        try:
            # Ù‚ÙŠØ§Ø³ ÙˆÙ‚Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            start_time = time.time()
            async with aiohttp.ClientSession() as session:
                async with session.get(self.config.target_url) as response:
                    content = await response.read()
                    load_time = time.time() - start_time
                    performance_data['page_load_time'] = round(load_time, 2)
                    performance_data['page_size_kb'] = round(len(content) / 1024, 2)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©
            assets_dir = os.path.join(self.result.output_path, "02_assets")
            if os.path.exists(assets_dir):
                for asset_type in ['images', 'styles', 'scripts', 'other']:
                    type_dir = os.path.join(assets_dir, asset_type)
                    if os.path.exists(type_dir):
                        files = os.listdir(type_dir)
                        total_size = 0
                        for file in files:
                            file_path = os.path.join(type_dir, file)
                            if os.path.isfile(file_path):
                                total_size += os.path.getsize(file_path)
                        
                        asset_key = asset_type if asset_type != 'styles' else 'css'
                        asset_key = asset_key if asset_key != 'scripts' else 'js'
                        
                        if asset_key in performance_data['asset_breakdown']:
                            performance_data['asset_breakdown'][asset_key] = {
                                'count': len(files),
                                'size_kb': round(total_size / 1024, 2)
                            }
                            performance_data['total_requests'] += len(files)
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØªÙˆØµÙŠØ§Øª
            if performance_data['page_load_time'] > 3.0:
                performance_data['optimization_opportunities'].append('ØµÙØ­Ø© Ø¨Ø·ÙŠØ¦Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„')
                performance_data['loading_recommendations'].append('ØªØ­Ø³ÙŠÙ† Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„')
                performance_data['performance_score'] -= 20
            
            if performance_data['page_size_kb'] > 1000:
                performance_data['optimization_opportunities'].append('Ø­Ø¬Ù… ØµÙØ­Ø© ÙƒØ¨ÙŠØ±')
                performance_data['loading_recommendations'].append('Ø¶ØºØ· Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ØµÙˆØ±')
                performance_data['performance_score'] -= 15
            
            if performance_data['asset_breakdown']['images']['count'] > 20:
                performance_data['optimization_opportunities'].append('Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„ØµÙˆØ±')
                performance_data['loading_recommendations'].append('ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ± ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… lazy loading')
                performance_data['performance_score'] -= 10
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            base_score = 100
            performance_data['performance_score'] = max(base_score + performance_data['performance_score'], 0)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
        
        return performance_data
        
    async def _extract_api_endpoints(self) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù„Ù€ API Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ÙƒÙˆØ¯"""
        self.logger.info("Ø§Ø³ØªØ®Ø±Ø§Ø¬ API endpoints...")
        
        api_endpoints = []
        
        try:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„ÙØ§Øª JavaScript
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            
                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† API calls ÙÙŠ JavaScript
                            api_patterns = [
                                r'fetch\(["\']([^"\']+)["\']',
                                r'axios\.(?:get|post|put|delete)\(["\']([^"\']+)["\']',
                                r'ajax\s*\(\s*["\']([^"\']+)["\']',
                                r'["\'](?:api|API)/([^"\']+)["\']',
                                r'/api/v\d+/[^"\'?\s]+',
                                r'/rest/[^"\'?\s]+',
                                r'/graphql[^"\'?\s]*'
                            ]
                            
                            for pattern in api_patterns:
                                matches = re.findall(pattern, content, re.IGNORECASE)
                                for match in matches:
                                    if match and not match.startswith('#'):
                                        full_url = urljoin(self.config.target_url, match)
                                        if full_url not in api_endpoints:
                                            api_endpoints.append(full_url)
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„ÙØ§Øª JavaScript Ø§Ù„Ù…Ø­Ù…Ù„Ø©
            scripts_dir = os.path.join(self.result.output_path, "02_assets", "scripts")
            if os.path.exists(scripts_dir):
                for script_file in os.listdir(scripts_dir):
                    if script_file.endswith('.js'):
                        file_path = os.path.join(scripts_dir, script_file)
                        try:
                            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                                script_content = await f.read()
                                
                                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† API endpoints ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª
                                endpoint_patterns = [
                                    r'["\']([^"\']*api[^"\']*)["\']',
                                    r'["\']([^"\']*/v\d+/[^"\']*)["\']',
                                    r'endpoint[:\s]*["\']([^"\']+)["\']',
                                    r'baseURL[:\s]*["\']([^"\']+)["\']'
                                ]
                                
                                for pattern in endpoint_patterns:
                                    matches = re.findall(pattern, script_content, re.IGNORECASE)
                                    for match in matches:
                                        if match and 'api' in match.lower():
                                            full_url = urljoin(self.config.target_url, match)
                                            if full_url not in api_endpoints:
                                                api_endpoints.append(full_url)
                        except:
                            continue
                        
                        # ÙÙ‚Ø· Ø£ÙˆÙ„ 5 Ù…Ù„ÙØ§Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
                        if len(os.listdir(scripts_dir)) > 5:
                            break
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ API endpoints: {e}")
        
        return api_endpoints[:20]  # Ø¥Ø±Ø¬Ø§Ø¹ Ø£ÙˆÙ„ 20 endpoint
        
    async def _analyze_database_structure(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©"""
        self.logger.info("ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        database_analysis = {
            'database_indicators': [],
            'data_storage_methods': [],
            'potential_tables': [],
            'data_relationships': [],
            'storage_technologies': [],
            'data_patterns': []
        }
        
        try:
            # ØªØ­Ù„ÙŠÙ„ forms Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ structure Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            soup = BeautifulSoup(content, 'html.parser')
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø¬Ø¯Ø§ÙˆÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                            forms = soup.find_all('form')
                            for form in forms:
                                if isinstance(form, Tag):
                                    form_inputs = form.find_all(['input', 'select', 'textarea'])
                                    if len(form_inputs) > 2:  # Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹Ù‚Ø¯
                                        form_fields = []
                                        for inp in form_inputs:
                                            if isinstance(inp, Tag):
                                                name = inp.get('name')
                                                input_type = inp.get('type', 'text')
                                                if name:
                                                    form_fields.append({
                                                        'field': str(name),
                                                        'type': str(input_type)
                                                    })
                                        
                                        if form_fields:
                                            table_name = self._infer_table_name(form_fields)
                                            database_analysis['potential_tables'].append({
                                                'table_name': table_name,
                                                'fields': form_fields,
                                                'source': 'form_analysis'
                                            })
                            
                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ JavaScript
                            script_tags = soup.find_all('script')
                            for script in script_tags:
                                if isinstance(script, Tag):
                                    script_content = script.get_text()
                                    if script_content:
                                        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                                        db_indicators = [
                                            'mysql', 'postgresql', 'mongodb', 'sqlite',
                                            'database', 'collection', 'table', 'schema',
                                            'INSERT', 'SELECT', 'UPDATE', 'DELETE'
                                        ]
                                        
                                        for indicator in db_indicators:
                                            if indicator.lower() in script_content.lower():
                                                if indicator not in database_analysis['database_indicators']:
                                                    database_analysis['database_indicators'].append(indicator)
                        
                        break  # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ Ù…Ù„Ù ÙÙ‚Ø·
            
            # ØªØ­Ù„ÙŠÙ„ localStorage Ùˆ sessionStorage usage
            if 'localStorage' in content or 'sessionStorage' in content:
                database_analysis['data_storage_methods'].append('Browser Storage')
            
            # ØªØ­Ù„ÙŠÙ„ cookies usage
            if 'cookie' in content.lower() or 'document.cookie' in content:
                database_analysis['data_storage_methods'].append('Cookies')
                
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        
        return database_analysis
    
    def _infer_table_name(self, fields: List[Dict]) -> str:
        """Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù…Ù† Ø§Ù„Ø­Ù‚ÙˆÙ„"""
        common_patterns = {
            'user': ['name', 'email', 'password', 'username'],
            'product': ['name', 'price', 'description', 'category'],
            'order': ['quantity', 'total', 'date', 'status'],
            'contact': ['name', 'email', 'message', 'phone'],
            'comment': ['name', 'email', 'comment', 'content']
        }
        
        field_names = [f['field'].lower() for f in fields]
        
        for table_name, pattern_fields in common_patterns.items():
            matches = sum(1 for field in pattern_fields if any(field in fname for fname in field_names))
            if matches >= 2:  # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ø­Ù‚Ù„ÙŠÙ† Ù…ØªØ·Ø§Ø¨Ù‚ÙŠÙ†
                return table_name
        
        return 'unknown_table'
        
    async def _extract_source_code(self) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        self.logger.info("Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ...")
        
        source_code_analysis = {
            'html_files': [],
            'css_files': [],
            'js_files': [],
            'code_structure': {},
            'programming_languages': [],
            'frameworks_detected': [],
            'total_lines_of_code': 0,
            'code_quality_metrics': {}
        }
        
        try:
            # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª HTML
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            lines_count = len(content.splitlines())
                            
                            source_code_analysis['html_files'].append({
                                'filename': html_file,
                                'size_kb': round(len(content) / 1024, 2),
                                'lines_of_code': lines_count,
                                'elements_count': len(BeautifulSoup(content, 'html.parser').find_all())
                            })
                            source_code_analysis['total_lines_of_code'] += lines_count
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª CSS
            css_dir = os.path.join(self.result.output_path, "02_assets", "styles")
            if os.path.exists(css_dir):
                for css_file in os.listdir(css_dir):
                    if css_file.endswith('.css'):
                        file_path = os.path.join(css_dir, css_file)
                        try:
                            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                                content = await f.read()
                                lines_count = len(content.splitlines())
                                
                                # ØªØ­Ù„ÙŠÙ„ CSS properties
                                css_rules = re.findall(r'{[^}]+}', content)
                                
                                source_code_analysis['css_files'].append({
                                    'filename': css_file,
                                    'size_kb': round(len(content) / 1024, 2),
                                    'lines_of_code': lines_count,
                                    'css_rules_count': len(css_rules)
                                })
                                source_code_analysis['total_lines_of_code'] += lines_count
                        except:
                            continue
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª JavaScript
            js_dir = os.path.join(self.result.output_path, "02_assets", "scripts")
            if os.path.exists(js_dir):
                for js_file in os.listdir(js_dir):
                    if js_file.endswith('.js'):
                        file_path = os.path.join(js_dir, js_file)
                        try:
                            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                                content = await f.read()
                                lines_count = len(content.splitlines())
                                
                                # ØªØ­Ù„ÙŠÙ„ JavaScript functions
                                functions = re.findall(r'function\s+\w+\s*\(', content)
                                arrow_functions = re.findall(r'=>', content)
                                
                                source_code_analysis['js_files'].append({
                                    'filename': js_file,
                                    'size_kb': round(len(content) / 1024, 2),
                                    'lines_of_code': lines_count,
                                    'functions_count': len(functions) + len(arrow_functions)
                                })
                                source_code_analysis['total_lines_of_code'] += lines_count
                        except:
                            continue
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
            if source_code_analysis['html_files']:
                source_code_analysis['programming_languages'].append('HTML')
            if source_code_analysis['css_files']:
                source_code_analysis['programming_languages'].append('CSS')
            if source_code_analysis['js_files']:
                source_code_analysis['programming_languages'].append('JavaScript')
            
            # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯
            source_code_analysis['code_quality_metrics'] = {
                'total_files': len(source_code_analysis['html_files']) + len(source_code_analysis['css_files']) + len(source_code_analysis['js_files']),
                'average_file_size_kb': round(sum(f['size_kb'] for files in [source_code_analysis['html_files'], source_code_analysis['css_files'], source_code_analysis['js_files']] for f in files) / max(1, len(source_code_analysis['html_files']) + len(source_code_analysis['css_files']) + len(source_code_analysis['js_files'])), 2),
                'code_organization_score': min(100, max(0, 100 - (source_code_analysis['total_lines_of_code'] // 500) * 10))
            }
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ: {e}")
        
        return source_code_analysis
        
    async def _analyze_interactions(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª ÙˆØ§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        self.logger.info("ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª...")
        
        interactions_analysis = {
            'forms': [],
            'buttons': [],
            'links': [],
            'interactive_elements': [],
            'javascript_events': [],
            'user_input_fields': [],
            'navigation_elements': [],
            'interactivity_score': 0
        }
        
        try:
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            soup = BeautifulSoup(content, 'html.parser')
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
                            forms = soup.find_all('form')
                            for form in forms:
                                if isinstance(form, Tag):
                                    form_data = {
                                        'action': str(form.get('action', '')),
                                        'method': str(form.get('method', 'get')),
                                        'inputs_count': len(form.find_all(['input', 'select', 'textarea'])),
                                        'has_validation': bool(form.find(['input'], required=True))
                                    }
                                    interactions_analysis['forms'].append(form_data)
                                    interactions_analysis['interactivity_score'] += 15
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø²Ø±Ø§Ø±
                            buttons = soup.find_all(['button', 'input'])
                            for button in buttons:
                                if isinstance(button, Tag):
                                    button_type = str(button.get('type', ''))
                                    if button.name == 'button' or button_type in ['button', 'submit']:
                                        button_data = {
                                            'type': button_type or 'button',
                                            'text': button.get_text(strip=True),
                                            'has_onclick': bool(button.get('onclick')),
                                            'has_id': bool(button.get('id'))
                                        }
                                        interactions_analysis['buttons'].append(button_data)
                                        interactions_analysis['interactivity_score'] += 5
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
                            links = soup.find_all('a')
                            internal_links = 0
                            external_links = 0
                            for link in links:
                                if isinstance(link, Tag):
                                    href = str(link.get('href', ''))
                                    link_data = {
                                        'href': href,
                                        'text': link.get_text(strip=True),
                                        'target': str(link.get('target', '')),
                                        'is_external': href.startswith(('http://', 'https://')) and not href.startswith(self.config.target_url)
                                    }
                                    interactions_analysis['links'].append(link_data)
                                    
                                    if link_data['is_external']:
                                        external_links += 1
                                    else:
                                        internal_links += 1
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ø§Ù„Ø£Ø®Ø±Ù‰
                            interactive_selectors = [
                                '[onclick]', '[onchange]', '[onsubmit]', '[onload]',
                                '.dropdown', '.modal', '.tab', '.accordion',
                                '[data-toggle]', '[data-target]'
                            ]
                            
                            for selector in interactive_selectors:
                                elements = soup.select(selector)
                                if elements:
                                    interactions_analysis['interactive_elements'].append({
                                        'type': selector,
                                        'count': len(elements)
                                    })
                                    interactions_analysis['interactivity_score'] += len(elements) * 3
                            
                            # ØªØ­Ù„ÙŠÙ„ Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
                            input_fields = soup.find_all(['input', 'select', 'textarea'])
                            for field in input_fields:
                                if isinstance(field, Tag):
                                    field_data = {
                                        'type': str(field.get('type', field.name)),
                                        'name': str(field.get('name', '')),
                                        'required': bool(field.get('required')),
                                        'placeholder': str(field.get('placeholder', ''))
                                    }
                                    interactions_analysis['user_input_fields'].append(field_data)
                            
                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† JavaScript events
                            script_tags = soup.find_all('script')
                            for script in script_tags:
                                if isinstance(script, Tag):
                                    script_content = script.get_text()
                                    if script_content:
                                        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† event listeners
                                        events = re.findall(r'addEventListener\(["\'](\w+)["\']', script_content)
                                        for event in events:
                                            if event not in interactions_analysis['javascript_events']:
                                                interactions_analysis['javascript_events'].append(event)
                                                interactions_analysis['interactivity_score'] += 8
                        
                        break  # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ Ù…Ù„Ù ÙÙ‚Ø·
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
            max_score = 500
            interactions_analysis['interactivity_score'] = min(interactions_analysis['interactivity_score'], max_score)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª: {e}")
        
        return interactions_analysis
        
    async def _create_replica_structure(self) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        self.logger.info("Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©...")
        
        replica_info = {
            'replica_path': '',
            'structure_created': False,
            'files_copied': 0,
            'directories_created': 0,
            'replica_type': 'static_html',
            'launch_instructions': {}
        }
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            replica_dir = os.path.join(self.result.output_path, "05_cloned_site")
            os.makedirs(replica_dir, exist_ok=True)
            replica_info['replica_path'] = replica_dir
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            subdirs = ['css', 'js', 'images', 'fonts', 'assets']
            for subdir in subdirs:
                subdir_path = os.path.join(replica_dir, subdir)
                os.makedirs(subdir_path, exist_ok=True)
                replica_info['directories_created'] += 1
            
            # Ù†Ø³Ø® Ù…Ù„ÙØ§Øª HTML
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        source_path = os.path.join(content_dir, html_file)
                        
                        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                        if html_file.startswith(('index', 'home', 'main')) or len(os.listdir(content_dir)) == 1:
                            dest_path = os.path.join(replica_dir, 'index.html')
                        else:
                            dest_path = os.path.join(replica_dir, html_file)
                        
                        async with aiofiles.open(source_path, 'r', encoding='utf-8') as src:
                            content = await src.read()
                            # ØªØ­Ø¯ÙŠØ« Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø£ØµÙˆÙ„
                            content = self._fix_asset_paths(content)
                            
                            async with aiofiles.open(dest_path, 'w', encoding='utf-8') as dst:
                                await dst.write(content)
                            
                            replica_info['files_copied'] += 1
            
            # Ù†Ø³Ø® Ø§Ù„Ø£ØµÙˆÙ„
            assets_dir = os.path.join(self.result.output_path, "02_assets")
            if os.path.exists(assets_dir):
                # Ù†Ø³Ø® Ø§Ù„ØµÙˆØ±
                images_src = os.path.join(assets_dir, "images")
                if os.path.exists(images_src):
                    images_dst = os.path.join(replica_dir, "images")
                    await self._copy_directory_contents(images_src, images_dst)
                
                # Ù†Ø³Ø® CSS
                css_src = os.path.join(assets_dir, "styles")
                if os.path.exists(css_src):
                    css_dst = os.path.join(replica_dir, "css")
                    await self._copy_directory_contents(css_src, css_dst)
                
                # Ù†Ø³Ø® JavaScript
                js_src = os.path.join(assets_dir, "scripts")
                if os.path.exists(js_src):
                    js_dst = os.path.join(replica_dir, "js")
                    await self._copy_directory_contents(js_src, js_dst)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù README
            readme_content = f"""# {self.config.target_url} - Ù†Ø³Ø®Ø© Ù…Ø·Ø§Ø¨Ù‚Ø©

ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø³Ø®Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Website Cloner Pro
ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„:
1. Ø§ÙØªØ­ Ù…Ù„Ù index.html ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­
2. Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø®Ø§Ø¯Ù… ÙˆÙŠØ¨ Ù…Ø­Ù„ÙŠ:
   - Python: python -m http.server 8000
   - Node.js: npx serve .
   - PHP: php -S localhost:8000

## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø³Ø®Ø©:
- Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø³ÙˆØ®Ø©: {replica_info['files_copied']}
- Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø´Ø£Ø©: {replica_info['directories_created']}
- Ù†ÙˆØ¹ Ø§Ù„Ù†Ø³Ø®Ø©: HTML Ø«Ø§Ø¨Øª
"""
            
            readme_path = os.path.join(replica_dir, 'README.md')
            async with aiofiles.open(readme_path, 'w', encoding='utf-8') as f:
                await f.write(readme_content)
            
            replica_info['structure_created'] = True
            replica_info['launch_instructions'] = {
                'browser': f"file://{os.path.join(replica_dir, 'index.html')}",
                'python_server': f"cd {replica_dir} && python -m http.server 8000",
                'node_server': f"cd {replica_dir} && npx serve .",
                'access_url': "http://localhost:8000"
            }
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {e}")
        
        return replica_info
    
    def _fix_asset_paths(self, html_content: str) -> str:
        """Ø¥ØµÙ„Ø§Ø­ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø£ØµÙˆÙ„ ÙÙŠ HTML"""
        # ØªØ­Ø¯ÙŠØ« Ù…Ø³Ø§Ø±Ø§Øª CSS
        html_content = re.sub(r'href=["\']([^"\']*\.css)["\']', r'href="css/\1"', html_content)
        # ØªØ­Ø¯ÙŠØ« Ù…Ø³Ø§Ø±Ø§Øª JavaScript
        html_content = re.sub(r'src=["\']([^"\']*\.js)["\']', r'src="js/\1"', html_content)
        # ØªØ­Ø¯ÙŠØ« Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ±
        html_content = re.sub(r'src=["\']([^"\']*\.(jpg|jpeg|png|gif|svg|ico))["\']', r'src="images/\1"', html_content)
        
        return html_content
    
    async def _copy_directory_contents(self, src_dir: str, dst_dir: str):
        """Ù†Ø³Ø® Ù…Ø­ØªÙˆÙŠØ§Øª Ù…Ø¬Ù„Ø¯"""
        if os.path.exists(src_dir):
            for item in os.listdir(src_dir):
                src_path = os.path.join(src_dir, item)
                dst_path = os.path.join(dst_dir, item)
                
                if os.path.isfile(src_path):
                    async with aiofiles.open(src_path, 'rb') as src_file:
                        content = await src_file.read()
                        async with aiofiles.open(dst_path, 'wb') as dst_file:
                            await dst_file.write(content)
        
    async def _copy_and_modify_files(self):
        """Ù†Ø³Ø® ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        self.logger.info("Ù†Ø³Ø® ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        try:
            replica_dir = os.path.join(self.result.output_path, "05_cloned_site")
            
            # ØªØ­Ø¯ÙŠØ« Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª HTML Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            for html_file in os.listdir(replica_dir):
                if html_file.endswith('.html'):
                    file_path = os.path.join(replica_dir, html_file)
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                    
                    # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØ³ÙˆØ±Ø©
                    content = self._fix_broken_links(content)
                    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ù…ÙƒØ³ÙˆØ±Ø©
                    content = self._clean_external_references(content)
                    
                    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                        await f.write(content)
            
            # ØªØ­Ø¯ÙŠØ« Ù…Ù„ÙØ§Øª CSS Ù„Ø¥ØµÙ„Ø§Ø­ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ±
            css_dir = os.path.join(replica_dir, "css")
            if os.path.exists(css_dir):
                for css_file in os.listdir(css_dir):
                    if css_file.endswith('.css'):
                        file_path = os.path.join(css_dir, css_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                        
                        # Ø¥ØµÙ„Ø§Ø­ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ± ÙÙŠ CSS
                        content = re.sub(r'url\(["\']?([^"\']*\.(jpg|jpeg|png|gif|svg))["\']?\)', r'url("../images/\1")', content)
                        
                        async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                            await f.write(content)
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù†Ø³Ø® ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª: {e}")
    
    def _fix_broken_links(self, html_content: str) -> str:
        """Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØ³ÙˆØ±Ø©"""
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ù…Ø­Ù„ÙŠØ©
        html_content = re.sub(r'href=["\']\.\.?/([^"\']*)["\']', r'href="\1"', html_content)
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØ§Ø±ØºØ©
        html_content = re.sub(r'href=["\']["\']', r'href="#"', html_content)
        return html_content
    
    def _clean_external_references(self, html_content: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"""
        # Ø¥Ø²Ø§Ù„Ø© Google Analytics Ùˆ tracking scripts
        html_content = re.sub(r'<script[^>]*google-analytics[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<script[^>]*gtag[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        # Ø¥Ø²Ø§Ù„Ø© Facebook Pixel
        html_content = re.sub(r'<script[^>]*facebook[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        return html_content
        
    async def _create_routing_system(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®"""
        self.logger.info("Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡...")
        
        try:
            replica_dir = os.path.join(self.result.output_path, "05_cloned_site")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù .htaccess Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø«Ø§Ø¨ØªØ©
            htaccess_content = """# Website Cloner Pro - Routing Configuration
DirectoryIndex index.html
ErrorDocument 404 /index.html

# Security Headers
Header always set X-Frame-Options DENY
Header always set X-Content-Type-Options nosniff
Header always set X-XSS-Protection "1; mode=block"

# Cache Control
<IfModule mod_expires.c>
    ExpiresActive on
    ExpiresByType text/css "access plus 1 year"
    ExpiresByType application/javascript "access plus 1 year"
    ExpiresByType image/png "access plus 1 year"
    ExpiresByType image/jpg "access plus 1 year"
    ExpiresByType image/jpeg "access plus 1 year"
    ExpiresByType image/gif "access plus 1 year"
</IfModule>
"""
            
            htaccess_path = os.path.join(replica_dir, '.htaccess')
            async with aiofiles.open(htaccess_path, 'w', encoding='utf-8') as f:
                await f.write(htaccess_content)
            
            # Ø¥Ù†Ø´Ø§Ø¡ server configuration Ù„Ù„ØªØ·ÙˆÙŠØ±
            server_config = {
                'python': {
                    'command': 'python -m http.server 8000',
                    'description': 'Python HTTP Server - Ù„Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø³Ø±ÙŠØ¹'
                },
                'node': {
                    'command': 'npx serve . -p 8000',
                    'description': 'Node.js Serve - Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ'
                },
                'php': {
                    'command': 'php -S localhost:8000',
                    'description': 'PHP Built-in Server - Ù„Ù„ØªØ·ÙˆÙŠØ±'
                }
            }
            
            server_config_path = os.path.join(replica_dir, 'server-config.json')
            async with aiofiles.open(server_config_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(server_config, ensure_ascii=False, indent=2))
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡: {e}")
        
    async def _setup_local_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®"""
        self.logger.info("Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©...")
        
        try:
            db_dir = os.path.join(self.result.output_path, "07_databases")
            os.makedirs(db_dir, exist_ok=True)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®
            db_path = os.path.join(db_dir, "cloned_site.db")
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… sqlite3 Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ø£Ø³Ø§Ø³ÙŠØ©
            import sqlite3
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
            database_analysis = await self._analyze_database_structure()
            
            for table_info in database_analysis.get('potential_tables', []):
                table_name = table_info['table_name']
                fields = table_info['fields']
                
                # Ø¥Ù†Ø´Ø§Ø¡ SQL CREATE TABLE
                create_sql = f"CREATE TABLE IF NOT EXISTS {table_name} (\n"
                create_sql += "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n"
                
                for field in fields:
                    field_name = field['field']
                    field_type = self._sql_type_from_html_type(field['type'])
                    create_sql += f"    {field_name} {field_type},\n"
                
                create_sql += "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n"
                create_sql += ");"
                
                cursor.execute(create_sql)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS site_analytics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    page_url TEXT,
                    visit_count INTEGER DEFAULT 0,
                    last_visit TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            """)
            
            conn.commit()
            conn.close()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            db_config = {
                'database_file': 'cloned_site.db',
                'type': 'sqlite',
                'tables_created': len(database_analysis.get('potential_tables', [])) + 1,
                'setup_date': datetime.now().isoformat(),
                'connection_string': f"sqlite:///{db_path}"
            }
            
            config_path = os.path.join(db_dir, 'database_config.json')
            async with aiofiles.open(config_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(db_config, ensure_ascii=False, indent=2))
            
            self.logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {db_path}")
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {e}")
    
    def _sql_type_from_html_type(self, html_type: str) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ù†ÙˆØ¹ HTML Ø¥Ù„Ù‰ Ù†ÙˆØ¹ SQL"""
        type_mapping = {
            'email': 'TEXT',
            'password': 'TEXT',
            'text': 'TEXT',
            'textarea': 'TEXT',
            'number': 'INTEGER',
            'tel': 'TEXT',
            'url': 'TEXT',
            'date': 'DATE',
            'datetime-local': 'TIMESTAMP',
            'checkbox': 'BOOLEAN',
            'radio': 'TEXT',
            'select': 'TEXT',
            'hidden': 'TEXT'
        }
        return type_mapping.get(html_type.lower(), 'TEXT')
        
    async def _test_links(self) -> Dict[str, Any]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØµØ­ØªÙ‡Ø§"""
        self.logger.info("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·...")
        
        link_test_results = {
            'total_links': 0,
            'working_links': 0,
            'broken_links': 0,
            'external_links': 0,
            'internal_links': 0,
            'link_details': [],
            'broken_link_details': []
        }
        
        try:
            content_dir = os.path.join(self.result.output_path, "01_extracted_content")
            if os.path.exists(content_dir):
                for html_file in os.listdir(content_dir):
                    if html_file.endswith('.html'):
                        file_path = os.path.join(content_dir, html_file)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            soup = BeautifulSoup(content, 'html.parser')
                            
                            links = soup.find_all('a', href=True)
                            for link in links:
                                if isinstance(link, Tag):
                                    href = str(link.get('href', ''))
                                    if href and href != '#':
                                        link_test_results['total_links'] += 1
                                        
                                        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø±Ø§Ø¨Ø·
                                        is_external = href.startswith(('http://', 'https://')) and not href.startswith(self.config.target_url)
                                        
                                        if is_external:
                                            link_test_results['external_links'] += 1
                                        else:
                                            link_test_results['internal_links'] += 1
                                        
                                        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· (Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© ÙÙ‚Ø·)
                                        if is_external:
                                            try:
                                                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                                                    async with session.head(href) as response:
                                                        if response.status < 400:
                                                            link_test_results['working_links'] += 1
                                                        else:
                                                            link_test_results['broken_links'] += 1
                                                            link_test_results['broken_link_details'].append({
                                                                'url': href,
                                                                'status': response.status,
                                                                'text': link.get_text(strip=True)
                                                            })
                                            except:
                                                link_test_results['broken_links'] += 1
                                                link_test_results['broken_link_details'].append({
                                                    'url': href,
                                                    'status': 'timeout/error',
                                                    'text': link.get_text(strip=True)
                                                })
                                        else:
                                            # Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ØªØ¹ØªØ¨Ø± ØªØ¹Ù…Ù„ Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹
                                            link_test_results['working_links'] += 1
                        
                        break  # ÙØ­Øµ Ø£ÙˆÙ„ Ù…Ù„Ù ÙÙ‚Ø· Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
        
        return link_test_results
        return {'working': 0, 'broken': 0}
        
    async def _validate_files(self) -> Dict[str, Any]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ù„ÙØ§Øª"""
        return {'valid': 0, 'invalid': 0}
        
    async def _generate_comprehensive_report(self):
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        pass
        
    async def _create_export_files(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØµØ¯ÙŠØ±"""
        pass
        
    async def _generate_checksums(self):
        """Ø¥Ù†ØªØ§Ø¬ checksums"""
        pass
        
    async def _create_readme_file(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù README"""
        pass


async def main():
    """ÙˆØ¸ÙŠÙØ© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø©"""
    config = CloningConfig(
        target_url="https://example.com",
        max_depth=3,
        max_pages=50,
        extract_all_content=True,
        analyze_with_ai=True,
        generate_reports=True
    )
    
    try:
        cloner = WebsiteClonerPro(config)
        result = await cloner.clone_website_complete(config.target_url)
        
        if result.success:
            print("âœ… ØªÙ… Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ù†Ø¬Ø§Ø­!")
            print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {result.output_path}")
            print(f"ğŸ“Š ØµÙØ­Ø§Øª Ù…Ø³ØªØ®Ø±Ø¬Ø©: {result.pages_extracted}")
            print(f"ğŸ¯ Ø£ØµÙˆÙ„ Ù…Ø­Ù…Ù„Ø©: {result.assets_downloaded}")
        else:
            print("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆÙ‚Ø¹")
            print(f"ğŸ” Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {len(result.error_log)}")
            
    except Exception as e:
        print(f"ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„: {e}")

# Integration function for Flask app
def create_cloner_instance(target_url: str, **kwargs) -> WebsiteClonerPro:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…Ù† Ø£Ø¯Ø§Ø© Ø§Ù„Ù†Ø³Ø® Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Flask"""
    config = CloningConfig(
        target_url=target_url,
        max_depth=kwargs.get('max_depth', 3),
        max_pages=kwargs.get('max_pages', 50),
        extract_all_content=kwargs.get('extract_all_content', True),
        analyze_with_ai=kwargs.get('analyze_with_ai', True),
        generate_reports=kwargs.get('generate_reports', True)
    )
    return WebsiteClonerPro(config)

if __name__ == "__main__":
    asyncio.run(main())