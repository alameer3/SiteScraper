"""
Ø£Ø¯Ø§Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© - Advanced Website Extractor
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ø£ØµÙˆÙ„
"""

import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import time
import logging
import hashlib
import mimetypes
import json
from collections import defaultdict
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import shutil

class AdvancedExtractor:
    def __init__(self, base_url, output_dir="extracted_sites", max_depth=3, max_threads=5):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.output_dir = Path(output_dir)
        self.max_depth = max_depth
        self.max_threads = max_threads
        
        self.visited_urls = set()
        self.downloaded_files = set()
        self.stats = {
            'pages_downloaded': 0,
            'images_downloaded': 0,
            'css_files': 0,
            'js_files': 0,
            'other_files': 0,
            'total_size': 0,
            'errors': 0
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Session Ù„Ù„Ø·Ù„Ø¨Ø§Øª
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.site_dir = self.output_dir / self._generate_site_id()
        self.site_dir.mkdir(parents=True, exist_ok=True)
        
        # Ù…Ø¬Ù„Ø¯Ø§Øª ÙØ±Ø¹ÙŠØ©
        self.pages_dir = self.site_dir / "pages"
        self.assets_dir = self.site_dir / "assets"
        self.images_dir = self.assets_dir / "images"
        self.css_dir = self.assets_dir / "css"
        self.js_dir = self.assets_dir / "js"
        self.fonts_dir = self.assets_dir / "fonts"
        
        for dir_path in [self.pages_dir, self.images_dir, self.css_dir, self.js_dir, self.fonts_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

    def _generate_site_id(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        site_hash = hashlib.md5(f"{self.base_url}_{time.time()}".encode()).hexdigest()[:12]
        return f"site_{site_hash}"

    def _get_safe_filename(self, url):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù† Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        parsed = urlparse(url)
        path = parsed.path
        
        if not path or path == '/':
            return 'index.html'
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        path = path.split('?')[0]
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        filename = unquote(path.split('/')[-1])
        if not filename:
            return 'index.html'
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù…ØªØ¯Ø§Ø¯ HTML Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯
        if not '.' in filename:
            filename += '.html'
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ø¢Ù…Ù†Ø©
        safe_chars = '-_.() abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
        filename = ''.join(c for c in filename if c in safe_chars)
        
        return filename[:100]  # ØªØ­Ø¯ÙŠØ¯ Ø·ÙˆÙ„ Ø§Ù„Ø§Ø³Ù…

    def _download_file(self, url, file_path):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù…Ù† Ø±Ø§Ø¨Ø·"""
        try:
            response = self.session.get(url, timeout=10, stream=True)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = file_path.stat().st_size
            self.stats['total_size'] += file_size
            return True
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {url}: {e}")
            self.stats['errors'] += 1
            return False

    def _process_html_content(self, html_content, base_url):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­ØªÙˆÙ‰ HTML ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±
        for img in soup.find_all('img'):
            src = img.get('src')
            if src:
                img_url = urljoin(base_url, src)
                if self._is_same_domain(img_url):
                    local_path = self._download_asset(img_url, 'images')
                    if local_path:
                        img['src'] = f"../assets/images/{local_path.name}"

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª CSS
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                css_url = urljoin(base_url, href)
                if self._is_same_domain(css_url):
                    local_path = self._download_asset(css_url, 'css')
                    if local_path:
                        link['href'] = f"../assets/css/{local_path.name}"

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª JavaScript
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src:
                js_url = urljoin(base_url, src)
                if self._is_same_domain(js_url):
                    local_path = self._download_asset(js_url, 'js')
                    if local_path:
                        script['src'] = f"../assets/js/{local_path.name}"

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
        for a in soup.find_all('a', href=True):
            href = a.get('href')
            if href and not href.startswith(('http://', 'https://', 'mailto:', 'tel:', '#')):
                full_url = urljoin(base_url, href)
                if self._is_same_domain(full_url):
                    filename = self._get_safe_filename(full_url)
                    a['href'] = filename

        return str(soup)

    def _download_asset(self, url, asset_type):
        """ØªØ­Ù…ÙŠÙ„ Ø£ØµÙ„ Ù…Ø¹ÙŠÙ† (ØµÙˆØ±Ø©ØŒ CSSØŒ JS)"""
        if url in self.downloaded_files:
            return None
        
        try:
            parsed = urlparse(url)
            filename = parsed.path.split('/')[-1]
            
            if not filename:
                filename = f"asset_{len(self.downloaded_files)}"
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
            if not '.' in filename:
                content_type = self.session.head(url, timeout=5).headers.get('content-type', '')
                ext = mimetypes.guess_extension(content_type.split(';')[0])
                if ext:
                    filename += ext
            
            # Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ
            if asset_type == 'images':
                local_path = self.images_dir / filename
            elif asset_type == 'css':
                local_path = self.css_dir / filename
            elif asset_type == 'js':
                local_path = self.js_dir / filename
            else:
                local_path = self.assets_dir / filename
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
            if self._download_file(url, local_path):
                self.downloaded_files.add(url)
                self.stats[f'{asset_type.rstrip("s")}_files' if asset_type.endswith('s') else f'{asset_type}_downloaded'] += 1
                return local_path
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙ„ {url}: {e}")
            self.stats['errors'] += 1
        
        return None

    def _is_same_domain(self, url):
        """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚"""
        try:
            return urlparse(url).netloc == self.domain
        except:
            return False

    def _extract_page(self, url, depth=0):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
        if url in self.visited_urls or depth > self.max_depth:
            return []
        
        self.visited_urls.add(url)
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            if 'text/html' not in response.headers.get('content-type', ''):
                return []
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­ØªÙˆÙ‰ HTML
            processed_html = self._process_html_content(response.text, url)
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙØ­Ø©
            filename = self._get_safe_filename(url)
            page_path = self.pages_dir / filename
            
            with open(page_path, 'w', encoding='utf-8') as f:
                f.write(processed_html)
            
            self.stats['pages_downloaded'] += 1
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù„Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ
            soup = BeautifulSoup(response.text, 'html.parser')
            links = []
            
            for a in soup.find_all('a', href=True):
                href = a.get('href')
                if href:
                    full_url = urljoin(url, href)
                    if self._is_same_domain(full_url) and full_url not in self.visited_urls:
                        links.append(full_url)
            
            return links[:10]  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 10 Ø±ÙˆØ§Ø¨Ø· Ù„ÙƒÙ„ ØµÙØ­Ø©
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {url}: {e}")
            self.stats['errors'] += 1
            return []

    def extract_complete_site(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„"""
        logging.info(f"Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.base_url}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        urls_to_process = [self.base_url]
        
        for depth in range(self.max_depth + 1):
            if not urls_to_process:
                break
            
            logging.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙˆÙ‰ {depth}: {len(urls_to_process)} Ø±Ø§Ø¨Ø·")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„ØµÙØ­Ø§Øª
            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:
                future_to_url = {
                    executor.submit(self._extract_page, url, depth): url 
                    for url in urls_to_process
                }
                
                next_urls = []
                for future in as_completed(future_to_url):
                    try:
                        links = future.result()
                        next_urls.extend(links)
                    except Exception as e:
                        logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©: {e}")
            
            urls_to_process = list(set(next_urls))
            time.sleep(0.5)  # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ·
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        self._create_start_file()
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        self._create_extraction_report()
        
        logging.info(f"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {self.stats}")
        return self.site_dir, self.stats

    def _create_start_file(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬"""
        pages = list(self.pages_dir.glob("*.html"))
        
        html_content = f"""
<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ - {self.domain}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin: 0;
            padding: 20px;
            min-height: 100vh;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255,255,255,0.1);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 8px 32px rgba(0,0,0,0.3);
        }}
        h1 {{
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5rem;
        }}
        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .stat-card {{
            background: rgba(255,255,255,0.2);
            padding: 20px;
            border-radius: 15px;
            text-align: center;
        }}
        .stat-number {{
            font-size: 2rem;
            font-weight: bold;
            color: #ffd700;
        }}
        .pages-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
        }}
        .page-card {{
            background: rgba(255,255,255,0.15);
            padding: 20px;
            border-radius: 15px;
            transition: transform 0.3s ease;
        }}
        .page-card:hover {{
            transform: translateY(-5px);
        }}
        .page-link {{
            color: #fff;
            text-decoration: none;
            font-weight: bold;
        }}
        .page-link:hover {{
            color: #ffd700;
        }}
        .launch-btn {{
            background: linear-gradient(45deg, #ff6b6b, #ee5a24);
            border: none;
            color: white;
            padding: 15px 30px;
            border-radius: 25px;
            font-size: 1.1rem;
            cursor: pointer;
            margin: 10px;
            transition: all 0.3s ease;
        }}
        .launch-btn:hover {{
            transform: scale(1.05);
            box-shadow: 0 5px 20px rgba(0,0,0,0.3);
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø¨Ù†Ø¬Ø§Ø­</h1>
        <p style="text-align: center; font-size: 1.2rem; margin-bottom: 30px;">
            ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ù†: <strong>{self.base_url}</strong>
        </p>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{self.stats['pages_downloaded']}</div>
                <div>ØµÙØ­Ø©</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{self.stats['images_downloaded']}</div>
                <div>ØµÙˆØ±Ø©</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{self.stats['css_files']}</div>
                <div>Ù…Ù„Ù CSS</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{self.stats['js_files']}</div>
                <div>Ù…Ù„Ù JS</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{round(self.stats['total_size']/1024/1024, 2)} MB</div>
                <div>Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ</div>
            </div>
        </div>
        
        <div style="text-align: center; margin-bottom: 30px;">
            <button class="launch-btn" onclick="openMainPage()">
                ğŸš€ ÙØªØ­ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            </button>
            <button class="launch-btn" onclick="showAllPages()">
                ğŸ“ Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª
            </button>
        </div>
        
        <div class="pages-grid" id="pagesGrid">
"""
        
        # Ø¥Ø¶Ø§ÙØ© Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„ØµÙØ­Ø§Øª
        for i, page in enumerate(pages[:12]):  # Ø£ÙˆÙ„ 12 ØµÙØ­Ø©
            html_content += f"""
            <div class="page-card">
                <h3>ğŸ“„ ØµÙØ­Ø© {i+1}</h3>
                <a href="pages/{page.name}" target="_blank" class="page-link">
                    {page.name}
                </a>
            </div>
"""
        
        html_content += """
        </div>
    </div>
    
    <script>
        function openMainPage() {
            window.open('pages/index.html', '_blank');
        }
        
        function showAllPages() {
            const pages = """ + json.dumps([p.name for p in pages]) + """;
            let pagesList = 'Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©:\\n\\n';
            pages.forEach((page, index) => {
                pagesList += `${index + 1}. ${page}\\n`;
            });
            alert(pagesList);
        }
    </script>
</body>
</html>
"""
        
        start_file = self.site_dir / "index.html"
        with open(start_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

    def _create_extraction_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªÙØµÙŠÙ„ÙŠ Ø¹Ù† Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
        report = {
            'site_info': {
                'url': self.base_url,
                'domain': self.domain,
                'extraction_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'output_directory': str(self.site_dir)
            },
            'statistics': self.stats,
            'files': {
                'pages': [p.name for p in self.pages_dir.glob("*.html")],
                'images': [i.name for i in self.images_dir.glob("*")],
                'css': [c.name for c in self.css_dir.glob("*")],
                'js': [j.name for j in self.js_dir.glob("*")]
            }
        }
        
        report_file = self.site_dir / "extraction_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

def extract_website_advanced(url, max_depth=2, max_threads=3):
    """Ø¯Ø§Ù„Ø© Ø³Ù‡Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙ‚Ø¹ Ù…ØªÙ‚Ø¯Ù…"""
    extractor = AdvancedExtractor(url, max_depth=max_depth, max_threads=max_threads)
    return extractor.extract_complete_site()