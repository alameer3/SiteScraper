"""
Ø£Ø¯Ø§Ø© Ù…ØªØ·ÙˆØ±Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªÙ†Ø¸ÙŠÙ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù…Ù„ÙØ§Øª
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSS Ùˆ JavaScript
- ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙˆØ¯ Ù…Ù† Ø§Ù„ØªØªØ¨Ø¹ ÙˆØ§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
- Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù‡ÙŠÙƒÙ„ Ù„ÙŠÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ø§Ù‹ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
"""

import os
import re
import json
import requests
import trafilatura
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import logging
from pathlib import Path
import hashlib
import mimetypes
from typing import Dict, List, Tuple, Optional
import time
import cssutils
import base64

# ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø³Ø¬Ù„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WebsiteExtractor:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø§Ù…Ù„ Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø¹ Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ†"""
    
    def __init__(self, base_url: str, output_dir: str = "extracted_website"):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.output_dir = Path(output_dir)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.create_directories()
        
        # ØªÙØ¹ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø­Ø¬Ø¨ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ù…ØªØ·ÙˆØ±
        from advanced_ad_blocker import AdvancedAdBlocker
        self.ad_blocker = AdvancedAdBlocker()
        
        # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© (Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©)
        self.ad_selectors = [
            '[class*="ad"]', '[id*="ad"]', '[class*="advertisement"]',
            '[class*="banner"]', '[class*="popup"]', '[class*="modal"]',
            '.google-ads', '.adsense', '[class*="promo"]',
            '[data-ad]', '[data-google-ad]', '[class*="sponsor"]'
        ]
        
        self.tracking_domains = [
            'google-analytics.com', 'googletagmanager.com', 'facebook.com',
            'doubleclick.net', 'googlesyndication.com', 'amazon-adsystem.com',
            'adsystem.amazon.com', 'googleadservices.com'
        ]
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'pages_extracted': 0,
            'images_downloaded': 0,
            'css_files': 0,
            'js_files': 0,
            'ads_removed': 0,
            'tracking_removed': 0
        }

    def create_directories(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª"""
        directories = [
            self.output_dir,
            self.output_dir / 'pages',
            self.output_dir / 'assets' / 'images',
            self.output_dir / 'assets' / 'css',
            self.output_dir / 'assets' / 'js',
            self.output_dir / 'assets' / 'fonts',
            self.output_dir / 'content',
            self.output_dir / 'data'
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ ÙÙŠ: {self.output_dir}")

    def extract_complete_website(self, max_pages: int = 50) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        logger.info(f"Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.base_url}")
        
        extraction_report = {
            'url': self.base_url,
            'start_time': time.time(),
            'pages': [],
            'assets': {
                'images': [],
                'css': [],
                'js': [],
                'fonts': []
            },
            'content_summary': {},
            'cleaned_elements': {},
            'stats': {}
        }
        
        try:
            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            main_page = self.extract_page(self.base_url)
            if main_page:
                extraction_report['pages'].append(main_page)
                self.stats['pages_extracted'] += 1
            
            # 2. Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
            internal_links = self.find_internal_links(self.base_url)
            logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(internal_links)} Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ")
            
            # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
            for i, link in enumerate(internal_links[:max_pages-1]):
                try:
                    logger.info(f"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {i+2}/{min(max_pages, len(internal_links)+1)}: {link}")
                    page_data = self.extract_page(link)
                    if page_data:
                        extraction_report['pages'].append(page_data)
                        self.stats['pages_extracted'] += 1
                    time.sleep(0.5)  # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ·
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {link}: {e}")
            
            # 4. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            extraction_report['content_summary'] = self.create_content_summary(extraction_report['pages'])
            
            # 5. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ø¹
            self.create_site_structure(extraction_report)
            
            # 6. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ
            extraction_report['cleaned_elements'] = {
                'ads_removed': self.stats['ads_removed'],
                'tracking_removed': self.stats['tracking_removed'],
                'cleaned_selectors': self.ad_selectors
            }
            
            extraction_report['stats'] = self.stats.copy()
            extraction_report['end_time'] = time.time()
            extraction_report['duration'] = extraction_report['end_time'] - extraction_report['start_time']
            
            # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            self.save_extraction_report(extraction_report)
            
            logger.info(f"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬! ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {self.stats['pages_extracted']} ØµÙØ­Ø©")
            return extraction_report
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„: {e}")
            return extraction_report

    def extract_page(self, url: str) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ø´Ø§Ù…Ù„"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØµÙØ­Ø© Ù…Ù† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ§Ù„ØªØªØ¨Ø¹
            cleaned_soup = self.clean_page(soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ
            text_content = self.extract_text_content(url, cleaned_soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø¸Ù…Ø©
            structured_data = self.extract_structured_data(cleaned_soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ØµÙˆÙ„ (CSS, JS, Images)
            assets = self.extract_assets(url, cleaned_soup, response.content)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù HTML Ù†Ø¸ÙŠÙ
            clean_html = self.create_clean_html(cleaned_soup, assets)
            
            page_data = {
                'url': url,
                'title': cleaned_soup.title.get_text().strip() if cleaned_soup.title else 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†',
                'text_content': text_content,
                'structured_data': structured_data,
                'assets': assets,
                'clean_html': clean_html,
                'meta_data': self.extract_meta_data(cleaned_soup),
                'word_count': len(text_content.split()) if text_content else 0,
                'extraction_time': time.time()
            }
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙØ­Ø©
            self.save_page(page_data)
            
            return page_data
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {url}: {e}")
            return None

    def clean_page(self, soup: BeautifulSoup) -> BeautifulSoup:
        """ØªÙ†Ø¸ÙŠÙ Ù…ØªØ·ÙˆØ± Ù„Ù„ØµÙØ­Ø© Ù…Ù† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ§Ù„ØªØªØ¨Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI"""
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªØ·ÙˆØ± Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
        html_content = str(soup)
        cleaned_html, cleaning_report = self.ad_blocker.clean_html_content(html_content)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ù† Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ØªØ·ÙˆØ±
        ads_blocked = len([x for x in cleaning_report.get('removed_elements', []) if 'CSS:' in x or 'Smart:' in x])
        tracking_blocked = cleaning_report.get('cleaned_scripts', 0)
        
        self.stats['ads_removed'] += ads_blocked
        self.stats['tracking_removed'] += tracking_blocked
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ­ÙˆÙŠÙ„ HTML Ø§Ù„Ù…Ù†Ø¸Ù Ø¥Ù„Ù‰ BeautifulSoup
        cleaned_soup = BeautifulSoup(cleaned_html, 'html.parser')
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø·Ø¨Ù‚Ø© ØªÙ†Ø¸ÙŠÙ Ø¥Ø¶Ø§ÙÙŠØ© ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
        cleaned_soup = self._apply_traditional_cleaning(cleaned_soup)
        
        logger.info(f"ğŸ›¡ï¸ ØªÙ… ØªÙ†Ø¸ÙŠÙ {ads_blocked} Ø¥Ø¹Ù„Ø§Ù† Ù…ØªØ·ÙˆØ± Ùˆ {tracking_blocked} Ø¹Ù†ØµØ± ØªØªØ¨Ø¹ Ø°ÙƒÙŠ")
        
        return cleaned_soup
    
    def _apply_traditional_cleaning(self, soup: BeautifulSoup) -> BeautifulSoup:
        """ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ø¸ÙŠÙ ØªÙ‚Ù„ÙŠØ¯ÙŠ ÙƒØ·Ø¨Ù‚Ø© Ø¥Ø¶Ø§ÙÙŠØ©"""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
        traditional_ads = 0
        for selector in self.ad_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    element.decompose()
                    traditional_ads += 1
            except Exception as e:
                logger.debug(f"ØªØ­Ø°ÙŠØ± ÙÙŠ selector {selector}: {e}")
        
        # Ø¥Ø²Ø§Ù„Ø© Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
        traditional_tracking = 0
        scripts = soup.find_all('script')
        for script in scripts:
            if script.get('src'):
                src = script.get('src')
                if any(domain in src for domain in self.tracking_domains):
                    script.decompose()
                    traditional_tracking += 1
            elif script.string:
                script_content = script.string.lower()
                if any(domain in script_content for domain in self.tracking_domains):
                    script.decompose()
                    traditional_tracking += 1
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©
        from bs4 import Comment
        comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for comment in comments:
            comment_text = str(comment).lower()
            if any(keyword in comment_text for keyword in ['ad', 'track', 'analytics', 'google']):
                comment.extract()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ÙÙŠØ© Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©
        hidden_elements = soup.find_all(attrs={'style': re.compile(r'display\s*:\s*none|visibility\s*:\s*hidden')})
        for element in hidden_elements:
            element_text = element.get_text(strip=True).lower()
            if any(keyword in element_text for keyword in ['ad', 'advertisement', 'sponsor', 'promo']):
                element.decompose()
        
        if traditional_ads > 0 or traditional_tracking > 0:
            logger.info(f"â• Ø·Ø¨Ù‚Ø© Ø¥Ø¶Ø§ÙÙŠØ©: {traditional_ads} Ø¥Ø¹Ù„Ø§Ù† ØªÙ‚Ù„ÙŠØ¯ÙŠ Ùˆ {traditional_tracking} ØªØªØ¨Ø¹ ØªÙ‚Ù„ÙŠØ¯ÙŠ")
        
        return soup

    def extract_text_content(self, url: str, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…ÙÙŠØ¯"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… trafilatura Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            html_content = str(soup)
            extracted_text = trafilatura.extract(html_content, include_comments=False, include_tables=True)
            
            if extracted_text:
                return extracted_text
            
            # Ø·Ø±ÙŠÙ‚Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            main_content = soup.find('main') or soup.find('article') or soup.find(class_=re.compile(r'content|main|article'))
            if main_content:
                return main_content.get_text(separator=' ', strip=True)
            
            return soup.get_text(separator=' ', strip=True)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† {url}: {e}")
            return ""

    def extract_structured_data(self, soup: BeautifulSoup) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø¸Ù…Ø© (JSON-LD, microdata, etc.)"""
        structured_data = {
            'json_ld': [],
            'meta_tags': {},
            'open_graph': {},
            'twitter_card': {},
            'schema_org': []
        }
        
        # JSON-LD
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                structured_data['json_ld'].append(data)
            except:
                pass
        
        # Meta tags
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            name = meta.get('name') or meta.get('property') or meta.get('itemprop')
            content = meta.get('content')
            if name and content:
                structured_data['meta_tags'][name] = content
                
                # Open Graph
                if name.startswith('og:'):
                    structured_data['open_graph'][name[3:]] = content
                
                # Twitter Card
                if name.startswith('twitter:'):
                    structured_data['twitter_card'][name[8:]] = content
        
        return structured_data

    def extract_assets(self, page_url: str, soup: BeautifulSoup, page_content: bytes) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„"""
        assets = {
            'images': [],
            'css': [],
            'js': [],
            'fonts': []
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
        images = soup.find_all(['img', 'picture', 'source'])
        for img in images:
            src = img.get('src') or img.get('data-src') or img.get('srcset', '').split()[0] if img.get('srcset') else None
            if src:
                full_url = urljoin(page_url, src)
                saved_path = self.download_asset(full_url, 'images')
                if saved_path:
                    assets['images'].append({
                        'original_url': full_url,
                        'local_path': saved_path,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', '')
                    })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù„ÙØ§Øª CSS
        css_links = soup.find_all('link', rel='stylesheet')
        for link in css_links:
            href = link.get('href')
            if href:
                full_url = urljoin(page_url, href)
                saved_path = self.download_and_clean_css(full_url)
                if saved_path:
                    assets['css'].append({
                        'original_url': full_url,
                        'local_path': saved_path
                    })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù„ÙØ§Øª JavaScript
        js_scripts = soup.find_all('script', src=True)
        for script in js_scripts:
            src = script.get('src')
            if src and not any(domain in src for domain in self.tracking_domains):
                full_url = urljoin(page_url, src)
                saved_path = self.download_asset(full_url, 'js')
                if saved_path:
                    assets['js'].append({
                        'original_url': full_url,
                        'local_path': saved_path
                    })
        
        return assets

    def download_asset(self, url: str, asset_type: str) -> Optional[str]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙˆÙ„"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            parsed_url = urlparse(url)
            filename = os.path.basename(parsed_url.path) or f"asset_{hashlib.md5(url.encode()).hexdigest()[:8]}"
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
            content_type = response.headers.get('content-type', '')
            if not '.' in filename:
                extension = mimetypes.guess_extension(content_type) or ''
                filename += extension
            
            # Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸
            save_path = self.output_dir / 'assets' / asset_type / filename
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            if asset_type == 'images':
                self.stats['images_downloaded'] += 1
            elif asset_type == 'css':
                self.stats['css_files'] += 1
            elif asset_type == 'js':
                self.stats['js_files'] += 1
            
            return str(save_path.relative_to(self.output_dir))
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {url}: {e}")
            return None

    def download_and_clean_css(self, url: str) -> Optional[str]:
        """ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª CSS Ù…Ø¹ Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            css_content = response.text
            
            # ØªÙ†Ø¸ÙŠÙ CSS Ù…Ù† Ø§Ù„Ø®Ø·ÙˆØ· ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ù‡Ù†Ø§
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
            filename = f"style_{hashlib.md5(url.encode()).hexdigest()[:8]}.css"
            save_path = self.output_dir / 'assets' / 'css' / filename
            
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(css_content)
            
            self.stats['css_files'] += 1
            return str(save_path.relative_to(self.output_dir))
            
        except Exception as e:
            logger.warning(f"ØªØ®Ø·ÙŠ Ù…Ù„Ù CSS {url}: {e}")
            return None

    def find_internal_links(self, url: str) -> List[str]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©"""
        try:
            response = self.session.get(url, timeout=30)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            internal_links = set()
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ
                if urlparse(full_url).netloc == self.domain:
                    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø±Ø§Ø³ÙŠ
                    clean_url = full_url.split('#')[0].split('?')[0]
                    if clean_url != url and clean_url not in internal_links:
                        internal_links.add(clean_url)
            
            return list(internal_links)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
            return []

    def extract_meta_data(self, soup: BeautifulSoup) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©"""
        meta_data = {
            'title': soup.title.get_text().strip() if soup.title else '',
            'description': '',
            'keywords': '',
            'author': '',
            'lang': soup.get('lang', ''),
            'charset': ''
        }
        
        # Meta description
        desc_meta = soup.find('meta', attrs={'name': 'description'})
        if desc_meta:
            meta_data['description'] = desc_meta.get('content', '')
        
        # Meta keywords
        keywords_meta = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_meta:
            meta_data['keywords'] = keywords_meta.get('content', '')
        
        # Author
        author_meta = soup.find('meta', attrs={'name': 'author'})
        if author_meta:
            meta_data['author'] = author_meta.get('content', '')
        
        # Charset
        charset_meta = soup.find('meta', attrs={'charset': True})
        if charset_meta:
            meta_data['charset'] = charset_meta.get('charset', '')
        
        return meta_data

    def create_clean_html(self, soup: BeautifulSoup, assets: Dict) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ HTML Ù†Ø¸ÙŠÙ Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
        # ØªØ­Ø¯ÙŠØ« Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£ØµÙˆÙ„ Ù„ØªØ´ÙŠØ± Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        for img in soup.find_all('img'):
            src = img.get('src')
            if src:
                for asset in assets['images']:
                    if asset['original_url'].endswith(src) or src in asset['original_url']:
                        img['src'] = asset['local_path']
                        break
        
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                for asset in assets['css']:
                    if asset['original_url'].endswith(href) or href in asset['original_url']:
                        link['href'] = asset['local_path']
                        break
        
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src:
                for asset in assets['js']:
                    if asset['original_url'].endswith(src) or src in asset['original_url']:
                        script['src'] = asset['local_path']
                        break
        
        return str(soup.prettify())

    def create_content_summary(self, pages: List[Dict]) -> Dict:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø­ØªÙˆÙ‰"""
        summary = {
            'total_pages': len(pages),
            'total_words': 0,
            'common_keywords': [],
            'content_types': {},
            'page_titles': [],
            'meta_descriptions': []
        }
        
        all_text = ""
        for page in pages:
            summary['total_words'] += page.get('word_count', 0)
            all_text += " " + (page.get('text_content', '') or '')
            
            if page.get('title'):
                summary['page_titles'].append(page['title'])
            
            if page.get('meta_data', {}).get('description'):
                summary['meta_descriptions'].append(page['meta_data']['description'])
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (ØªØ­Ø³ÙŠÙ† Ø¨Ø³ÙŠØ·)
        words = re.findall(r'\b\w+\b', all_text.lower())
        word_freq = {}
        for word in words:
            if len(word) > 3:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Ø£Ø´Ù‡Ø± 20 ÙƒÙ„Ù…Ø©
        summary['common_keywords'] = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]
        
        return summary

    def save_page(self, page_data: Dict):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø©"""
        url_hash = hashlib.md5(page_data['url'].encode()).hexdigest()[:8]
        
        # Ø­ÙØ¸ HTML
        html_path = self.output_dir / 'pages' / f"{url_hash}.html"
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(page_data['clean_html'])
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ
        text_path = self.output_dir / 'content' / f"{url_hash}.txt"
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(page_data['text_content'])
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
        json_path = self.output_dir / 'data' / f"{url_hash}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                'url': page_data['url'],
                'title': page_data['title'],
                'meta_data': page_data['meta_data'],
                'structured_data': page_data['structured_data'],
                'word_count': page_data['word_count'],
                'extraction_time': page_data['extraction_time']
            }, f, ensure_ascii=False, indent=2)

    def create_site_structure(self, extraction_report: Dict):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„ØµÙØ­Ø§Øª
        index_data = {
            'site_url': self.base_url,
            'extraction_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pages': [
                {
                    'url': page['url'],
                    'title': page['title'],
                    'word_count': page['word_count']
                }
                for page in extraction_report['pages']
            ]
        }
        
        with open(self.output_dir / 'site_index.json', 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        # Ø¥Ù†Ø´Ø§Ø¡ README
        readme_content = f"""# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.base_url}

## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
- ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {time.strftime('%Y-%m-%d %H:%M:%S')}
- Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {self.stats['pages_extracted']}
- Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±: {self.stats['images_downloaded']}
- Ù…Ù„ÙØ§Øª CSS: {self.stats['css_files']}
- Ù…Ù„ÙØ§Øª JavaScript: {self.stats['js_files']}

## Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø·Ø¨Ù‚
- Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù…Ø­Ø°ÙˆÙØ©: {self.stats['ads_removed']}
- Ø¹Ù†Ø§ØµØ± ØªØªØ¨Ø¹ Ù…Ø­Ø°ÙˆÙØ©: {self.stats['tracking_removed']}

## Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
- `pages/`: Ù…Ù„ÙØ§Øª HTML Ø§Ù„Ù†Ø¸ÙŠÙØ©
- `content/`: Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
- `assets/`: Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„ (ØµÙˆØ±ØŒ CSSØŒ JS)
- `data/`: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© ÙˆØ§Ù„Ù…Ù†Ø¸Ù…Ø©

## Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
1. Ø§Ù†Ø³Ø® Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ù…Ø¬Ù„Ø¯ `pages/` ÙƒÙ†Ù‚Ø·Ø© Ø¨Ø¯Ø§ÙŠØ©
2. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† `content/` Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©
3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ØµÙˆÙ„ Ù…Ù† `assets/` Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
4. Ø±Ø§Ø¬Ø¹ `data/` Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø¸Ù…Ø© ÙˆØ§Ù„ÙˆØµÙÙŠØ©
"""
        
        with open(self.output_dir / 'README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)

    def save_extraction_report(self, report: Dict):
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„"""
        with open(self.output_dir / 'extraction_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„ ÙÙŠ: {self.output_dir / 'extraction_report.json'}")

def extract_website_cli():
    """ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙˆÙŠØ¨')
    parser.add_argument('url', help='Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡')
    parser.add_argument('-o', '--output', default='extracted_website', help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬')
    parser.add_argument('-p', '--pages', type=int, default=50, help='Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù‚ØµÙˆÙ‰')
    
    args = parser.parse_args()
    
    extractor = WebsiteExtractor(args.url, args.output)
    report = extractor.extract_complete_website(args.pages)
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬!")
    print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {report['stats']['pages_extracted']}")
    print(f"ğŸ–¼ï¸ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±: {report['stats']['images_downloaded']}")
    print(f"ğŸ¨ Ù…Ù„ÙØ§Øª CSS: {report['stats']['css_files']}")
    print(f"âš¡ Ù…Ù„ÙØ§Øª JS: {report['stats']['js_files']}")
    print(f"ğŸ§¹ Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù…Ø­Ø°ÙˆÙØ©: {report['stats']['ads_removed']}")
    print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {args.output}")

if __name__ == '__main__':
    extract_website_cli()