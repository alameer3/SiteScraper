#!/usr/bin/env python3
"""
Ø£Ù…Ø«Ù„Ø© Ù„ØªØ´ØºÙŠÙ„ unified_extractor
"""

from unified_extractor import UnifiedWebsiteExtractor
import json

def main():
    # Ø¥Ù†Ø´Ø§Ø¡ instance Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
    extractor = UnifiedWebsiteExtractor()
    
    # Ù…Ø«Ø§Ù„ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ø§Ø³ÙŠ
    print("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ø§Ø³ÙŠ...")
    result = extractor.extract_website("https://httpbin.org", "basic")
    print(f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {result['success']}")
    
    # Ù…Ø«Ø§Ù„ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ‚Ø¯Ù…
    print("\nğŸš€ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ‚Ø¯Ù…...")
    result = extractor.extract_website("https://example.com", "advanced")
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬:")
    print(f"- Ø§Ù„Ù…ÙˆÙ‚Ø¹: {result.get('url', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
    print(f"- Ø§Ù„Ù†Ø¬Ø§Ø­: {result.get('success', False)}")
    print(f"- Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {result.get('pages_analyzed', 0)}")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    with open('extraction_results.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: extraction_results.json")

if __name__ == "__main__":
    main()