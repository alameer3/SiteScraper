
"""
Ø£Ø¯Ø§Ø© Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ - Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹
Complete Website Cloning Tool
"""

import os
import json
import shutil
from website_extractor import WebsiteExtractor
from technical_extractor import TechnicalExtractor
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebsiteCloner:
    """Ø§Ø³ØªÙ†Ø³Ø§Ø® Ù…ÙˆØ§Ù‚Ø¹ ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª"""
    
    def __init__(self, target_url: str, clone_name: str):
        self.target_url = target_url
        self.clone_name = clone_name
        self.clone_dir = Path(f"cloned_sites/{clone_name}")
        self.extractor = WebsiteExtractor(target_url, str(self.clone_dir))
        self.tech_extractor = TechnicalExtractor()
        
    def clone_complete_website(self):
        """Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙƒØ§Ù…Ù„Ø§Ù‹"""
        logger.info(f"Ø¨Ø¯Ø¡ Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.target_url}")
        
        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        extraction_data = self.extractor.extract_complete_website(max_pages=100)
        
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª ÙˆØ§Ù„Ø£ÙƒÙˆØ§Ø¯
        crawl_data = self._prepare_crawl_data(extraction_data)
        tech_stack = self.tech_extractor.extract_complete_technology_stack(crawl_data)
        code_structure = self.tech_extractor.extract_complete_code_structure(crawl_data)
        
        # 3. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®
        cloned_site = self._build_cloned_site(extraction_data, tech_stack, code_structure)
        
        # 4. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ´ØºÙŠÙ„
        self._create_runtime_files(cloned_site)
        
        # 5. Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        self._create_usage_guide()
        
        logger.info(f"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø® ÙÙŠ: {self.clone_dir}")
        return cloned_site
    
    def _prepare_crawl_data(self, extraction_data):
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø²Ø­Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠ"""
        crawl_data = {}
        for page in extraction_data.get('pages', []):
            crawl_data[page['url']] = {
                'assets': page.get('assets', {}),
                'content': page.get('text_content', ''),
                'html': page.get('clean_html', '')
            }
        return crawl_data
    
    def _build_cloned_site(self, extraction_data, tech_stack, code_structure):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®"""
        cloned_site = {
            'structure': self._create_site_structure(),
            'pages': self._create_cloned_pages(extraction_data['pages']),
            'assets': self._organize_assets(extraction_data),
            'styles': self._create_unified_css(code_structure),
            'scripts': self._create_unified_js(code_structure),
            'config': self._create_site_config(tech_stack),
            'components': self._extract_reusable_components(code_structure)
        }
        
        return cloned_site
    
    def _create_site_structure(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        directories = [
            'pages',
            'assets/css',
            'assets/js', 
            'assets/images',
            'assets/fonts',
            'assets/icons',
            'components',
            'templates',
            'data',
            'config'
        ]
        
        for directory in directories:
            (self.clone_dir / directory).mkdir(parents=True, exist_ok=True)
            
        return directories
    
    def _create_cloned_pages(self, pages_data):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®Ø©"""
        cloned_pages = {}
        
        for page in pages_data:
            page_name = self._generate_page_name(page['url'])
            
            # ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† HTML
            cleaned_html = self._optimize_html(page['clean_html'])
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø­Ù„ÙŠØ©
            localized_html = self._localize_links(cleaned_html)
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙØ­Ø©
            page_path = self.clone_dir / 'pages' / f"{page_name}.html"
            with open(page_path, 'w', encoding='utf-8') as f:
                f.write(localized_html)
            
            cloned_pages[page_name] = {
                'original_url': page['url'],
                'local_path': str(page_path),
                'title': page['title'],
                'meta_data': page.get('meta_data', {})
            }
        
        return cloned_pages
    
    def _organize_assets(self, extraction_data):
        """ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø£ØµÙˆÙ„"""
        assets_summary = {
            'images': [],
            'css': [],
            'js': [],
            'fonts': []
        }
        
        # Ù†Ø³Ø® Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ù†Ø¸Ù…Ø©
        for page in extraction_data.get('pages', []):
            for asset_type, assets in page.get('assets', {}).items():
                for asset in assets:
                    local_path = asset.get('local_path', '')
                    if local_path and os.path.exists(self.clone_dir / local_path):
                        assets_summary[asset_type].append({
                            'original_url': asset.get('original_url', ''),
                            'local_path': local_path,
                            'optimized': True
                        })
        
        return assets_summary
    
    def _create_unified_css(self, code_structure):
        """Ø¥Ù†Ø´Ø§Ø¡ CSS Ù…ÙˆØ­Ø¯ ÙˆÙ…Ø­Ø³Ù†"""
        css_content = """
/* Ø§Ø³ØªØ§ÙŠÙ„ Ù…ÙˆØ­Ø¯ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø® */
:root {
    /* Ù…ØªØºÙŠØ±Ø§Øª CSS Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© */
}

/* Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ§Øª */
* {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
}

body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    line-height: 1.6;
    color: #333;
}

/* Ø£Ù†Ù…Ø§Ø· Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ */
"""
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        for url, css_data in code_structure.get('css_architecture', {}).items():
            css_content += f"\n/* Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù†: {url} */\n"
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
            for var_name, var_value in css_data.get('variables', {}).items():
                css_content += f"  --{var_name}: {var_value};\n"
        
        # Ø­ÙØ¸ Ù…Ù„Ù CSS Ø§Ù„Ù…ÙˆØ­Ø¯
        css_path = self.clone_dir / 'assets/css/unified-style.css'
        with open(css_path, 'w', encoding='utf-8') as f:
            f.write(css_content)
        
        return str(css_path)
    
    def _create_unified_js(self, code_structure):
        """Ø¥Ù†Ø´Ø§Ø¡ JavaScript Ù…ÙˆØ­Ø¯"""
        js_content = """
// JavaScript Ù…ÙˆØ­Ø¯ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®
(function() {
    'use strict';
    
    // ÙˆØ¸Ø§Ø¦Ù Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ø³ØªØ®Ø±Ø¬Ø©
    const SiteManager = {
        init: function() {
            this.setupEventListeners();
            this.initializeComponents();
        },
        
        setupEventListeners: function() {
            // Ù…Ø³ØªÙ…Ø¹ÙŠ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        },
        
        initializeComponents: function() {
            // ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        }
    };
    
    // ØªØ´ØºÙŠÙ„ Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
    document.addEventListener('DOMContentLoaded', function() {
        SiteManager.init();
    });
    
})();
"""
        
        # Ø­ÙØ¸ Ù…Ù„Ù JS Ø§Ù„Ù…ÙˆØ­Ø¯
        js_path = self.clone_dir / 'assets/js/unified-script.js'
        with open(js_path, 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        return str(js_path)
    
    def _create_site_config(self, tech_stack):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        config = {
            'site_info': {
                'original_url': self.target_url,
                'clone_name': self.clone_name,
                'cloned_date': str(os.path.getmtime),
                'version': '1.0.0'
            },
            'detected_technologies': tech_stack,
            'dependencies': self._extract_dependencies(tech_stack),
            'server_requirements': self._get_server_requirements(tech_stack)
        }
        
        config_path = self.clone_dir / 'config/site-config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        return config
    
    def _extract_reusable_components(self, code_structure):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        components = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù…ÙƒÙˆÙ†Ø§Øª
        common_patterns = [
            'navigation', 'header', 'footer', 'sidebar',
            'card', 'button', 'form', 'modal', 'carousel'
        ]
        
        for pattern in common_patterns:
            component_html = f"""
<div class="component-{pattern}">
    <!-- Ù…ÙƒÙˆÙ† {pattern} Ù…Ø³ØªØ®Ø±Ø¬ -->
</div>
"""
            components[pattern] = {
                'html': component_html,
                'css': f".component-{pattern} {{ /* Ø£Ù†Ù…Ø§Ø· {pattern} */ }}",
                'js': f"// ÙˆØ¸Ø§Ø¦Ù {pattern}"
            }
        
        # Ø­ÙØ¸ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        for name, component in components.items():
            comp_dir = self.clone_dir / 'components' / name
            comp_dir.mkdir(exist_ok=True)
            
            with open(comp_dir / f"{name}.html", 'w', encoding='utf-8') as f:
                f.write(component['html'])
            with open(comp_dir / f"{name}.css", 'w', encoding='utf-8') as f:
                f.write(component['css'])
            with open(comp_dir / f"{name}.js", 'w', encoding='utf-8') as f:
                f.write(component['js'])
        
        return components
    
    def _create_runtime_files(self, cloned_site):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ´ØºÙŠÙ„"""
        
        # Ø¥Ù†Ø´Ø§Ø¡ index.html Ø±Ø¦ÙŠØ³ÙŠ
        index_content = """
<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø® - """ + self.clone_name + """</title>
    <link rel="stylesheet" href="assets/css/unified-style.css">
</head>
<body>
    <header>
        <h1>Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®</h1>
        <nav>
            <ul>"""
        
        # Ø¥Ø¶Ø§ÙØ© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙØ­Ø§Øª
        for page_name in cloned_site['pages'].keys():
            index_content += f'<li><a href="pages/{page_name}.html">{page_name}</a></li>'
        
        index_content += """
            </ul>
        </nav>
    </header>
    
    <main>
        <p>ØªÙ… Ø§Ø³ØªÙ†Ø³Ø§Ø® Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ù†Ø¬Ø§Ø­ Ù…Ù†: """ + self.target_url + """</p>
        <p>ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ØªØµÙØ­ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®.</p>
    </main>
    
    <script src="assets/js/unified-script.js"></script>
</body>
</html>
"""
        
        with open(self.clone_dir / 'index.html', 'w', encoding='utf-8') as f:
            f.write(index_content)
    
    def _create_usage_guide(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        guide_content = f"""# Ø¯Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®

## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹
- **Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ**: {self.target_url}
- **Ø§Ø³Ù… Ø§Ù„Ù†Ø³Ø®Ø©**: {self.clone_name}
- **ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø®**: {str(os.path.getmtime)}

## Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù„ÙØ§Øª
- `index.html`: Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
- `pages/`: Ø¬Ù…ÙŠØ¹ ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹
- `assets/`: Ø§Ù„Ø£ØµÙˆÙ„ (CSS, JS, ØµÙˆØ±)
- `components/`: Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
- `config/`: Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†
- `data/`: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©

## ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ´ØºÙŠÙ„
1. Ø§ÙØªØ­ Ù…Ù„Ù `index.html` ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­
2. Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø®Ø§Ø¯Ù… Ù…Ø­Ù„ÙŠ:
   ```bash
   python -m http.server 8000
   ```
3. Ø«Ù… Ø§Ø°Ù‡Ø¨ Ø¥Ù„Ù‰: http://localhost:8000

## Ø§Ù„ØªØ®ØµÙŠØµ ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±
- Ø¹Ø¯Ù„ Ù…Ù„ÙØ§Øª CSS ÙÙŠ `assets/css/`
- Ø£Ø¶Ù ÙˆØ¸Ø§Ø¦Ù JavaScript ÙÙŠ `assets/js/`
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ù† `components/`
- Ø±Ø§Ø¬Ø¹ `config/site-config.json` Ù„Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª

## Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®Ø©
- âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰
- âœ… Ø§Ù„ØªØµÙ…ÙŠÙ… ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·
- âœ… Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙˆØ³Ø§Ø¦Ø·
- âœ… Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
- âœ… Ø§Ù„Ø¨Ù†ÙŠØ© ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ…
- âœ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©

## Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©
- ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø­ØªØ±Ø§Ù… Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø·Ø¨Ø¹ ÙˆØ§Ù„Ù†Ø´Ø±
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ù„Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ØªØ·ÙˆÙŠØ± ÙÙ‚Ø·
- Ø±Ø§Ø¬Ø¹ Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ
"""
        
        with open(self.clone_dir / 'README.md', 'w', encoding='utf-8') as f:
            f.write(guide_content)
    
    def _generate_page_name(self, url):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„ØµÙØ­Ø© Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        import re
        from urllib.parse import urlparse
        
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            return 'home'
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø± ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ø§Ø³Ù… ØµØ§Ù„Ø­
        name = re.sub(r'[^\w\-_.]', '_', path)
        return name[:50]  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·ÙˆÙ„
    
    def _optimize_html(self, html_content):
        """ØªØ­Ø³ÙŠÙ† ÙˆØªÙ†Ø¸ÙŠÙ HTML"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Ø¥Ø¶Ø§ÙØ© meta tags Ù…Ø­Ø³Ù†Ø©
        if not soup.find('meta', attrs={'name': 'viewport'}):
            viewport_meta = soup.new_tag('meta')
            viewport_meta.attrs['name'] = 'viewport'
            viewport_meta.attrs['content'] = 'width=device-width, initial-scale=1.0'
            if soup.head:
                soup.head.append(viewport_meta)
        
        return str(soup.prettify())
    
    def _localize_links(self, html_content):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¥Ù„Ù‰ Ù…Ø­Ù„ÙŠØ©"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ØªØ­Ø¯ÙŠØ« Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£ØµÙˆÙ„
        for link in soup.find_all('link', href=True):
            href = link['href']
            if self.target_url in href:
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ø§Ø¨Ø· Ù…Ø­Ù„ÙŠ
                local_path = href.replace(self.target_url, '.')
                link['href'] = local_path
        
        return str(soup)
    
    def _extract_dependencies(self, tech_stack):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
        dependencies = {
            'css_frameworks': [],
            'js_libraries': [],
            'fonts': [],
            'icons': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
        for category, technologies in tech_stack.items():
            if 'frameworks' in category.lower():
                for tech_name in technologies.keys():
                    if 'css' in category.lower():
                        dependencies['css_frameworks'].append(tech_name)
                    elif 'javascript' in category.lower():
                        dependencies['js_libraries'].append(tech_name)
        
        return dependencies
    
    def _get_server_requirements(self, tech_stack):
        """ØªØ­Ø¯ÙŠØ¯ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø®Ø§Ø¯Ù…"""
        requirements = {
            'server_type': 'static',  # Ù…Ø¹Ø¸Ù… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®Ø© Ø³ØªÙƒÙˆÙ† Ø«Ø§Ø¨ØªØ©
            'php_required': False,
            'database_required': False,
            'special_requirements': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
        backend_tech = tech_stack.get('backend_technologies', {})
        if 'WordPress' in backend_tech or 'PHP' in str(backend_tech):
            requirements['server_type'] = 'php'
            requirements['php_required'] = True
        
        return requirements

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
def clone_website_example():
    """Ù…Ø«Ø§Ù„ Ù„Ø§Ø³ØªÙ†Ø³Ø§Ø® Ù…ÙˆÙ‚Ø¹"""
    target_url = "https://example.com"
    clone_name = "example_clone"
    
    cloner = WebsiteCloner(target_url, clone_name)
    result = cloner.clone_complete_website()
    
    print(f"ØªÙ… Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"Ø§Ù„Ù…Ø¬Ù„Ø¯: {cloner.clone_dir}")
    return result

if __name__ == '__main__':
    # ØªØ´ØºÙŠÙ„ Ù…Ù† Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±
    import argparse
    
    parser = argparse.ArgumentParser(description='Ø§Ø³ØªÙ†Ø³Ø§Ø® Ù…ÙˆØ§Ù‚Ø¹ ÙƒØ§Ù…Ù„Ø©')
    parser.add_argument('url', help='Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ø³ØªÙ†Ø³Ø§Ø®Ù‡')
    parser.add_argument('-n', '--name', required=True, help='Ø§Ø³Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø³Ø®Ø©')
    
    args = parser.parse_args()
    
    cloner = WebsiteCloner(args.url, args.name)
    cloner.clone_complete_website()
    
    print(f"âœ… ØªÙ… Ø§Ø³ØªÙ†Ø³Ø§Ø® {args.url} Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"ğŸ“ Ø§Ù„Ù…Ø¬Ù„Ø¯: cloned_sites/{args.name}")
    print(f"ğŸŒ Ø§ÙØªØ­ index.html Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©")
